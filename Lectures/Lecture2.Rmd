---
title: "Chapter 2: Statistical Modeling"
output: html_notebook
---

library(here)
library(vcd)
library(Biostrings)

<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> Statistical Modeling
<br> &emsp; <a href="#2">[1.1]</a> Chapter summary
<br> &emsp; <a href="#3">[1.2]</a> The difference between statistical and probabilistic models
<br> &emsp; <a href="#4">[1.3]</a> A simple example of statistical modeling
<br> &emsp; <a href="#5">[1.4]</a> Classical statistics for classical data
<br> &emsp; <a href="#6">[1.5]</a> Binomial distributions and maximum likelihood
<br> &emsp; <a href="#7">[1.6]</a> More boxes:multinomial data
<br> &emsp; <a href="#8">[1.7]</a> The χ<sup>2</sup> distribution
<br> &emsp; <a href="#9">[1.8]</a> Chargaff’s Rule
<br> &emsp; <a href="#10">[1.9]</a> Modeling sequential dependencies: Markov chains
<br> &emsp; <a href="#11">[2.0]</a> Bayesian Thinking
<br> &emsp; <a href="#11">[2.1]</a> Example: occurrence of a nucleotide pattern in a genome
<br> &emsp; <a href="#12">[2.2]</a> Excercises</br> 
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">Statistical Modeling</b></font><a href="#0"><sup>Return</sup></a></p>
* Previous chapter = generative modelling using know parameters to calculate probabilities
  * Problem in real life - don't know which generative model to use and parameters are often unknown
  * Solution in this chapter - estimate parameters/model using data
    * **Upwards model** unlike the previous chapter (Top down)
      * AKA statistical infeence
  * Examples in this chapter are all parametric
    * models w/ low# unknown parameters
    * estimated parameters from data is denoted <span class="math inline">\(\widehat{\theta}\)</span>

* **

<p id="2"><b><font size="5">Chapter Summary</b></font><a href="#0"><sup>Return</sup></a></p>
* Working from data back to generating distibutions & how to estimate parameters for distribution
  * Statistical models w/ categorical outcomes (binomial & multinomial)
    * **Goodness of fit**
      * visualization + simulations to test if data fits multinomial model
      * Chi-square statistic
      * compare simulation + theory via qqplot
    * **Estimation**
      * Maximum likelihood & Bayesian estimation
        * ex) nt pattern discovery & halotype estimation
    * **Prior & posterior distirbution**
      * Previously studied data (ex) halotypes) give posterior data distribtuion
        * Add uncertainty to decisison
        * Priors have small effect on result if large amount of data
    * **CpG island & Markov Chains**
      * Dependency along DNA modelled via Markov transitions
        * Build scores based on likelihood ratios -> see if DNA seqs are CpG islands or not
      * Histogram of scores
        * Saw bimodal distribution (ch4)
      * Modelling based on training data (known sequences of known CpG islands used to classify input data)

* **

<p id="3"><b><font size="5">The difference between statistical and probabilistic models</b></font><a href="#0"><sup>Return</sup></a></p>
* Probabilistic analysis requires a known distribution model which explains the randomness in data well
  * Parameters values are provided
  * ex) CH1 epitope example
    * False positives occured w/ Bernoulli(p=0.01) per position
      * #patients assyed (size) & 100 different positions of the protein (size) were fully given parameters
      * Since parameters given P(X|θ) where x is observed and θ is known parameter value
      * Used poisson for null hypothesis model (parameter λ=0.5)
        * Found probability of seeing max value of >=7 (1-P(x<=(1-max))^n is around 10<sup>-4</sup> -> reject null
    * == mathematical deduction
* What if _size_ & _n_ was known but the _probability_ was not, and why was Poisson distribution chosen?
  * Use data and move upwards to estimate best probabilty model to use (Poisson, normal, or binomial) - denoted F
    * Then find parameters for the model chosen
    * == statistical inference
    
* **

<p id="4"><b><font size="5">A simple example of statistical modeling</b></font><a href="#0"><sup>Return</sup></a></p>
* 2 steps to modeling
  1. need reasonable probability distribution
    * ex) CH1 - discrete count data modelled via simple distributions (binomial, multinomial, Poisson)
    * <mark style='background-color:yellow'>Normal distribution often good measurement for continous data</mark>
      * Distributions may be more complex (CH4)
```{r}
# Ex) CH1 epitope problem w/o outlier(value 7)
  # which.max() finds position of max value so (-) means to remove that value/position
load(here("BookStuff", "data", "e100.RData"))
e99 = e100[-which.max(e100)]
```

* For this example, evaluate <mark style='background-color:lightblue'>**Goodness-of-fit** via visualization</mark>
  * Step1: find which distribtuion would fit data best by visualizing data
    * <mark style='background-color:yellow'>Discrete data = barplot of frequencies (this data)</mark>
    * <mark style='background-color:yellow'>Continous data = Histogram</mark>
    * Hard to decide which distribution best fits data without comparing distributions
      * Solution = use a goodness-of-fit diagram visualization technique **rootogram**
        * Hangs bars w/ observed counts using theoretical red points
          * Shows square root of observed seqs via bars dropping below x-axis
        * <mark style='background-color:yellow'>If counts == theoretical values then bottom of boxes(of boxplot) will align w/ x-axis</mark>
        
```{r}
barplot(table(e99), space=0.5, col="red", xlab="Levels", ylab="counts")

#rootogram aspect
library(vcd)
gf1 <-  goodfit(e99, "binomial")
rootogram(gf1, xlab="Levels" ,rect_gp=gpar(fill="green"), main="Binomial good-fit test")
gf1.1 <- goodfit(e99, "poisson") 
rootogram(gf1.1, xlab="Levels" ,rect_gp=gpar(fill="green"), main="Poisson good-fit test")
```


```{r}
# Q) Calibrate plot w/ known parameter of lambda=0.05 to generate 100 Poisson distributed numbers ```rpois``` and generate rootogram

x <- rpois(n=100, 0.05)
x1 <-  goodfit(x, "poisson")
rootogram(x1, xlab="Levels" ,rect_gp=gpar(fill="green"), main="Poisson good-fit question")
```

* lambda is the poission mean of theroretical distribution (estimated by sample mean)
  * Solution
    * Poisson is determined by one parameter, lambda AKA the poisson mean
      * If we assume the data follows a Poisson distribution based upon the rootogram, then need to estimate lambda because parameter is missing
      * To do this, the estimated lambda value is denoted <span class="math inline">\(\hat{\lambda}\)</span> 
        * Or also known as the **likelihood estimator (MLE)**
        * Note - outlier max value was removed beforehand to generate e99 and estimate Poisson distribtuion
          * Going to use e100 for this example because real world data wouldn't notice outliers
          * Outliers will increase the mean estimate of <span class="math inline">\(\hat{\lambda}\)</span>
            * More likely to observe an outlier max of 7 in null hypothesis H0 == Larger p-value
              *  <mark style='background-color:yellow'>If p-value is small, the analysis is probably due to something real</mark>
              * conservative: err on side of caution by assuming not detecting something
```{r}
# Estimate poisson parameter
  # 1) Tally up counts per level
table(e100) #this is what rpois with predicted mean should look like

 #2) Simulate different values for Poisson mean to see which value best fits data
   # e100 = 100 observed data (n) so
table(rpois(n=100,lambda=1))
table(rpois(n=100,lambda=2))
table(rpois(n=100,lambda=3))
```

* the counts for the tables generated with these predicted means are way different from the data (note counts for levels 3, 4, 5, 6)

* Q) Repeat simulation w/ different lambda values to find one close to counts (Brute force)
  * Solution - mathematical equation to **find lambda value maximaizing probability of observing data**
    * Calculate probability of data if Poisson mean (m)
      * **Assumption** -> picking from data points independently w/ replacment
        * Probability == product of individual probabilities
      * ex) P(58 zeros, 34 ones, 7 twos, 1 seven | data Poisson(m)) = P(0)<sup>58</sup> x P(1)<sup>34</sup> x P(2)<sup>7</sup> x P(7)<sup>1</sup>
        * Probability is **likelihood function of lambda** when only data is available (no parameters)
        
        \begin{equation*}
L\left(\lambda ,\,\ x=(k_1,k_2,k_3,...)\right) = \prod_{i=1}^{100} f(k_i)
\end{equation*}
           * Where L = likelihood
           * f(k) = (e<sup>-lambda</sup>)(lambda<sup>k</sup>)/(k!) == Poisson formula

        * Instead of using product of k<sub>i</sub> levels, just **use log**
          * **logs are always increasing, but once it reaches the max interval this is equivalent to max probability**
        
```{r}
# Assuming a lambda=3=m from the last question, use prod() function to multiply each probability
  # 0, 1, 2, 7 are the levels of the dataset e100
  # power of 58, 34, 7, 1 is the counts of each level
  # Recall chapter 1 to find max probability of observing a value as extreme as 7 (1-P(x <= (1-max))^n)
    # However, we are not looking for cumulative probabilities (P<=x) -> ppois() 
prod(dpois(x=c(0,1,2,7), lambda=3)^c(58,34,7,1))
```

```{r}
# Q) Try out when lambda=m= 0, 1, 2 or a non-integer 0.4
#test <- function(i){
#  for(i in 0:2){
#    x <- prod(dpois(x=c(0,1,2,7), lambda=i)^c(58,34,7,1))
#    print(x)
#  }
#}
#test()

test <- function(x){
  for(i in x){
    x <- prod(dpois(x=c(0,1,2,7), lambda=i)^c(58,34,7,1))
    print(x)
  }
}
test(c(0:2, 0.4))
```

```{r}
# Likelihood log method
  # sum() function because one lambda value will be applied to 100 data points = 100 output evaluations which get summed per lambda
loglikelihood = function(lambda, data=e100){
  sum(log(dpois(x=data,lambda)))
}
```

```{r}
# Generate vector of lambda values
lambdas = seq(from=0.05, to=0.95, length=100)
```

```{r}
# Compute likelihood using lambda vector
# To a vectory of numbers (lambdas) apply function (vapply())
  # function applied to EACH lambda value is loglikelihood made above
  # an additonal function is converting the output numbers to numberic with length 1
    # same as saying as.numeric(loglikelihood(lambdas[1])), as.numeric(loglikelihood(lambdas[2])), as.numeric(loglikelihood(lambdas[3])), ..., as.numeric(loglikelihood(lambdas[n]))

loglikelihood2 = vapply(lambdas, loglikelihood, numeric(length=1))
```

```{r}
m0 = mean(e100) #Find mean of the 100 data values
m0

# Visualize with plot
  # lwd is the thickness of the line/points
plot(x=lambdas, y=loglikelihood2, ylab="Sum of log of likelihood probabilities", xlab=expression(lambda), type="p", col="red", lwd=2, cex=0.5)
abline(v=m0, col="blue", lwd=2) #v for vertical
abline(h=loglikelihood(m0), col="green", lwd=2) #h for horizontal - find sum of log when lambda = mean of all 100 values
```

* Note the intersecting point between the mean of 100 data points is the lambda value 0.55

* Shortcut to vapply is ```goodfit()```
  * Output of ```goodfit()``` is a list
    * One component _par_ contains the values of the fitted parameters of the distribution specified
    * For this example of poission, only the lambda parameter
```{r}
gf <- goodfit(e100, "poisson")
typeof(gf) #see if dataframe, matrix, vector, or list
names(gf) #column names of list
gf$par  #call value of column par
```

* So basically
  1. use ```goodfit()``` on data testing the distributions: poisson, binomial, nbinomial
  2. use a ```rootogram()``` from package vcd to visualize which distriutions best aligns to x-axis
  3. go back to stored ```goodfit()``` variable and call $par to find estimated parameters

```{r}
table(e100) #this is the expected counts of the real data

#Testing how closely the estimated parameter matches the actual data
aa <- goodfit(e100, "poisson")
rootogram(aa,  xlab="Levels" ,rect_gp=gpar(fill="green"), main="Poisson good-fit final test") #bars align w/ x-axis approximately - 7 is obvious outlier because it goes negative (Not really obvious unless outlier removed and test multiple distributions as above with e99)
aa$par #since Poisson matches well, this is parameters equalling lambda
```
```{r}
# Test the estimated poisson parameter
table(rpois(n=100, lambda=0.55))
```

* Close enough I guess?

```{r}
#from ch1 use lamba = 0.55 instead of 0.5
maxes = replicate(1e5, {max(rpois(n=100,lambda=0.55))})
table(maxes)
mean(maxes>=7)
```


* **

<p id="5"><b><font size="5">Classical statistics for classical data</b></font><a href="#0"><sup>Return</sup></a></p>
* Mathematic proof that finding the mean maximizes (log)likelihood

\begin{align}
\log L(\lambda, x)=&\sum_{i=1}^{100} -\lambda +k_i \log\lambda-\log (k_i!) \tag{2.1} \\
=&-100\lambda +\log\lambda\left(\sum_{i=1}^{100} k_i\right)  + \text{const.} \tag{2.2} 
\end{align}

* Statistical approach - bottom up approach to infer model parameters
  * Estimate parameter from dataset
  * Choose what distribution best fits data (Evaluate goodness of fit)
* Statistical testing
  * Null model for data
    * means an "uninteresting baseline" = all obs from same random distribution regardless of which group or treatment
    * Test for alternative = obs not from same random distribution
  * Other situations - 2 competeing models to compare
  
* Q) What is the value of modeling w/ known distribution? Ie) why know what variable has Poisson distribution?
  * A) Models are consise, expressive representations of data generation
    * In Poisson - knowing lambda gives knowledge of the distribution -> find probability of extremely rare events
    * Can also determine regression
      * Know how count-based response variable (result of counting) depends on continous covariate
      * ex) linear regression: y= ax+b+e
        * y = response variable
        * x = covariate x
        * a & b = parameters (estimated)
        * e = residual
        * Normal distribution probability model (need to estimate variance)
      * Can apply similar regression model for count data, but probability distribution of residuals are non-normal
        * **Generalized linear model**
          * seen in CH8 & 9
    * If known that probability model is Poisson, binomial, or multinomial distribution (or other parametric test) - can find parameters of a model & compute quantities (p-values & confidence intervals)

* **

<p id="6"><b><font size="5">Binomial distributions and maximum likelihood</b></font><a href="#0"><sup>Return</sup></a></p>
* Two parameters for binomial distributions (n trials (usually known) & p probability of 1 trial (unknown))

```{r}
# ex) Sample of n=120 males, tested for color blindness - 0 not blind & 1 blind
cb <- c(rep(0, 110), rep(1,10))
table(cb)

```

```{r}
# Find the estimated-value of this data (p hat)

bb <- goodfit(cb, "binomial")
bb$par
```

```{r}
# Perform likelihood calculation, similar to Possion example, find likelihood of possible outcome based upon vector of estimated probabilities and plot to find max
  # Actual binomial formula B(n,p) - chance of seeing x successes when there are n trials
  # Rbinom function
    # size = #number of trial is length of vector
    # n = observations based on sum of seeing success (1's) -> sum(cb) - seeing
probs <- seq(0, 0.3, by=0.005) #This makes a vector of estimated probabilities - MAY NOT CONTAIN BEST MATCH EXACT PROB
likelihoodBinom <- dbinom(x=sum(cb), size=length(cb), prob=probs) #each probability from vector(61) applied -> output(61) 
  # Probability of seeing a number as large as x = sum(cb) = 10, when size(n)=length(cb)=120 trials B(n,p) - P(x=10)
plot(x=probs, y=likelihoodBinom, pch=16, xlab="Probability of Success", ylab="likelihood probability simulation", cex=0.6)
probs[which.max(likelihoodBinom)] #in probs vector which is the closest max value matching likelihoodBinom vector
```

* Likelihood for binomial distribution
  * likelihood and probability are the same function - Have different interpretations
    * Probability to see set of values when given parameters (probability - one p)
    * Data is fixed but what parameter value will fit the data better (likeliness - test multi p)
  * ex) If B(n,p) where n=300 and observe x=40 successes - the binomial distribution
    * where theta = estimate parameter
  
  \begin{equation}
f(\theta\,|\,n,y) = f(y\,|\,n,\theta)={n \choose y} \, \theta^y \, (1-\theta)^{(n-y)}.
\tag{2.5}
\end{equation}
  
    * when {n obs \choose x sucess} large (~e<sup>115</sup>) - then can use log of likelihood for the formula
    
    \begin{equation*}
\log f(\theta |y) = 115 + 40\log(\theta)+(300-40)\log(1-\theta).
\end{equation*}

```{r}
# equivalent function of log binomial

loglikelihoodBinom <- function(theta, n=300, k=40){
  115+(k*log(theta))+((n-k)*log(1-theta))
}
```

```{r}
# Assume theta parameter can range from 0 to 1
thetas = seq(0, 1, by=0.001)
plot(x=thetas, y=loglikelihoodBinom(thetas), xlab=expression(theta), ylab=expression(paste("log f(",theta, " | y)")), type="l")
```

* Maxima here is k/n = 40/300 = 0.133

* **

<p id="7"><b><font size="5">More boxes:multinomial data</b></font><a href="#0"><sup>Return</sup></a></p>
* DNA nucleotides - purines(A G) & pyrimidines (C T)
  * Binomial model works for purines/pyrimidines as sets but not all together (ACGT) -> needs multinomial model
* Nucleotide bias
  * Estimation testing + Simulation
```{r}
# Load set of seqs
staph <- readDNAStringSet(here("BookStuff", "data", "staphsequence.ffn.txt"), "fasta")
staph[1]
letterFrequency(staph[[1]], letters="ACGT", OR=0)
```
  
* Double brackets in the above code [[i]] is used to extract the ith gene as DNA string -> length 1362 (extract each nt)
  * Single brackets [i] returns DNAStringSet w/ single DNAString -> length 1 (extract sequence)

```{r}
# Similar procedure as 1.8
# to be done
```

* Selection acts on nucelotide frequencies
  * Example question: Do the first 10 genes (seqs) from the staph data come from the same multinomial?
    * AKA do nucleotides occur in the same proportions for the first 10 genes? -yes = no selection, -no = selection
```{r}
letterfreq <- vapply(staph, letterFrequency, FUN.VALUE=numeric(4), letters="ACGT", OR=0) #to vector staph apply the function letterFrequency count for letters "A|C|G|T" & output counts as numeric with length 4 
colnames(letterfreq) <- paste("gene", seq(along=staph)) #replace the column names of letterfreq vector w/ gene+number in staph vector
tab10 <- letterfreq[,1:10] # subset to columns 1 to 10 (gene 1 to 10)
computeproportions <- function(x){x/sum(x)} #make function that takes x and divides by sum of x (takes single value & divides by column or row totol depending on margin of next function)
prob10 <- round(apply(tab10, 2, computeproportions), digits=2) # apply to each value in those 10 columns the computeproportions() function made
prob10 #This is the predicted probabilities for each nt in each gene
```

```{r}
# Calculate mean of probabilities
p0 = rowMeans(prob10)
p0
```

* p0 is vector of multinomial probabilities for all 10 genes (seqs) - use a Monte Carlo simulation to test differences between observed letter freqs & expected value range under this simulation is plausible
```{r}
# Compute expected counts via - take outer product of vector of probabilities(p0)
  # Sum nucleotide counts from each of the 10 columns (10 genes)
Csum <- colSums(tab10) #tab10 is the variable where a matrix of counted nucleotides resides - so here we're just counting the total ATCG per gene seqeucne
Csum


#outer product means to mutiply (sum A+C+G+T per gene)*(predecited probabilities from rowMeans of all nucleotides)
  # So predicted probability of Adenosine(A) is 0.346 from rowMeans() * Count of 1362 nts for gene 11 = 471 A's for gene1
expectedtab10 <- outer(p0, Csum, FUN="*")  #outer product of arrays X & Y - FUN="*" is a special case as.vector(X) --???
round(expectedtab10)
```

* Now know predicted probabilities & expected counts of nt per gene
  * Can create random table w/ correct column sums via ```rmultinom``` function
    * **Table assumes that null hypothesis is true (proportions given by p0 is correct)**
```{r}
# The function made in sapply() takes an input, s, and applys rmultinom (probability p0, number of obs=1, size=s)
  # s in this case is Columnsum calculated from above
  # Creates a new table using
    # Csum = vector of summed ATCG per gene sequence
    # p0 = probability predicted per nucleotide per gene
    # So size of Csum is equal to gene1=1362, gene2=1134, gene3= 246, etc 
       # simulate 1 observation per nt using probabilities from p0
       # so function only works on one column of Csum ex) rmultinom(1, size=Csum[1], prob=p0) - where Csum[1] = 1362
          # By using sapply, we can apply this for each column of Csum (10 genes)

randomtab10 = sapply(Csum, function(s){rmultinom(1, size=s, prob=p0)}) #works on list or vector - function called on each item on list/vector
all(colSums(randomtab10)==Csum)  # all() returns logical true or false - compares column sum of above sapply to original columnsum calculated above -> they should equal to the same column sums b/c of the `size` parameter from rmultinom()
```

```{r}
# Chi-squared goodness of fit function from 1.4.1 - needed for the next step of Monte Carlo Simulation
stat = function(obsvd, exptd = 20 * pvec) {
  sum((obsvd - exptd)^2 / exptd)
}
```


```{r}
# Repeat this for 1000 simulations - for each table, find the test statistic using the chi-squared goodness of fit function 
  # Store results in vector "simulstat"
  # This will be the null distribution generated assuming the null hypothesis - p0 predicted probabilites is vector of multinomial proportions for each of the 10 genes


# stimulationofstat replicate() is for Monte Carlo Simulation method
  # replicate 1000 tomes (B)
  # generate a table of simulated values randomtab10.1 using predicted counts of Csum & predicted probabilities
  # chi-squared calculation of observed count values generated from randomtab10.1 (calculated using predicted probabilities), with expected values (expectedtab10) calculated from predicted probabilities(p0)

#S1 is the chi-squared calulation using actual observed count values, and expected values (exptectedtab10) calculated from predicted probabilities(p0)

B <-  1000
simulate1000tablesANDApplychiStat <- replicate(B,{ randomtab10.1 <- sapply(Csum, function(s){rmultinom(n=1, size=s, prob=p0)}) #generate 1000 table simulations
stat(randomtab10.1, expectedtab10)  #apply the stat chi-squared function to all 1000 tables using predicted expected values
})


S1 <- stat(tab10, expectedtab10) # Chi-squared using actual observed counts of ATCG

sum(simulate1000tablesANDApplychiStat>=S1)  #if any chi-squared values generated from the 1000 Monte-Carlo generated tables is >= chi-squared value generated from actual observed counts -> sum all True's
```

```{r}
#Visualize 

hist(simulate1000tablesANDApplychiStat, col="red", breaks=seq(from=0, to=75, length.out=50))
abline(v=S1, col="purple") #add a line for chi-squared calculation from actual ACTG counts
abline(v=quantile(simulate1000tablesANDApplychiStat, probs=c(0.95, 0.99)), col=c("blue", "green"), lty=2) #add a line for 95th and 99th quantiles of the simulationofstat chi-squared using simulated observed count values
```

* from the histogram - probability of seeing value as large as S1=70.1 (blue line - chi-squared value from actual ATCG counts) is small under this null model
  * Occured 0/1000 simulations (no bars at 70.1) -> **therefore, the 10 sequences do not come from the same multinomial model**


* **
<p id="8"><b><font size="5">The χ<sup>2</sup> distribution</b></font><a href="#0"><sup>Return</sup></a></p>
* Can use stats theory for same conclusion w/o using Monte Carlo simulations
  * Distribution of simulate1000tablesANDApplychiStat is approximately X<sup>2</sup> distribution (Chi-Squared)
    * Approximation is good if counts in table are large
    * **Parameter of 30 total for all genes evaluated (=10 genes * (4-1 nts d.f.))**
  * Can compute probability of having value as large as S1=70.1
  * **Monte Carlo problem - doesn't really work for small probabiltiies**
    * Can't estimate probabilites smaller than the simulation (B=1000) so 1/1000 lowest probability estimated -> so estimated values will be larger ("Granularity of Monte Carlo simulations")
    * Therefore, mathematical theories are better
      * Example: Check how theory and simulation matches using the **goodness-of-fit tool (QQ) plot**
        * Compare 2 distributions, with different samples or same samples in different theoretical models - looking at histograms is not informative
        * **QQ plot finds quantiles of each distribution**
* Quntiles & quantile-quantile plot
  * ex) previous chapter - used order statistics to order 100 sample values 
    * (Random sample size (n) -> order each random var from smallest exptected success to largest (x<sub>(1)</sub> < x<sub>(2)</sub>, x<sub>(n)</sub>))
    * Find 22th percentile 
      * Take any value between the 22nd & 23rd value (x<sub>22</sub><=c<sub>0.22</sub><x<sub>23</sub> = 0.22 quantile c)
      * so c<sub>0.22</sub> defined by 
      
\begin{equation*}
\frac{\# x_i's \leq c_{0.22}}{n} = 0.22.
\end{equation*}
    
      * In chapter 3 -> emperical cumulative distribution function \hat{F}
        * Which can define c<sub>0.22</sub> as \hat{F}_n(c_{0.22}) = 0.22
  * In the simulate1000tablesANDApplychiStat variable, we calculated the c<sub>0.95</sub> & c<sub>0.99</sub> quantiles

# <mark style='background-color:red'>Skipped for now</mark>

```{r}

# Compare simulate1000tablesANDApplychiStat chi-squared values to 1000 generated simulations of chi-squared with parameter 30
  # Display in histograms w/ 50bins each chi-squared value

# Compute quantiles of simulate1000tablesANDApplychiStat chi-squared values & compare to chi-squared distribution w/ parameter 30

qs <- ppoints(100)
quantile(simulstat, qs)
quantile(qchisq(p=qs, df=30), qs)
```


* **
<p id="9"><b><font size="5">Chargaff’s Rule</b></font><a href="#0"><sup>Return</sup></a></p>

* Tetranucleotide hypothesis
  * Used MW of molecules to see if nucleotides occured at equal freqs (P(A)=P(G)=P(C)=P(T))
```{r}
#Reported in percentages
load(here("BookStuff", "data", "ChargaffTable.RData"))
ChargaffTable
```

* <mark style='background-color:red</mark>Q) Skipped</mark>

* Pattern visualized by Chargaff is base pairing for matching frequencies of AT & CG = Chargaff's Rule
  * Statistic = ((P(C)-P(G))<sup>2</sup>+(P(A)-P(T))<sup>2</2> -> summed over all rows of a table
* What if there as no relationships between the base pairs? - assume nt's are exchangeable w/ no order (independent) --> H0
```{r}
# Make Chargaff stat function, where x is the table of counts
statChargaff <- function(x){sum(
  (x[,"C"]-x[,"G"])^2 + (x[,"A"]-x[,"T"])^2
)}

chfstat <- statChargaff(ChargaffTable) #apply chargaff stat to original table

#Monte carlo simulation x100,000 times
permstat <- replicate(100000, {
  permuted <- t(apply(ChargaffTable,1,sample)) #take random sample from the rows of Chargaff table(Default replace=FALSE) - t(transpose row/col)
  colnames(permuted) <- colnames(ChargaffTable) #set the column names of permuted to equal ChargaffTable
  statChargaff(permuted) #calculate base pairing
})
```


```{r}
pChf <- mean(permstat <= chfstat)  #compare simulation to original
pChf
```
```{r}
hist(permstat, breaks=100,main="100,000 simulation of Chargaff", col="red")
abline(v=chfstat, lwd=2, col="blue") # Chargaff stat calculated using non-simulated table
```

* Histogram indiactes it's rare to have a value as small as the blue line (chfstat = 11.08)
  * Probability of observing a value as small as pChf = 1.6x10^-4
    * significant to reject the null, so Ha means that nt's are not exchangeable & have a relationship (dependent)

* Q. only looked at one-side (values smaller than observed) b/c.........

* Two categorical variables 
  * From ch1 until now, the data has been binomial (yes/no binary), or multinomuial (ATCG, genotypes)
  * What if we want to measure >=2 categorical variables on a set of subjects?
    * ex) Eye color + hair color
      * Cross tabulate counts for each eye color/hair color combinations = **Contingency table (table of counts)**
```{r}
HairEyeColor[,,2] #[eye color total, hair color total, 1=male 2=female]
str(HairEyeColor) # numeric array w/ 3 dimensions
```

* Color blindness & sex
```{r}
# Red-green colourblindless - Deuteranopia data
load(here("BookStuff", "data", "Deuteranopia.RData"))
Deuteranopia
```

* Test if sex is related w/ colourblindness
  * H0 - no relationship between sex & colourblindness (2 independent binomials - sex & colourblindness)
    * estimate multinomial probabilities & compare observed vs expected counts via ```chisq.test()``` function
```{r}
chisq.test(Deuteranopia)
```

* Small p-value indicates - low proability of seeing observed counts under H0 model
  * -> Again, H0 = model assumes fraction of colourblindness among male/female is the same (no relationship)
  * Can also test w/ **Fisher's exact test (hypergeometric test)** - seen in CH10
    * Used to test for overrepresentation of certain types of genes
    
* Hardy-Weinberg equilibrium (Special multinomial)
  *<mark style='background-color:yellow'>A multinomial with 3 possible levels via combinding two alleles (M & N)</mark>
    * p = overall frequency of allele M in population
    * q = N allele = 1-p
  * Hardy-Weinberg model -> looks for **genotype independent frequency of alleles** relation between p & q
    * ex) Random mating in large population w/ 3 possible genotypes
    <center> p<sub>MM</sub> = p<sup>2</sup>, p<sub>NN</sub> = q<sup>2</sup>, p<sub>NM</sub> = 2pq</center>
    * Observed frequencies (n<sub>MM</sub>,n<sub>MN</sub>,n<sub>NN</sub>) = genotypes MM, MN, NN
      * Total number S = n<sub>MM</sub>+n<sub>MN</sub>+n<sub>NN</sub>
      * Multinomial Likelihood  formula (Probability of seeing observed data when probabilities of the frequencies given)
      
      \begin{equation*}
P(n_{\text{MM}},\,n_{\text{MN}},\,n_{\text{NN}}\;|\;p) =
{S \choose n_{\text{MM}},n_{\text{MN}},n_{\text{NN}}}
(p^2)^{n_{\text{MM}}} \,\times\,
(2pq)^{n_{\text{MN}}} \,\times\,
(q^2)^{n_{\text{NN}}},
\label{eq:likHWE}
\end{equation*}

      * Log-likelihood form of Hardy-Weinberg formula
      
\begin{equation*}
L(p)=n_{\text{MM}}\log(p^2)+n_{\text{MN}} \log(2pq)+n_{\text{NN}}\log(q^2).
\label{eq:loglikHWE}
\end{equation*}

      * p value to maximize log-likelihood
      
\begin{equation*}
p = \frac{n_{\text{MM}} + n_{\text{MN}}/2}{S}.
\end{equation*}

* If given data of observed frequencies (n<sub>MM</sub>,n<sub>MN</sub>,n<sub>NN</sub>), the log-likelihood(L) is a function with a parameter p (q doesn't matter b/c 1-p)

```{r}
# Test HW log-likelihood function for different p-values in a dataset of blood group alleles

library("HardyWeinberg")
data("Mourant")
nMM <- Mourant$MM[216] #subset only row 216 counts
nMN <- Mourant$MN[216]
nNN <- Mourant$NN[216]

Mourant[214:216,] #short view of dataset from rows 214 to 216
```

```{r}
# Make the HW-log function

loglikHW <- function(p,q=1-p){
  2*nMM*log(p) + nMN*log(2*p*q) + 2*nNN*log(q)
}
```

```{r}
xv <- seq(from=0.01,to=0.99, by=0.01) #vector of probabilities
p <- loglikHW(xv) #insert vector of probabilities into the function
plot(x=xv, y=p, type="l", lwd=2, xlab="probabilities", ylab="log-likelihood of probabilities")
imax=which.max(p) #position of highest (+)log-likelihood value
abline(v=xv[imax], h=p[imax], lwd=1.5, col="blue") #xv[imax] = in probability vector which is position imax(58)
abline(h=p[imax], lwd=1.5, col="purple")
```

* Maximum likelihood estimate for probabilities in multinomial can be observed via observed freqs in binomial model, but estimates in binomial need to account for relationships between all 3 probabilities 
  * <span class="math inline">\(\hat{p}_{MM}, \hat{p}_{MN}, \hat{p}_{NN}\)</span> 

```{r}
# af() function in HardyWeinberg package to calculate probabilities when counts are known
phat <- af(c(nMM,nMN, nNN))
phat
```

```{r}
pMM <- phat^2
qhat <- 1-phat
pHW <- c(MM=phat^2, MN=2*phat*qhat, NN=qhat^2)
sum(c(nMM,nMN,nNN)) * pHW  #observed values
```

```{r}
#compare to original expected values
Mourant[216,]
```

* Can further test if observed values can be used to reject H0 (Hardy-Weinberg model) via simulation or chi-squared test as previous.
  * There is a visual evaluation of goodness of fit for Hardy-Weinberg (ternary plot)
    * Sample points @coordinates given by proportions of each different allele
  * ```HWTenaryPlot()``` function
```{r}
pops <- c(1,69,128,148.192) #specific column numbers (countries) used to subset in HWTernaryPlot() function
genotypeFrequencies <- as.matrix(Mourant[,c("MM","MN","NN")])
HWTernaryPlot(genotypeFrequencies[pops,],
              markerlab=Mourant$Country[pops],
              alpha=0.0001, curvecols=c("red", rep("purple",4)),
              mcex=0.75, vertex.cex=1)
```

* The red line is the HW-model, between the 2 purple lines is the region where HW-model (H0) can't be rejected
  * Countries farther away from red line = farther from HW equilibrium

```{r}
# Make ternary plot & add other data points
HWTernaryPlot(genotypeFrequencies[155,], alpha=0.0001, newframe=TRUE, cex=0.5)
HWChisq(genotypeFrequencies[155,])
```

```{r}
# Divide all total frequencies by 50 - keep same porportion for each genotypes & re-make ternary plot
newgfHW <- round(genotypeFrequencies/50)
HWTernaryPlot(newgfHW[pops,],
              markerlab=Mourant$Country[pops],
              alpha=0.0001, curvecols=c("red", rep("purple",4)),
              mcex=0.75, vertex.cex=1)
```

* Points stay the same but confidence regions have a wider spread b/c count frequencies decreased (but proportions remained)?

* Concatenating several multinomials - sequence motif & logos
  * Kozak motif = seq occuring close to start codon(ATG) of coding ORF
    * nt position#1 to 5 left to ATG -> differs in nucleotide (not equally likely)
  * **Position weight matrix/Position-specific scoring matrix** = multinomial probabilities at every position of a seq
    * Visualized via sequence logo
```{r}
#BiocManager::install("seqLogo")
library(seqLogo)
load(here("BookStuff", "data", "kozak.RData"))
pwn <- makePWM(kozak)
seqLogo(pwn, ic.scale=FALSE)
```

* Multinomial decisions("boxes") rarely have equal probabilities/frequencies
  * Parameters p1, p2,.., pn differ depending on data (ex) 20 AA's, blood types, hair colour)
  * If multiple categorical variables, they are rarely indepdendent (ex) sex & cb, hair & eye colour)
    * More in CH9 - multivariate decomposition of contingency tables

* **
<p id="10"><b><font size="5">Modeling sequential dependencies: Markov Chains</b></font><a href="#0"><sup>Return</sup></a></p>
* Special case of dependencies between categorical variables - along a seq/chain of categorical variables
* Predict weather

* **
<p id="11"><b><font size="5">Bayesian Thinking</b></font><a href="#0"><sup>Return</sup></a></p>


* **
<p id="12"><b><font size="5">Example: occurrence of a nucleotide pattern in a genome</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="13"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>


* **

<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*  

* μ = mean of normal distribution
* θ = multiple parameters for probability model
  * ex) binomial model θ=(n,p) -> 2 numbers n=positive integer & p=real number between 0 & 1

</body>
  