---
title: "Chapter 2: Statistical Modeling"
output: html_notebook
---


```{r, message=FALSE, warning=F}
library(here)
library(vcd)
library(Biostrings)
```

<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> Statistical Modeling
<br> &emsp; <a href="#2">[1.1]</a> Chapter summary
<br> &emsp; <a href="#3">[1.2]</a> The difference between statistical and probabilistic models
<br> &emsp; <a href="#4">[1.3]</a> A simple example of statistical modeling
<br> &emsp; <a href="#5">[1.4]</a> Classical statistics for classical data
<br> &emsp; <a href="#6">[1.5]</a> Binomial distributions and maximum likelihood
<br> &emsp; <a href="#7">[1.6]</a> More boxes:multinomial data
<br> &emsp; <a href="#8">[1.7]</a> The χ<sup>2</sup> distribution
<br> &emsp; <a href="#9">[1.8]</a> Chargaff’s Rule
<br> &emsp; <a href="#10">[1.9]</a> Modeling sequential dependencies: Markov chains
<br> &emsp; <a href="#11">[2.0]</a> Bayesian Thinking
<br> &emsp; <a href="#12">[2.1]</a> Example: occurrence of a nucleotide pattern in a genome
<br> &emsp; <a href="#13">[2.2]</a> Excercises</br> 
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">Statistical Modeling</b></font><a href="#0"><sup>Return</sup></a></p>
* Previous chapter = generative modelling using know parameters to calculate probabilities
  * Problem in real life - don't know which generative model to use and parameters are often unknown
  * Solution in this chapter - estimate parameters/model using data
    * **Upwards model** unlike the previous chapter (Top down)
      * AKA statistical infeence
  * Examples in this chapter are all parametric
    * models w/ low# unknown parameters
    * estimated parameters from data is denoted <span class="math inline">\(\widehat{\theta}\)</span>

* **

<p id="2"><b><font size="5">Chapter Summary</b></font><a href="#0"><sup>Return</sup></a></p>
* Working from data back to generating distibutions & how to estimate parameters for distribution
  * Statistical models w/ categorical outcomes (binomial & multinomial)
    * **Goodness of fit**
      * visualization + simulations to test if data fits multinomial model
      * Chi-square statistic
      * compare simulation + theory via qqplot
    * **Estimation**
      * Maximum likelihood & Bayesian estimation
        * ex) nt pattern discovery & halotype estimation
    * **Prior & posterior distirbution**
      * Previously studied data (ex) halotypes) give posterior data distribtuion
        * Add uncertainty to decisison
        * Priors have small effect on result if large amount of data
    * **CpG island & Markov Chains**
      * Dependency along DNA modelled via Markov transitions
        * Build scores based on likelihood ratios -> see if DNA seqs are CpG islands or not
      * Histogram of scores
        * Saw bimodal distribution (ch4)
      * Modelling based on training data (known sequences of known CpG islands used to classify input data)

* **

<p id="3"><b><font size="5">The difference between statistical and probabilistic models</b></font><a href="#0"><sup>Return</sup></a></p>
* Probabilistic analysis requires a known distribution model which explains the randomness in data well
  * Parameters values are provided
  * ex) CH1 epitope example
    * False positives occured w/ Bernoulli(p=0.01) per position
      * #patients assyed (size) & 100 different positions of the protein (size) were fully given parameters
      * Since parameters given P(X|θ) where x is observed and θ is known parameter value
      * Used poisson for null hypothesis model (parameter λ=0.5)
        * Found probability of seeing max value of >=7 (1-P(x<=(1-max))^n is around 10<sup>-4</sup> -> reject null
    * == mathematical deduction
* What if _size_ & _n_ was known but the _probability_ was not, and why was Poisson distribution chosen?
  * Use data and move upwards to estimate best probabilty model to use (Poisson, normal, or binomial) - denoted F
    * Then find parameters for the model chosen
    * == statistical inference
    
* **

<p id="4"><b><font size="5">A simple example of statistical modeling</b></font><a href="#0"><sup>Return</sup></a></p>
* 2 steps to modeling
  1. need reasonable probability distribution
    * ex) CH1 - discrete count data modelled via simple distributions (binomial, multinomial, Poisson)
    * <mark style='background-color:yellow'>Normal distribution often good measurement for continous data</mark>
      * Distributions may be more complex (CH4)
```{r}
# Ex) CH1 epitope problem w/o outlier(value 7)
  # which.max() finds position of max value so (-) means to remove that value/position
load(here("BookStuff", "data", "e100.RData"))
e99 = e100[-which.max(e100)]
```

### For this example, evaluate <mark style='background-color:lightblue'>**Goodness-of-fit** via visualization</mark>
  * Step1: find which distribtuion would fit data best by visualizing data
    * <mark style='background-color:yellow'>Discrete data = barplot of frequencies (this data)</mark>
    * <mark style='background-color:yellow'>Continous data = Histogram</mark>
    * Hard to decide which distribution best fits data without comparing distributions
      * Solution = use a goodness-of-fit diagram visualization technique **rootogram**
        * Hangs bars w/ observed counts using theoretical red points
          * Shows square root of observed seqs via bars dropping below x-axis
        * <mark style='background-color:yellow'>If counts == theoretical values then bottom of boxes(of boxplot) will align w/ x-axis</mark>
        
```{r}
barplot(table(e99), space=0.5, col="red", xlab="Levels", ylab="counts")

#rootogram aspect
library(vcd)
gf1 <-  goodfit(e99, "binomial")
rootogram(gf1, xlab="Levels" ,rect_gp=gpar(fill="green"), main="Binomial good-fit test")
gf1.1 <- goodfit(e99, "poisson") 
rootogram(gf1.1, xlab="Levels" ,rect_gp=gpar(fill="green"), main="Poisson good-fit test")
```


```{r}
# Q) Calibrate plot w/ known parameter of lambda=0.05 to generate 100 Poisson distributed numbers ```rpois``` and generate rootogram

x <- rpois(n=100, 0.05)
x1 <-  goodfit(x, "poisson")
rootogram(x1, xlab="Levels" ,rect_gp=gpar(fill="green"), main="Poisson good-fit question")
```

### lambda is the poission mean of theroretical distribution (estimated by sample mean)
  * Solution
    * Poisson is determined by one parameter, lambda AKA the poisson mean
      * If we assume the data follows a Poisson distribution based upon the rootogram, then need to estimate lambda because parameter is missing
      * To do this, the estimated lambda value is denoted <span class="math inline">\(\hat{\lambda}\)</span> 
        * Or also known as the **likelihood estimator (MLE)**
        * Note - outlier max value was removed beforehand to generate e99 and estimate Poisson distribtuion
          * Going to use e100 for this example because real world data wouldn't notice outliers
          * Outliers will increase the mean estimate of <span class="math inline">\(\hat{\lambda}\)</span>
            * More likely to observe an outlier max of 7 in null hypothesis H0 == Larger p-value
              *  <mark style='background-color:yellow'>If p-value is small, the analysis is probably due to something real</mark>
              * conservative: err on side of caution by assuming not detecting something
```{r}
# Estimate poisson parameter
  # 1) Tally up counts per level
table(e100) #this is what rpois with predicted mean should look like

 #2) Simulate different values for Poisson mean to see which value best fits data
   # e100 = 100 observed data (n) so
table(rpois(n=100,lambda=1))
table(rpois(n=100,lambda=2))
table(rpois(n=100,lambda=3))
```

* the counts for the tables generated with these predicted means are way different from the data (note counts for levels 3, 4, 5, 6)

* Q) Repeat simulation w/ different lambda values to find one close to counts (Brute force)
  * Solution - mathematical equation to **find lambda value maximaizing probability of observing data**
    * Calculate probability of data if Poisson mean (m)
      * **Assumption** -> picking from data points independently w/ replacment
        * Probability == product of individual probabilities
      * ex) P(58 zeros, 34 ones, 7 twos, 1 seven | data Poisson(m)) = P(0)<sup>58</sup> x P(1)<sup>34</sup> x P(2)<sup>7</sup> x P(7)<sup>1</sup>
        * Probability is **likelihood function of lambda** when only data is available (no parameters)
        
        \begin{equation*}
L\left(\lambda ,\,\ x=(k_1,k_2,k_3,...)\right) = \prod_{i=1}^{100} f(k_i)
\end{equation*}
           * Where L = likelihood
           * f(k) = (e<sup>-lambda</sup>)(lambda<sup>k</sup>)/(k!) == Poisson formula

        * Instead of using product of k<sub>i</sub> levels, just **use log**
          * **logs are always increasing, but once it reaches the max interval this is equivalent to max probability**
        
```{r}
# Assuming a lambda=3=m from the last question, use prod() function to multiply each probability
  # 0, 1, 2, 7 are the levels of the dataset e100
  # power of 58, 34, 7, 1 is the counts of each level
  # Recall chapter 1 to find max probability of observing a value as extreme as 7 (1-P(x <= (1-max))^n)
    # However, we are not looking for cumulative probabilities (P<=x) -> ppois() 
prod(dpois(x=c(0,1,2,7), lambda=3)^c(58,34,7,1))
```

```{r}
# Q) Try out when lambda=m= 0, 1, 2 or a non-integer 0.4
#test <- function(i){
#  for(i in 0:2){
#    x <- prod(dpois(x=c(0,1,2,7), lambda=i)^c(58,34,7,1))
#    print(x)
#  }
#}
#test()

test <- function(x){
  for(i in x){
    x <- prod(dpois(x=c(0,1,2,7), lambda=i)^c(58,34,7,1))
    print(x)
  }
}
test(c(0:2, 0.4))
```

```{r}
# Likelihood log method
  # sum() function because one lambda value will be applied to 100 data points = 100 output evaluations which get summed per lambda
loglikelihood = function(lambda, data=e100){
  sum(log(dpois(x=data,lambda)))
}
```

```{r}
# Generate vector of lambda values
lambdas = seq(from=0.05, to=0.95, length=100)
```

```{r}
# Compute likelihood using lambda vector
# To a vectory of numbers (lambdas) apply function (vapply())
  # function applied to EACH lambda value is loglikelihood made above
  # an additonal function is converting the output numbers to numberic with length 1
    # same as saying as.numeric(loglikelihood(lambdas[1])), as.numeric(loglikelihood(lambdas[2])), as.numeric(loglikelihood(lambdas[3])), ..., as.numeric(loglikelihood(lambdas[n]))

loglikelihood2 = vapply(lambdas, loglikelihood, numeric(length=1))
```

```{r}
m0 = mean(e100) #Find mean of the 100 data values
m0

# Visualize with plot
  # lwd is the thickness of the line/points
plot(x=lambdas, y=loglikelihood2, ylab="Sum of log of likelihood probabilities", xlab=expression(lambda), type="p", col="red", lwd=2, cex=0.5)
abline(v=m0, col="blue", lwd=2) #v for vertical
abline(h=loglikelihood(m0), col="green", lwd=2) #h for horizontal - find sum of log when lambda = mean of all 100 values
```

* Note the intersecting point between the mean of 100 data points is the lambda value 0.55

* Shortcut to vapply is ```goodfit()```
  * Output of ```goodfit()``` is a list
    * One component _par_ contains the values of the fitted parameters of the distribution specified
    * For this example of poission, only the lambda parameter
```{r}
gf <- goodfit(e100, "poisson")
typeof(gf) #see if dataframe, matrix, vector, or list
names(gf) #column names of list
gf$par  #call value of column par
```

* So basically
  1. use ```goodfit()``` on data testing the distributions: poisson, binomial, nbinomial
  2. use a ```rootogram()``` from package vcd to visualize which distriutions best aligns to x-axis
  3. go back to stored ```goodfit()``` variable and call $par to find estimated parameters

```{r}
table(e100) #this is the expected counts of the real data

#Testing how closely the estimated parameter matches the actual data
aa <- goodfit(e100, "poisson")
rootogram(aa,  xlab="Levels" ,rect_gp=gpar(fill="green"), main="Poisson good-fit final test") #bars align w/ x-axis approximately - 7 is obvious outlier because it goes negative (Not really obvious unless outlier removed and test multiple distributions as above with e99)
aa$par #since Poisson matches well, this is parameters equalling lambda
```
```{r}
# Test the estimated poisson parameter
table(rpois(n=100, lambda=0.55))
```

* Close enough I guess?

```{r}
#from ch1 use lamba = 0.55 instead of 0.5
maxes = replicate(1e5, {max(rpois(n=100,lambda=0.55))})
table(maxes)
mean(maxes>=7)
```


* **

<p id="5"><b><font size="5">Classical statistics for classical data</b></font><a href="#0"><sup>Return</sup></a></p>
* Mathematic proof that finding the mean maximizes (log)likelihood

\begin{align}
\log L(\lambda, x)=&\sum_{i=1}^{100} -\lambda +k_i \log\lambda-\log (k_i!) \tag{2.1} \\
=&-100\lambda +\log\lambda\left(\sum_{i=1}^{100} k_i\right)  + \text{const.} \tag{2.2} 
\end{align}

* Statistical approach - bottom up approach to infer model parameters
  * Estimate parameter from dataset
  * Choose what distribution best fits data (Evaluate goodness of fit)
* Statistical testing
  * Null model for data
    * means an "uninteresting baseline" = all obs from same random distribution regardless of which group or treatment
    * Test for alternative = obs not from same random distribution
  * Other situations - 2 competeing models to compare
  
* Q) What is the value of modeling w/ known distribution? Ie) why know what variable has Poisson distribution?
  * A) Models are consise, expressive representations of data generation
    * In Poisson - knowing lambda gives knowledge of the distribution -> find probability of extremely rare events
    * Can also determine regression
      * Know how count-based response variable (result of counting) depends on continous covariate
      * ex) linear regression: y= ax+b+e
        * y = response variable
        * x = covariate x
        * a & b = parameters (estimated)
        * e = residual
        * Normal distribution probability model (need to estimate variance)
      * Can apply similar regression model for count data, but probability distribution of residuals are non-normal
        * **Generalized linear model**
          * seen in CH8 & 9
    * If known that probability model is Poisson, binomial, or multinomial distribution (or other parametric test) - can find parameters of a model & compute quantities (p-values & confidence intervals)

* **

<p id="6"><b><font size="5">Binomial distributions and maximum likelihood</b></font><a href="#0"><sup>Return</sup></a></p>
* Two parameters for binomial distributions (n trials (usually known) & p probability of 1 trial (unknown))

```{r}
# ex) Sample of n=120 males, tested for color blindness - 0 not blind & 1 blind
cb <- c(rep(0, 110), rep(1,10))
table(cb)

```

```{r}
# Find the estimated-value of this data (p hat)

bb <- goodfit(cb, "binomial") #remember to check by rootogram to see if good distribution fit
bb$par
```

```{r}
# Perform likelihood calculation, similar to Possion example, find likelihood of possible outcome based upon vector of estimated probabilities and plot to find max
  # Actual binomial formula B(n,p) - chance of seeing x successes when there are n trials
  # Rbinom function
    # size = #number of trial is length of vector
    # n = observations based on sum of seeing success (1's) -> sum(cb) - seeing
probs <- seq(0, 0.3, by=0.005) #This makes a vector of estimated probabilities - MAY NOT CONTAIN BEST MATCH EXACT PROB
likelihoodBinom <- dbinom(x=sum(cb), size=length(cb), prob=probs) #each probability from vector(61) applied -> output(61) 
  # Probability of seeing a number as large as x = sum(cb) = 10, when size(n)=length(cb)=120 trials B(n,p) - P(x=10)
plot(x=probs, y=likelihoodBinom, pch=16, xlab="Probability of Success", ylab="likelihood probability simulation", cex=0.6)
probs[which.max(likelihoodBinom)] #in probs vector which is the closest max value matching likelihoodBinom vector
```

### Likelihood for binomial distribution
  * likelihood and probability are the same function - Have different interpretations
    * Probability to see set of values when given parameters (probability - one p)
    * Data is fixed but what parameter value will fit the data better (likeliness - test multi p)
  * ex) If B(n,p) where n=300 and observe x=40 successes - the binomial distribution
    * where theta = estimate parameter
  
  \begin{equation}
f(\theta\,|\,n,y) = f(y\,|\,n,\theta)={n \choose y} \, \theta^y \, (1-\theta)^{(n-y)}.
\tag{2.5}
\end{equation}
  
    * when {n obs \choose x sucess} large (~e<sup>115</sup>) - then can use log of likelihood for the formula
    
    \begin{equation*}
\log f(\theta |y) = 115 + 40\log(\theta)+(300-40)\log(1-\theta).
\end{equation*}

```{r}
# equivalent function of log binomial

loglikelihoodBinom <- function(theta, n=300, k=40){
  115+(k*log(theta))+((n-k)*log(1-theta))
}
```

```{r}
# Assume theta parameter can range from 0 to 1
thetas = seq(0, 1, by=0.001)
plot(x=thetas, y=loglikelihoodBinom(thetas), xlab=expression(theta), ylab=expression(paste("log f(",theta, " | y)")), type="l")
```

* Maxima here is k/n = 40/300 = 0.133

* **

<p id="7"><b><font size="5">More boxes:multinomial data</b></font><a href="#0"><sup>Return</sup></a></p>
### DNA nucleotides - purines(A G) & pyrimidines (C T)
  * Binomial model works for purines/pyrimidines as sets but not all together (ACGT) -> needs multinomial model
### Nucleotide bias
  * Estimation testing + Simulation
```{r}
# Load set of seqs
staph <- readDNAStringSet(here("BookStuff", "data", "staphsequence.ffn.txt"), "fasta")
staph[1]
letterFrequency(staph[[1]], letters="ACGT", OR=0)
```
  
* Double brackets in the above code [[i]] is used to extract the ith gene as DNA string -> length 1362 (extract each nt)
  * Single brackets [i] returns DNAStringSet w/ single DNAString -> length 1 (extract sequence)

```{r}
# Similar procedure as 1.8
# to be done
```

* Selection acts on nucelotide frequencies
  * Example question: Do the first 10 genes (seqs) from the staph data come from the same multinomial?
    * AKA do nucleotides occur in the same proportions for the first 10 genes? -yes = no selection, -no = selection
```{r}
#Find each nt freq per each sequence 

letterfreq <- vapply(staph, letterFrequency, FUN.VALUE=numeric(4), letters="ACGT", OR=0) #to vector staph apply the function letterFrequency count for letters "A|C|G|T" & output counts as numeric with length 4 
colnames(letterfreq) <- paste("gene", seq(along=staph)) #replace the column names of letterfreq vector w/ gene+number in staph vector
tab10 <- letterfreq[,1:10] # subset to columns 1 to 10 (gene 1 to 10)
computeproportions <- function(x){x/sum(x)} #make function that takes x and divides by sum of x (takes single value & divides by column or row totol depending on margin of next function)
prob10 <- round(apply(tab10, 2, computeproportions), digits=2) # apply to each value in those 10 columns the computeproportions() function made
prob10 #This is the predicted probabilities for each nt in each gene
```

```{r}
# Calculate mean of probabilities
p0 = rowMeans(prob10)
p0
```

* p0 is vector of multinomial probabilities for all 10 genes (seqs) - use a Monte Carlo simulation to test differences between observed letter freqs & expected value range under this simulation is plausible
```{r}
# Compute expected counts via - take outer product of vector of probabilities(p0)
  # Sum nucleotide counts from each of the 10 columns (10 genes)
Csum <- colSums(tab10) #tab10 is the variable where a matrix of counted nucleotides resides - so here we're just counting the total ATCG per gene seqeucne
Csum


#outer product means to mutiply (sum A+C+G+T per gene)*(predecited probabilities from rowMeans of all nucleotides)
  # So predicted probability of Adenosine(A) is 0.346 from rowMeans() * Count of 1362 nts for gene 11 = 471 A's for gene1
expectedtab10 <- outer(p0, Csum, FUN="*")  #outer product of arrays X & Y - FUN="*" is a special case as.vector(X) --???
round(expectedtab10)
```

* Now know predicted probabilities & expected counts of nt per gene
  * Can create random table w/ correct column sums via ```rmultinom``` function
    * **Table assumes that null hypothesis is true (proportions given by p0 is correct)**
```{r}
# The function made in sapply() takes an input, s, and applys rmultinom (probability p0, number of obs=1, size=s)
  # s in this case is Columnsum calculated from above
  # Creates a new table using
    # Csum = vector of summed ATCG per gene sequence
    # p0 = probability predicted per nucleotide per gene
    # So size of Csum is equal to gene1=1362, gene2=1134, gene3= 246, etc 
       # simulate 1 observation per nt using probabilities from p0
       # so function only works on one column of Csum ex) rmultinom(1, size=Csum[1], prob=p0) - where Csum[1] = 1362
          # By using sapply, we can apply this for each column of Csum (10 genes)

randomtab10 = sapply(Csum, function(s){rmultinom(1, size=s, prob=p0)}) #works on list or vector - function called on each item on list/vector
all(colSums(randomtab10)==Csum)  # all() returns logical true or false - compares column sum of above sapply to original columnsum calculated above -> they should equal to the same column sums b/c of the `size` parameter from rmultinom()
```

```{r}
# Chi-squared goodness of fit function from 1.4.1 - needed for the next step of Monte Carlo Simulation
stat = function(obsvd, exptd = 20 * pvec) {
  sum((obsvd - exptd)^2 / exptd)
}
```


```{r}
# Repeat this for 1000 simulations - for each table, find the test statistic using the chi-squared goodness of fit function 
  # Store results in vector "simulstat"
  # This will be the null distribution generated assuming the null hypothesis - p0 predicted probabilites is vector of multinomial proportions for each of the 10 genes


# stimulationofstat replicate() is for Monte Carlo Simulation method
  # replicate 1000 tomes (B)
  # generate a table of simulated values randomtab10.1 using predicted counts of Csum & predicted probabilities
  # chi-squared calculation of observed count values generated from randomtab10.1 (calculated using predicted probabilities), with expected values (expectedtab10) calculated from predicted probabilities(p0)

#S1 is the chi-squared calulation using actual observed count values, and expected values (exptectedtab10) calculated from predicted probabilities(p0)

B <-  1000
simulate1000tablesANDApplychiStat <- replicate(B,{ randomtab10.1 <- sapply(Csum, function(s){rmultinom(n=1, size=s, prob=p0)}) #generate 1000 table simulations
stat(randomtab10.1, expectedtab10)  #apply the stat chi-squared function to all 1000 tables using predicted expected values
})


S1 <- stat(tab10, expectedtab10) # Chi-squared using actual observed counts of ATCG

sum(simulate1000tablesANDApplychiStat>=S1)  #if any chi-squared values generated from the 1000 Monte-Carlo generated tables is >= chi-squared value generated from actual observed counts -> sum all True's
```

```{r}
#Visualize 

hist(simulate1000tablesANDApplychiStat, col="red", breaks=seq(from=0, to=75, length.out=50))
abline(v=S1, col="purple") #add a line for chi-squared calculation from actual ACTG counts
abline(v=quantile(simulate1000tablesANDApplychiStat, probs=c(0.95, 0.99)), col=c("blue", "green"), lty=2) #add a line for 95th and 99th quantiles of the simulationofstat chi-squared using simulated observed count values
```

* from the histogram - probability of seeing value as large as S1=70.1 (blue line - chi-squared value from actual ATCG counts) is small under this null model
  * Occured 0/1000 simulations (no bars at 70.1) -> **therefore, the 10 sequences do not come from the same multinomial model**


* **
<p id="8"><b><font size="5">The χ<sup>2</sup> distribution</b></font><a href="#0"><sup>Return</sup></a></p>
* Can use stats theory for same conclusion w/o using Monte Carlo simulations
  * Distribution of simulate1000tablesANDApplychiStat is approximately X<sup>2</sup> distribution (Chi-Squared)
    * Approximation is good if counts in table are large
    * **Parameter of 30 total for all genes evaluated (=10 genes * (4-1 nts d.f.))**
  * Can compute probability of having value as large as S1=70.1
  * **Monte Carlo problem - doesn't really work for small probabiltiies**
    * Can't estimate probabilites smaller than the simulation (B=1000) so 1/1000 lowest probability estimated -> so estimated values will be larger ("Granularity of Monte Carlo simulations")
    * Therefore, mathematical theories are better
      * Example: Check how theory and simulation matches using the **goodness-of-fit tool (QQ) plot**
        * Compare 2 distributions, with different samples or same samples in different theoretical models - looking at histograms is not informative
        * **QQ plot finds quantiles of each distribution**
### Quntiles & quantile-quantile plot
  * ex) previous chapter - used order statistics to order 100 sample values 
    * (Random sample size (n) -> order each random var from smallest exptected success to largest (x<sub>(1)</sub> < x<sub>(2)</sub>, x<sub>(n)</sub>))
    * Find 22th percentile 
      * Take any value between the 22nd & 23rd value (x<sub>22</sub><=c<sub>0.22</sub><x<sub>23</sub> = 0.22 quantile c)
      * so c<sub>0.22</sub> defined by 
      
\begin{equation*}
\frac{\# x_i's \leq c_{0.22}}{n} = 0.22.
\end{equation*}
    
      * In chapter 3 -> emperical cumulative distribution function \hat{F}
        * Which can define c<sub>0.22</sub> as \hat{F}_n(c_{0.22}) = 0.22
  * In the simulate1000tablesANDApplychiStat variable, we calculated the c<sub>0.95</sub> & c<sub>0.99</sub> quantiles

# <mark style='background-color:red'>Skipped for now</mark>

```{r}

# Compare simulate1000tablesANDApplychiStat chi-squared values to 1000 generated simulations of chi-squared with parameter 30
  # Display in histograms w/ 50bins each chi-squared value

# Compute quantiles of simulate1000tablesANDApplychiStat chi-squared values & compare to chi-squared distribution w/ parameter 30

#qs <- ppoints(100)
#quantile(simulstat, qs)
#quantile(qchisq(p=qs, df=30), qs)
```

* Q) Another aname for the 0.5 quantile?
  * A) The median
  
* Q) How is the quantile computed for any number between 0 & 1 - include numbers not multiple of 1/n
  * A) Check manual for `?quantile` and the arguement `type`
    * Can make a QQ-plot
```{r}
# ex) plot quantiles of stimulate100tablesANDApplychiStat values
qqplot(qchisq(ppoints(B), df=30), simulate1000tablesANDApplychiStat, main="",
       xlab = expression(chi[nu==30]^2), asp=1, cex=0.5, pch=16)
abline(a=0,b=1,col="red")
```
* As we can see, the simulate100tablesANDApplychiStat values fit well using the chisq (df=30) distribution
  * Can now compure P-value (probability under the null hypothesis)
    * Counts are distributed as multinomial w/ probabilities Pa=0.35, PC=0.35, PG=0.2, PT=0.3
      * Observe a value as high as S1=70.1
```{r}
1-pchisq(S1, df=30)
```
* Small p-value indicates to reject the H0

* **
<p id="9"><b><font size="5">Chargaff’s Rule</b></font><a href="#0"><sup>Return</sup></a></p>

* Tetranucleotide hypothesis
  * Used MW of molecules to see if nucleotides occured at equal freqs (P(A)=P(G)=P(C)=P(T))
```{r}
#Reported in percentages
load(here("BookStuff", "data", "ChargaffTable.RData"))
ChargaffTable
```

* <mark style='background-color:red'>Q) Skipped</mark>

* Pattern visualized by Chargaff is base pairing for matching frequencies of AT & CG = Chargaff's Rule
  * Statistic = ((P(C)-P(G))<sup>2</sup>+(P(A)-P(T))<sup>2</2> -> summed over all rows of a table
* What if there as no relationships between the base pairs? - assume nt's are exchangeable w/ no order (independent) --> H0
```{r}
# Make Chargaff stat function, where x is the table of counts
statChargaff <- function(x){sum(
  (x[,"C"]-x[,"G"])^2 + (x[,"A"]-x[,"T"])^2
)}

chfstat <- statChargaff(ChargaffTable) #apply chargaff stat to original table

#Monte carlo simulation x100,000 times
permstat <- replicate(100000, {
  permuted <- t(apply(ChargaffTable,1,sample)) #take random sample from the rows of Chargaff table(Default replace=FALSE) - t(transpose row/col)
  colnames(permuted) <- colnames(ChargaffTable) #set the column names of permuted to equal ChargaffTable
  statChargaff(permuted) #calculate base pairing
})
```


```{r}
pChf <- mean(permstat <= chfstat)  #compare simulation to original
pChf
```
```{r}
hist(permstat, breaks=100,main="100,000 simulation of Chargaff", col="red")
abline(v=chfstat, lwd=2, col="blue") # Chargaff stat calculated using non-simulated table
```

* Histogram indiactes it's rare to have a value as small as the blue line (chfstat = 11.08)
  * Probability of observing a value as small as pChf = 1.6x10^-4
    * significant to reject the null, so Ha means that nt's are not exchangeable & have a relationship (dependent)

* <mark style='background-color:red'>Q) only looked at one-side (values smaller than observed) b/c.........</mark>

### Two categorical variables 
  * From ch1 until now, the data has been binomial (yes/no binary), or multinomuial (ATCG, genotypes)
  * What if we want to measure >=2 categorical variables on a set of subjects?
    * ex) Eye color + hair color
      * Cross tabulate counts for each eye color/hair color combinations = **Contingency table (table of counts)**
```{r}
HairEyeColor[,,2] #[eye color total, hair color total, 1=male 2=female]
str(HairEyeColor) # numeric array w/ 3 dimensions
```

* Color blindness & sex
```{r}
# Red-green colourblindless - Deuteranopia data
load(here("BookStuff", "data", "Deuteranopia.RData"))
Deuteranopia
```

* Test if sex is related w/ colourblindness
  * H0 - no relationship between sex & colourblindness (2 independent binomials - sex & colourblindness)
    * estimate multinomial probabilities & compare observed vs expected counts via ```chisq.test()``` function
```{r}
chisq.test(Deuteranopia)
```

* Small p-value indicates - low proability of seeing observed counts under H0 model
  * -> Again, H0 = model assumes fraction of colourblindness among male/female is the same (no relationship)
  * Can also test w/ **Fisher's exact test (hypergeometric test)** - seen in CH10
    * Used to test for overrepresentation of certain types of genes
    
### Hardy-Weinberg equilibrium (Special multinomial)
  *<mark style='background-color:yellow'>A multinomial with 3 possible levels via combinding two alleles (M & N)</mark>
    * p = overall frequency of allele M in population
    * q = N allele = 1-p
  * Hardy-Weinberg model -> looks for **genotype independent frequency of alleles** relation between p & q
    * ex) Random mating in large population w/ 3 possible genotypes
    <center> p<sub>MM</sub> = p<sup>2</sup>, p<sub>NN</sub> = q<sup>2</sup>, p<sub>NM</sub> = 2pq</center>
    * Observed frequencies (n<sub>MM</sub>,n<sub>MN</sub>,n<sub>NN</sub>) = genotypes MM, MN, NN
      * Total number S = n<sub>MM</sub>+n<sub>MN</sub>+n<sub>NN</sub>
      * Multinomial Likelihood  formula (Probability of seeing observed data when probabilities of the frequencies given)
      
      \begin{equation*}
P(n_{\text{MM}},\,n_{\text{MN}},\,n_{\text{NN}}\;|\;p) =
{S \choose n_{\text{MM}},n_{\text{MN}},n_{\text{NN}}}
(p^2)^{n_{\text{MM}}} \,\times\,
(2pq)^{n_{\text{MN}}} \,\times\,
(q^2)^{n_{\text{NN}}},
\label{eq:likHWE}
\end{equation*}

      * Log-likelihood form of Hardy-Weinberg formula
      
\begin{equation*}
L(p)=n_{\text{MM}}\log(p^2)+n_{\text{MN}} \log(2pq)+n_{\text{NN}}\log(q^2).
\label{eq:loglikHWE}
\end{equation*}

      * p value to maximize log-likelihood
      
\begin{equation*}
p = \frac{n_{\text{MM}} + n_{\text{MN}}/2}{S}.
\end{equation*}

* If given data of observed frequencies (n<sub>MM</sub>,n<sub>MN</sub>,n<sub>NN</sub>), the log-likelihood(L) is a function with a parameter p (q doesn't matter b/c 1-p)

```{r}
# Test HW log-likelihood function for different p-values in a dataset of blood group alleles

library("HardyWeinberg")
data("Mourant")
nMM <- Mourant$MM[216] #subset only row 216 counts
nMN <- Mourant$MN[216]
nNN <- Mourant$NN[216]

Mourant[214:216,] #short view of dataset from rows 214 to 216
```

```{r}
# Make the HW-log function

loglikHW <- function(p,q=1-p){
  2*nMM*log(p) + nMN*log(2*p*q) + 2*nNN*log(q)
}
```

```{r}
xv <- seq(from=0.01,to=0.99, by=0.01) #vector of probabilities
p <- loglikHW(xv) #insert vector of probabilities into the function
plot(x=xv, y=p, type="l", lwd=2, xlab="probabilities", ylab="log-likelihood of probabilities")
imax=which.max(p) #position of highest (+)log-likelihood value
abline(v=xv[imax], h=p[imax], lwd=1.5, col="blue") #xv[imax] = in probability vector which is position imax(58)
abline(h=p[imax], lwd=1.5, col="purple")
```

* Maximum likelihood estimate for probabilities in multinomial can be observed via observed freqs in binomial model, but estimates in binomial need to account for relationships between all 3 probabilities 
  * <span class="math inline">\(\hat{p}_{MM}, \hat{p}_{MN}, \hat{p}_{NN}\)</span> 

```{r}
# af() function in HardyWeinberg package to calculate probabilities when counts are known
phat <- af(c(nMM,nMN, nNN))
phat
```

```{r}
pMM <- phat^2
qhat <- 1-phat
pHW <- c(MM=phat^2, MN=2*phat*qhat, NN=qhat^2)
sum(c(nMM,nMN,nNN)) * pHW  #observed values
```

```{r}
#compare to original expected values
Mourant[216,]
```

* Can further test if observed values can be used to reject H0 (Hardy-Weinberg model) via simulation or chi-squared test as previous.
  * There is a visual evaluation of goodness of fit for Hardy-Weinberg (ternary plot)
    * Sample points @coordinates given by proportions of each different allele
  * ```HWTenaryPlot()``` function
```{r}
pops <- c(1,69,128,148.192) #specific column numbers (countries) used to subset in HWTernaryPlot() function
genotypeFrequencies <- as.matrix(Mourant[,c("MM","MN","NN")])
HWTernaryPlot(genotypeFrequencies[pops,],
              markerlab=Mourant$Country[pops],
              alpha=0.0001, curvecols=c("red", rep("purple",4)),
              mcex=0.75, vertex.cex=1)
```

* The red line is the HW-model, between the 2 purple lines is the region where HW-model (H0) can't be rejected
  * Countries farther away from red line = farther from HW equilibrium

```{r}
# Make ternary plot & add other data points
HWTernaryPlot(genotypeFrequencies[155,], alpha=0.0001, newframe=TRUE, cex=0.5)
HWChisq(genotypeFrequencies[155,])
```

```{r}
# Divide all total frequencies by 50 - keep same porportion for each genotypes & re-make ternary plot
newgfHW <- round(genotypeFrequencies/50)
HWTernaryPlot(newgfHW[pops,],
              markerlab=Mourant$Country[pops],
              alpha=0.0001, curvecols=c("red", rep("purple",4)),
              mcex=0.75, vertex.cex=1)
```

* Points stay the same but confidence regions have a wider spread b/c count frequencies decreased (but proportions remained)?

### Concatenating several multinomials - sequence motif & logos
  * Kozak motif = seq occuring close to start codon(ATG) of coding ORF
    * nt position#1 to 5 left to ATG -> differs in nucleotide (not equally likely)
  * **Position weight matrix/Position-specific scoring matrix** = multinomial probabilities at every position of a seq
    * Visualized via sequence logo
```{r}
#BiocManager::install("seqLogo")
library(seqLogo)
load(here("BookStuff", "data", "kozak.RData"))
pwn <- makePWM(kozak)
seqLogo(pwn, ic.scale=FALSE)
```

* Multinomial decisions("boxes") rarely have equal probabilities/frequencies
  * Parameters p1, p2,.., pn differ depending on data (ex) 20 AA's, blood types, hair colour)
  * If multiple categorical variables, they are rarely indepdendent (ex) sex & cb, hair & eye colour)
    * More in CH9 - multivariate decomposition of contingency tables

* **
<p id="10"><b><font size="5">Modeling sequential dependencies: Markov Chains</b></font><a href="#0"><sup>Return</sup></a></p>
* Special case of dependencies between categorical variables - along a seq/chain of categorical variables
* Predict tomorrow's/yesterday's weather using probabilities for changes. 
  * Markov assumption: Predicting tomorrow's/yesterday's weather <u>depends on state of current day</u>
    * Assumption doesn't need to be true but it should be close
    * Can extend assumption to depend on previous days (k), where k is a limited number
      * Assumption based on limited memory so can predictions rely on limited previous days
* ex) Biological seqs - specific succession of nt pair patterns = digrams (frequency CG =/= CA =/= CC =/= CT)
  * Some parts of genome are more frequent in CA  (not independent)
    * P(CA) =/= P(C)P(A)
  * Dependency modelled as Markov chain
    * P(CA) = P(nCA) = P(nnCA) = P(...CA) = P(C)P(A|C)
      * n = any nucleotide
      * P(A|C) = probability of A given that the base before is C
  
* **
<p id="11"><b><font size="5">Bayesian Thinking</b></font><a href="#0"><sup>Return</sup></a></p>

* Bayesian Thinking 
  * Classical approach parameters for distrabutions - probabilities of different outcomes represent long-term frequencies
    * Parameters are definite & fixed numbers
    * If not known, estimate from data
    * Problem: **Doesn't account for information already known (priors)**
      * Constains parameters, or makes certains parameters more likely
  * Bayesian approach - uses probability distributions to show knowledge about parameters, where data updates knowledge (shift distribution/make more narrow)
    * **Prior (before)** & **Posterior (after)** data distirubtions used as models 
      * Useful to combine data from different sources
    * ex) Hypothesis(H) - use data to test if hypothesis is true
      * prior knowledge of data (before seeing data)= prior probability P(H)
      * after knowledge of data (after seeing data)= posterior probability P(H|D) - probability of H given that D was seen
        * posterior probability can be higher/lower than prior P(H) - depends on data D
### Haplotypes
  * foresnics - signatures from Y-chromosomes = haplotypes -> collection of alleles (DNA-seq variants) adjactent on a chromosome
    * Usually inherited together (low recombination separation) -> genetically linked
  * Ex) Looked for linked variants on Y-chromosome
    * Short tandem repeats = signature for haplotypes - usually SNP located upstream

```{r}
# Estimate porportion of Y-haplotype w/ set of short tandem repeat signatures
  # repeats labelled by number of repeats & postion

haplo6 <- read.table(here("BookStuff", "data", "haplotype6.txt"), header=TRUE)
haplo6
```

* Interpretation of table
  * haplotype H1 has 14 repeats @position DYS19, 12 repeats @position DXYS156Y, etc 
  * Find proportion (<span class="math inline">\(\theta\)</span>) of haplotype of interest in population
    * Assume occrence of haplotype = "success" in binomial distribution
    
* Simulate haplotype of Bayesian approach for binomial distribution model
  * Instead of guessing single parameter value <span class="math inline">\(\theta\)</span> - Bayesian approach shows parameter as a draw from a statistical distribution.
    * Chosen distribution shows believed values of the parameters
    * Can use any distribution - but the values should be representitive for parameter
      * Parameter should express proportion or probability, taking a value between 0-1
        * **Beta-distribution** - density formula to guess parameter theta

\begin{equation*}
f_{\alpha,\beta}(x) = \frac{x^{\alpha-1} (1-x)^{\beta-1}}{\text{B}(\alpha, \beta)} \quad\text{where}\quad
\text{B}(\alpha ,\beta)=\frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}
\end{equation*}

         * Beta-distributition function needs 2 parameters (a & B)  
           * Can fit many different data  
           * Math property - start w/ prior belief that parameter <span class="math inline">\(\theta\)</span> is beta-shaped  
             * Then observe dataset of n binomial trials B(n,p) -> size for rbinom()  
               * Use this to update belief = posterior on <span class="math inline">\(\theta\)</span> also beta-shaped but w/ updated parameters  
           * Ex) B(10,30), B(20,60), B(50,150) -> alpha = 10,20,60 & beta=30,60,150 (priors)

### Distrubtion of Y
  * For any single choice of <span class="math inline">\(\theta\)</span>, distribution Y follows the likelihood for binomial distribution formula
  
  \begin{equation}
f(\theta\,|\,n,y) = f(y\,|\,n,\theta)={n \choose y} \, \theta^y \, (1-\theta)^{(n-y)}.
\tag{2.5}
\end{equation}

```{r}
# What if theta varies among distribution Y = Marginal distribution
  # simulate 100,000 theta values via vapply to all elements of beta-distribution simulation of rtheta
rtheta <- rbeta(n=100000, shape1=50, shape2=350) #simulate probabilities for theta

y <- vapply(rtheta, function(th){ rbinom(1,prob=th,size=300)}, numeric(1)) #use the theta probabilities in binomial distirbution for observed values
hist(y, breaks=50, col="orange", main="Marginal distribution of Y", xlab="") #histogram for viewing observed values
```

```{r}
# Verify alternative function

y1 <- rbinom(length(rtheta), prob=rtheta, size = 300)  #alternative way without using vapply for each individual probability (length=100000)
hist(y1, breaks=50, col="orange", main="Marginal distribution of Y", xlab="")
```

### Histogram of all theta probabilities, such that Y=40 (posterior distribution)
  * Calculate posterior values of parameter <span class="math inline">\(\theta\)</span>
    * Condition the outcomes such that Y=40
      * Compare values to theoretical posterior (```densPostTheory```)
```{r}
# When posterior value = 40
thetaPostEmp <- rtheta[y==40] #y==40 is theta probabilities when prior y=40
hist(thetaPostEmp, breaks=40, col="red", main="Posteriror distribution when Y=40", probability=TRUE,
  xlab=expression("posterior"~theta))

# Compare to theoretical theta posterior densPostTheory
  # note dbeta not rbeta simulation
  #thetas = vector of values from 0 to 1 created above
densPostTheory <- dbeta(x=thetas, shape1=90, shape2=610) #what does shape mean and where do these numbers come from?
lines(thetas, densPostTheory, type="l", lwd = 3) #lines() is basically a connected scatter plot - needs a histogram to work on
```

```{r}
# Check mean of 100,000 simulation distribution of posterior theta to theoretical theta
mean(thetaPostEmp)
```
```{r}
# Check mean if theoretical posterior theta generated via dbeta
dtheta <- thetas[2]-thetas[1]   
sum(thetas*densPostTheory*dtheta) # How does this eqn work???? - Oh, they calculated the integral
```
* Calculating the mean for the theoretical posterior theta (densPostTheory) uses the integral calculation (numerical integration)
  * sum over integrant
  * **This calculation is not always feasible** - ex) <u>if parameters have many dimensions (>1 scalar theta parameter)</u>
    * Solution: Use Monte Carlo integration

```{r}
# Monte Carlo integration instead of calculating mean of posterior
thetaPostMC <- rbeta(n=1000000, shape1=90, shape2=610) #simulate theta probabilities 
mean(thetaPostEmp)
```

```{r}
# Check Monte Carlo simulation (thetaPostMC) vs (thetaPostEmp) via QQplot
  # alternatively can compare either thetaPostMC or thetaPostEmp simulations to pbeta(q=,shape1=90,shape2=610)
  # If QQ plot curve overlaps the line y=x (abline) -> good agreement
    # Tails will have random differences 

qqplot(x=thetaPostMC, y=thetaPostEmp, type="l", asp=1)
abline(a=0, b=1, col="blue") #a=intercept, b=slope
```

* Q) Difference between ```thetaPostEmp``` vs ```thetaPostMC``` simulation? -> shape1/2 parameters whatever those mean (oh, alpha and beta parameters of beta distribution)

### Posterior distribution is also a beta distribution
  * parameters a=90, b=610
    * Obtained via summing the prior parameters (a=50, b=350) w/ observed successes y=40 + observed failures (n-y=260)
       * n=220
       * Output is the posterior value
       
\begin{equation*}
\text{beta}(90,\, 610)=\text{beta}(\alpha+y,\beta+(n-y)).
\end{equation*}

         * Gives best estimate for theta based upon uncertainty of posterior distirbution 
         
* Maximixe posterior distribution estimate via MAP (maximize approximate posterior?)

\begin{equation*}
\frac{\alpha-1}{\alpha+\beta-2}=\frac{89}{698}\doteq 0.1275
\end{equation*}

* New scenario
  * First series of data
    * prior beta(90,610)
      * a=90
      * b=610
  * New second series of data
    * n=150 obs and y=25 successes, so 150-25= 125 failures
    * guess theta

\begin{equation*}
\text{beta}(\alpha+y,\beta+(n-y)).
\end{equation*}

\begin{equation*}
\text{beta}((90+25)=115,\, (610+125)=735)
\end{equation*}

* Mean of distribution is 

\begin{equation*}
\frac{\alpha+y}{(\alpha+y)+(\beta+(n-y))}
\end{equation*}

\begin{equation*}
\frac{115}{115+735}=\frac{115}{850}\simeq 0.135
\end{equation*}

* The theoretical maximum a posteriori (MAP) estimate of beta(115,735) is 

\begin{equation*}
\frac{\alpha-1}{\alpha+\beta-2}=\frac{114}{848}\simeq 0.134
\end{equation*}

```{r}
# check

denspost2 <- dbeta(thetas, 115, 735) #distribution
mcpost2 <- rbeta(1e6, 115,735) #monte carlo simulation

sum(thetas*denspost2*dtheta) # mean calculation via numeric integration
```
```{r}
# mean by Monte Caralo simulation

mean(mcpost2)
```

```{r}
#MAP estimate
  # uses Monte Carlo method to find MAP from sample of rbeta(.,115,735)

thetas[which.max(denspost2)]
```

* Q) If re-did computations using a lower peak prior (less prior information), how does this affect the results?
  * A) Prior's will rarely change posterior distribution unless highly peaked
    * ex) highly sure of expected outcome at the beginning
    * or prior's will change posterior if low dataset -> best to have high data for prior so it doesn't impact final results as much
    * low likelihood maxima (wider/less peaked) = variable posterior (highly influenced by prior)
    
### Confidence Statements for proportion parameter
  * Conclude where proportion lies in given data
    * Summary1: posterior credibility interval (Bayesian analog of confidence interval)
      * take 2.5 & 97.5th percentiles of posterior distribution P(L<=theta<=U) = 0.95

```{r}
quantile(mcpost2, c(0.025,0.975)) #from monte carlo simulation
```

* **
<p id="12"><b><font size="5">Example: occurrence of a nucleotide pattern in a genome</b></font><a href="#0"><sup>Return</sup></a></p>
* Examples so far have focused on distributions w/ discrete counts + categorical data
* Now, distribution of quasi-continous distances (distributions of distances between specific motifs in genome)

```{r}
# Biostrings package = tools for working w/ sequence data
  # classes -> DNAString & DNAStringSet
library(Biostrings)

#browseVignettes("packagename")
#browseVignettes("Biostrings")
#browseVignettes("phyloseq")
```

```{r}
#vignette(package="Biostrings")
#vignette("BiostringsQuickOverview", package="Biostrings")
#vignette()
```

* Biostrings
  * 1st line -> genetic code
  * 2nd line -> IUPAC nucleotide ambiguity code
  * 3rd line -> lists all vignette available in Biostrings package
  * 4th line -> displays particular vignette
  
```{r}
# BSgenome package - many whole genome seqs which can be accessed 

# BiocManager::install("BSgenome")
library(BSgenome)
ag <- available.genomes()
length(ag)

ag[1:2]

```

* Want to look for "AGGAGGT" motif frequency in E.coli strain K12 genome
```{r}
# BiocManager::install("BSgenome.Ecoli.NCBI.20080805")
library(BSgenome.Ecoli.NCBI.20080805)
Ecoli
RBS <- "AGGAGGT"
ecoli <- Ecoli$NC_010473
```

```{r}
# Count motif pattern frequency in length of 50,000nt via countPattern

window <- 50000
starts <- seq(1, length(ecoli)-window, by=window)
ends <- (starts+window)-1

numMatches <- vapply(seq_along(starts), function(i){countPattern(RBS, ecoli[starts[i]:ends[i]], max.mismatch = 0)}, numeric(1))

table(numMatches)
```

* Q) What distribution might fit the table 
```{r}
# A) Poisson

library(vcd)
gf <- goodfit(numMatches, "poisson")
summary(gf)

distplot(numMatches, type="poisson")
```

```{r}
# Inspect matches via matchPattern()
sdMatches <- matchPattern(pattern=RBS, subject=ecoli, max.mismatch = 0)
sdMatches
```
```{r}
# Check distances between the matches/patterns

betweenmotifs <- gaps(sdMatches)
betweenmotifs
```

* 66 complementary regions 

* Find model for the gap sizes between motifs distribution
  * Motifs occur at random locations -> gap lengths should follow an exponential distribution
    * why guess exponential distribution? -> Independent Bernoulli occurances along seq (gap/no gap?) then gap lengths exponential
      * Radioactive decay (times between emission = exponentially distributed)
```{r}
# Assess if exponential distribution is a good fit for gap lengths
  # Points should lie on s8 line
  # exponential distributions have one parameter (rate), line of slope = estimate from data

#install.packages("Renext")
library(Renext)
expplot(width(betweenmotifs), rate=1/mean(width(betweenmotifs)),labels="fitted exponential")
```

* Q) Why is there a slight deviation at the right tail (upper limit) of data?
  * Gap lengths in further positions less closely distributed??
  
### Modeling in the case of dependencies
  * nt seqs are often dependent - prob of seeing certain nt at position depends on surrounding seq
  * Dependency modelling using **Markov Chain**
    * ex) Try to find difference between CpG island & non-CpG island from chromosome
```{r}
# BiocManager::install("BSgenome.Hsapiens.UCSC.hg19")
  # has the start/end points of CpG islands - want to look at nucleotide freqs (CG, CT, CA, CC) to see dependencies

library("BSgenome.Hsapiens.UCSC.hg19")
chr8 <- Hsapiens$chr8
CpGtab <- read.table(here("BookStuff", "data", "model-based-cpg-islands-hg19.txt"), header=TRUE)
nrow(CpGtab)

head(CpGtab[CpGtab$chr=="chr8",])
```
    
```{r}
# subset data (CpGtab) using filter() for only chromosome 8. Create IRanges object w/ start/end positions defined by column names of CpGtab data
  # Specifically call 'filter' from dplyr via "::" b/c many other packages have a filter function
  # IRanges is a container for mathemateical cintervals (groups interval - "I" = interval, "G" = Genomic Ranges)
    # start/end = column from dataframe subsetted by filter


irCpG <- with(dplyr::filter(CpGtab, chr=="chr8"), IRanges(start=start, end=end))
irCpG
```
```{r}
grCpG <- GRanges(ranges=irCpG, seqnames="chr8", strand="+")
grCpG

genome(grCpG) <- "hg19" # changes the bottom line "unspecified genome" to "hg19"
```

```{r}
# Visualization of CpG locations in selected region of human chromosome 8

# BiocManager::install("Gviz")
library(Gviz)
ideo <- IdeogramTrack(genome="hg19", chromosome="chr8")
plotTracks(list(GenomeAxisTrack(), AnnotationTrack(grCpG, name="CpG"), ideo), from = 2200000, to = 5800000, shape= "box",
          fill = "#006400", stacking = "dense")
```

* CpG island locations on chr8 -> `irCpG`
* Regions between CpG island -> `gaps(irCpG)`
* `CGIview` & `NonCGIview` are coordinates + seq combined

```{r}
CGIview <- Views(unmasked(Hsapiens$chr8), irCpG)
NonCGIview <- Views(unmasked(Hsapiens$chr8), gaps(irCpG))

CGIview
NonCGIview
```

```{r}
# Compute transition counts in CpG island & non-island

seqCGI      = as(CGIview, "DNAStringSet")
seqNonCGI   = as(NonCGIview, "DNAStringSet")

dinucCpG    = sapply(seqCGI, dinucleotideFrequency)
dinucNonCpG = sapply(seqNonCGI, dinucleotideFrequency)

dinucNonCpG[, 1]
```

```{r}
#sum up each of the dinucleotide counts

NonIslandCounts = rowSums(dinucNonCpG)
IslandCounts  = rowSums(dinucCpG)
```

```{r}
# 4 State Markov Chain
  # Transition matrix -> matrix where rows=from state, and columns=to state

TIsl <- matrix(IslandCounts, ncol=4, byrow=TRUE)
TnonIsl <- matrix(NonIslandCounts, ncol=4, byrow=TRUE) 
dimnames(TIsl) = dimnames(TnonIsl) =
  list(c("A", "C", "G", "T"), c("A", "C", "G", "T"))

TIsl
TnonIsl
```

```{r}
# Use counts of #of transitions for each type -> compute frequencies and put into 2matracies

MIsl <- TIsl/rowSums(TIsl)
MIsl
```

```{r}

MNonIsl <- TnonIsl/rowSums(TnonIsl)
MNonIsl
```

* Q) Transitions different per row?
  * ex) P(A|C) =/= P(A|T)
  * A) Yes, see MIsl -> P(A|C) "C to A"=0.201 vs P(A|T)=0.098
  
* Q) Frequencies of nts different in CpG island vs nonIsland?

```{r}
freqIsl = alphabetFrequency(seqCGI, baseOnly = TRUE, collapse = TRUE)[1:4]
freqIsl / sum(freqIsl)
```

```{r}
freqNon = alphabetFrequency(seqNonCGI, baseOnly = TRUE, collapse = TRUE)[1:4]
freqNon / sum(freqNon)
```
* Inverse pattern in CpG island seen (C/G freq ~0.32 in island vs ~0.19 in nonIsland)

* Q) Use differences to decide if given seq comes from CpG island?
  * Use Chi-squared statistic to compare freqs observed `freqIsl` vs `freqNonIsl`
    * Chi-squared may not be sensitive for shorter seqs (use below formula)

* ex) Given a seq which we don't know if it's a CpG island, what is the probability of the seq being a CpG island?
  * Compute score based on **odds ratio**
    * SeqX = ACGTTATACTACG
      * If modelled as first order Markov chain - assuming seq comes from CpG island
        * first order -> 1st nt = A, slide over = AC, slide over = CG, GT, TT, TA, AT, etc (slide one nt frame until end)

\begin{align}
P_{\text{i}}(x = \mathtt{ACGTTATACTACG}) = \;
&P_{\text{i}}(\mathtt{A})\, P_{\text{i}}(\mathtt{AC})\, P_{\text{i}}(\mathtt{CG})\, P_{\text{i}}(\mathtt{GT})\, P_{\text{i}}(\mathtt{TT}) \times \nonumber\\
&P_{\text{i}}(\mathtt{TA})\, P_{\text{i}}(\mathtt{AT})\, P_{\text{i}}(\mathtt{TA})\, P_{\text{i}}(\mathtt{AC})\, P_{\text{i}}(\mathtt{CG}). \tag{2.7} 
\end{align}   

* Compare this probability to probability of non-CpG island
  * Take ratios & see if > or < 1 == **log-likelihood ratio score**
  
  \begin{align}
\log&\frac{P(x\,|\, \text{island})}{P(x\,|\,\text{non-island})}=\nonumber\\
\log&\left(
\frac{P_{\text{i}}(\mathtt{A})\, P_{\text{i}}(\mathtt{A}\rightarrow \mathtt{C})\,
P_{\text{i}}(\mathtt{C}\rightarrow \mathtt{G})\,
P_{\text{i}}(\mathtt{G}\rightarrow \mathtt{T})\, P_{\text{i}}(\mathtt{T}\rightarrow \mathtt{T})\, P_{\text{i}}(\mathtt{T}\rightarrow \mathtt{A})}
{P_{\text{n}}(\mathtt{A})\, P_{\text{n}}(\mathtt{A}\rightarrow \mathtt{C})\, P_{\text{n}}(\mathtt{C}\rightarrow \mathtt{G})\,
P_{\text{n}}(\mathtt{G}\rightarrow \mathtt{T})\, P_{\text{n}}( \mathtt{T}\rightarrow  \mathtt{T})\, P_{\text{n}}( \mathtt{T}\rightarrow \mathtt{A})} \right. \times\nonumber\\
&\left.
\frac{P_{\text{i}}(\mathtt{A}\rightarrow \mathtt{T})\, P_{\text{i}}(\mathtt{T}\rightarrow \mathtt{A})\, P_{\text{i}}(\mathtt{A}\rightarrow \mathtt{C})\,
P_{\text{i}}(\mathtt{C}\rightarrow \mathtt{G})}
{P_{\text{n}}(\mathtt{A}\rightarrow \mathtt{T})\, P_{\text{n}}(\mathtt{T}\rightarrow \mathtt{A})\, P_{\text{n}}(\mathtt{A}\rightarrow \mathtt{C})\, P_{\text{n}}(\mathtt{C}\rightarrow \mathtt{G})} \right) \tag{2.8} 
\end{align}

* This is equivalent to calculating by summing each individual log ratio 
\begin{align}
\ Σ log(P_{\text{i}}(\mathtt{A})/P_{\text{n}}(\mathtt{A})),..., \log(P_{\text{i}}(\mathtt{T}\rightarrow \mathtt{A})/P_{\text{n}}(\mathtt{T}\rightarrow \mathtt{A}))
\end{align}

```{r}
# Log-likelihood ratio with R

alpha <- log((freqIsl/sum(freqIsl))/((freqNon)/sum(freqNon)))
beta <- log(MIsl/MNonIsl)
x <- "ACGTTATACTACG"
```

```{r}
LogLScore <- function(x){
  s = unlist(strsplit(x, ""))
  score = alpha[s[1]]
  if (length(s)>=2)
    for (j in 2:length(s))
      score = score+beta[s[j-1],s[j]]
  score
}
```

```{r}
#Run log-likelihood score function on sequence X

LogLScore(x)
```


* Now pick sequences of length 100 out of the 2855 seqs in `seqCGI` & `seqNonCGI` (CpG & non island in chr8)
  * Make a function `generateRandomScores`
    * first 3 lines -> remove sequences w/ nts other than "ATCG" (ex) ambiguous N or . for undefined)
    * Then calculate probabilities of remaining seqs (length-100)
    * Then pick subsequences of length 100 out of the remaining seqs
      * start pts of subseqs sampled uniformly (constraint -> subseqs have to fit in)
```{r}
generateRandomScores = function(s, len = 100, B = 1000) {
  alphFreq = alphabetFrequency(s)
  isGoodSeq = rowSums(alphFreq[, 5:ncol(alphFreq)]) == 0
  s = s[isGoodSeq]
  slen = sapply(s, length)
  prob = pmax(slen - len, 0)
  prob = prob / sum(prob)
  idx  = sample(length(s), B, replace = TRUE, prob = prob)
  ssmp = s[idx]
  start = sapply(ssmp, function(x) sample(length(x) - len, 1))
  scores = sapply(seq_len(B), function(i)
    LogLScore(as.character(ssmp[[i]][start[i]+(1:len)]))
  )
  scores / len
}
```


```{r}
scoresCGI    <-  generateRandomScores(seqCGI)
scoresNonCGI <-  generateRandomScores(seqNonCGI)

head(scoresCGI)
head(scoresNonCGI)
```

```{r}
br = seq(-0.6, 0.8, length.out = 50)
h1 = hist(scoresCGI,    breaks = br, plot = FALSE)
h2 = hist(scoresNonCGI, breaks = br, plot = FALSE)
plot(h1, col = rgb(0, 0, 1, 1/4), xlim = c(-0.5, 0.5), ylim=c(0,120))
plot(h2, col = rgb(1, 0, 0, 1/4), add = TRUE)
```

* This is known as the **training data** = information from orignal data (types)
  * Use to see if score is useful for discrimination
    

* **
<p id="13"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>


```{r}
#2.1

ex1 <- rbinom(n=1000, prob=1e-4, size=1000)
ex1s <- sum(ex1)
ex1sg <- goodfit(ex1, "binomial")
rootogram(ex1sg, xlab="Levels" ,rect_gp=gpar(fill="green"), main="Binomial good-fit test")
```

```{r}
# 2.2

ex2 <- function(n){
  runif(n=n, min=0, max=7)
}

ex2(25)

# Monte Carlo simulate
ex2m <- replicate(100, {max(ex2(25))})
hist(ex2m, col="red", xlab="Maxima value of Uniform")

# don't know which MLE formula to use -> poison or binomial? But this is a uniform distribution
```

```{r}
# 2.3
  # note multiplicity = number of codons(3nts) that code for same AA (ranges 2-6)

mtb <- read.table(here("BookStuff", "data", "M_tuberculosis.txt"), header=TRUE)
head(mtb[mtb$AmAcid=="Pro",], n=4) #n=4 -> view 4 rows
```

```{r}
# 2.3 cont
  # Frequency of proline seen

prolineMTB <- mtb[mtb$AmAcid=="Pro", "Number"]
prolineMTB/sum(prolineMTB)
```
```{r}
# 2.3 cont
ex2.3 <- mtb[,c("AmAcid","Codon")]
PerThous <- ????
ex2.3f <- function(table){
  a <- table$AmAcid[which.max(mtb$PerThous)]
}

ex2.3f(ex2.3)
```

```{r}
# 2.4

ex2.4GC <- letterFrequency(staph, letters="CG", OR=0)

staph[1:3]
ex2.4 <- letterFrequencyInSlidingView(staph[[1]], view.width=100, letters="GC")
ex2.4f <- letterFrequencyInSlidingView(staph[[1]], view.width=100, letters="GC")/100

hist(ex2.4f, col="red", xlab="Proportion")
```


```{r}
# 2.5 No clue how beta diversity works
ex2.5 <- rbeta(n=5, shape1=3, shape2=3)
plot(ex2.5, col="blue")
```

```{r}
# 2.6 B(6.88,8.5)

#Make simulation counts using simulated probabilities
ex2.6theta <- rbeta(1e6, shape1=6.88, shape2=8.5)
ex2.6y <- vapply(ex2.6theta, function(th){ rbinom(1,prob=th,size=300)}, numeric(1)) 
hist(ex2.6y, breaks=50, col="orange", main="Marginal distribution of ex2.6y", xlab="")
```
```{r}
# 2.6 cont.
  # Check when posterior y=40 success

post40 <- ex2.6theta[ex2.6y==40]
hist(post40, breaks=40, col="red", main="Posteriror distribution when Y=40", probability=TRUE,
  xlab=expression("posterior"~theta))

```

```{r}
#2.6 cont QQ-plot

qqplot(thetaPostMC, post40, type="l", asp=1)
abline(a=0, b=1, col="blue")
```
* Not good shape parameters

* **

<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*  

* μ = mean of normal distribution
* θ = multiple parameters for probability model
  * ex) binomial model θ=(n,p) -> 2 numbers n=positive integer & p=real number between 0 & 1
  
* **Observed variables**
  * measured numbers (including gender)-> datafiles
  * discrete or continous
  * Subclassification
    * exogenous variable = ~independent variable -> not controlled by other variables
    * endogenous variable = ~dependent variable -> controlled by other variables
    * Dichotomous -> binary output of 1 or 0
  * Latent vs observed variable
    * Latent = hidden non-observed
    
* **Expected Value**
  * "<u>the output</u> from some action"
    * Expected value of random variable = mean of the random variable
  * Basic formula
    * **(Probability P(x))(#times of event) = expected value **
    * formula changes depending on event
    * Formula for Binomial random variable = basic formula
    * Formula for Multiple events 
      * E(X) = ∑ P(X)(n)
  * Formula for continous random variables   
  ![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2013/09/expected-value-formula.png)

* **Probability Distribution**
  * Tells probability of event happening 
    * simple events (coin toss) & complex events (drug success)
  * Types
    * Basic probability distributions -> probability distribution table
    * Binomial distribution -> pass/fail
    * Normal distribution -> Bell curve
  * Display via table/graph/formula
  * Family/List
    * Discrete probability distribution
      * probability distributions (binomial, normal, Poisson) -> classified as continous or discrete
        * Discrete distribtuion = discrete random variables
      * examples
        * Binomial, Geometric, Hypergeometric, Multinomial, Negative Binomial, Poisson
    * Beta Distribution
      * Represent outcomes for percentages/proportions
      * 3 Betas associated /w function
        * Beta(α, β)= the name of the probability distribution
        * B(α, β ) = the name of function in denomiantor of probability distribution function
          * normalizing constant to ensure area under curve of probability distribution function =1
        * β= the name of the second shape parameter in the probability distribution function  
![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2015/08/beta-distribution.png)
           * > α and β are two positive shape parameters which control the shape of the distribution. 
           * > For example, if α < 1 and Β < 1, the graph’s shape will be a “U” 
           * > if α = 1 and Β = 2, the graph is a straight line
        * so alpha and beta parameters control the shape of this function
    * [Beta-Binomial Distribution & Beta-Bernoulli Distribution](https://www.statisticshowto.datasciencecentral.com/beta-binomial-distribution/)
      * Mixture of beta distribution + binomial distribution 
      * Simplist model for Bayesian statistics 
      * > A distribution is beta-binomial if p, the probability of success, in a binomial distribution has a beta distribution with shape parameters α > 0 and β > 0. The shape parameters define the probability of success. 
        * Large alpha and beta -> approach binomial distribution
        * If alpha and beta = 1 then distribution is discrete(finite) uniform from 0 to n
        * If n=1 then Bernoulli distribution
      * Binomial vs Beta-binomial 
        * Binomial p-value fixed for set of trials
        * Beta-binomial p-value non-fixed and differs from trial to trial
      
* **Monte Carlo Simulation Method**
  * Stochastic Model -> uncertainty is present = model for randomness
  * account for risk when making decision/quantitative analysis
  * Finds all possible outcomes of decision & assess risk
  * Used for statistical sampling methods which are complex
    * Input model simulated >100 times
      * end simulations are equally likely
      * results in probability distribution of possible outcomes
        * one/many different distributions (chi, normal, uniform, etc)
    * Transforms numbers from random number generators -> seqs of transformed numbers repeat after certain #of samples
      * error calculation(mean) does not remove errors
        * Violate Central Limit Theorem & Law of Large #'s
      * **Output of Monte Carlo simulations are approximates (5-10% error)** never precise
    * Tells
      * All possible events that could/will happen
      * Probability of each outcome 
    * Quantified probability
      * Returned by MC simulation -> gives scenarios w/ numbers
    * Accuracy
      * MC can be accurate but never precise because
        * Large amounts of data involved
        * Unknowns in system
        * Probability based -> random prediction = margin of error in results
      * Resons leading to running bad MC simulation
        * Using incorrect model/unrealistic probability distribution
        * Risk factors not complete (unspecified variables)
        * MC isn't suitable for data
        * Random #generator used is not good enough

* **Chi-Square statistic**
  * Can only use on numbers, not precentages, proportions, or mean (ex) bad->10% of 200 people = 20people <-good)
  * 2 types of tests - both use statistic + distribution
    * Goodness of fit test = see if data matches a population
    * test for independence = compare 2 variables in contingency table & see if related (see if distributions of categorical vars differ)
      * Small chi test statistic =  observed data fits expected data well (relationship)
      * Large chi statistic = data doesn't fit well (no relationship)
  * Chi statistic  
 ![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2013/09/chi-square-formula.jpg)
    * c = degrees of freedom
    * O = observed value
    * E = expected value
  * Show relationship between <u>categorical variables (non-numerical)</u>
    * Tells how much difference there is between observed counts if no relationship in population
      * Low value = high correlation between 2 sets of data
      * X^2 = 0 means observed values=expected values
    * Need to compare chi value to critical value from table
      * **If X^2 > critical value == significant difference between observed & expected**
      * Or use p-value w/ H0 and HA hypothesis
        * Small p-values = significant difference
          * Need chi degrees of freedom (n-1)
          * Need alpha level (0.10, 0.05, or 0.01) -> 10%, 5%, 1%

* **Maximum Likelihood Estimate**
  * Finds most likely function to explain observed data
    * ex) Given model to find probabilities (find prob X >2 if Poisson(2.4) distributed)
      * parameter of lambda = 2.4
      * what if model not given for data & need to fit data to model? -> use MLE
  * MLE takes known probability distributions & compares your dataset to find good fitting match
    * Distribtuions can have infinite #of parameters
      * **MLE finds the parameter of the population & most likely generates sample tested**
      * Match data to distribution = goodness of fit
  * MLE function
    * Likelihood of a sample = probability of getting the sample under a distribution model
      * Likelihood function expresses the probability
      * parameters maximizing probability of getting the sample = MLE
    * ex) set of random variables(X) taken from unknown population distribution w/ parameter theta
      * Distribtuion has probability density function f(X|theta)
        * f = model
        * X=set of random vars
        * Theta=unknown parameter
      * MLE function finds most likely value for theta when given X random variables
      ![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2015/09/maximum-likelihood-function.png)
      
* **Goodness of Fit Test**
  * Test if sample data fits distribution for a certain population (see if sample data represents data in population)
  * **Common types**
    * Chi-square
      * Discrete distributions (binomial, Poisson)
      * Problem
        * can only be used for data classifed into classes(bins) -> need to make freq table/histogram
        * **Needs large sample size for close approximation**
      * 2 types
        * Test for independence = compares 2 sets of data to see if there is a relationship
        * Goodness of fit = fit one categorical variable to distribution
    * Kolmogorov-Smirnov
      * Continuous distributions
      * Test for normallity
        * Tells when it's unlikely the data follows a normal distribution
        * Doesn't make assumptions of the distribtion of the data
      * Problem
        * Doesn't tell if particular sample came from a normal population
    * Anderson-Darling
      * Continuous distributions
      * Modification of Kolmogorov-Smirnov
       * Tells if data is unlikely to have normal distribution
       * More sensitive to deviations in distribution
    * Shipiro-Wilk
      * Calculates W-value to see if random sample came from normal population distribution 
      * Recommends samples up to n=2000
      
* **Bayesian Statistics/infererence & probability**
  * Statistics using prior probabilities to answer
    * Has an event occured before?
    * Will the event occur based upon current knowledge?
  * Prior probability = probability distribution summarizing beliefs of an event before(prior) new evidence considered
    * Adding new evidence = posterior probability
  * Examples using Bayesian probabilities
    * Bayes Theorem
    * Inverse Probability 


</body>
  