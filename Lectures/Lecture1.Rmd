---
title: "Chapter 1: Generative Models for Discrete Data"
output: html_notebook
---


```{r, message=F, warning=F}
library(here)
library(tidyverse)
```

<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> Intro  
<a href="#2">[2]</a> Chapter 1: Generative Models for Discrete Data
<br> &emsp; <a href="#3">[2.1]</a> Chapter Summary
<br> &emsp; <a href="#4">[2.2]</a> Example
<br> &emsp; <a href="#5">[2.3]</a> Discrete Probability Models
<br> &emsp; <a href="#6">[2.4]</a> Multinomial distributions
<br> &emsp; <a href="#7">[2.5]</a> Excercises</br> 
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">Intro</b></font><a href="#0"><sup>Return</sup></a></p>
* Book literally states, <u>"The book will often throw readers into the pool and hope they can swim in spite of so many missing details."</u>
  * Book also states, "We assume no prior training in statistics."
* This book is based upon R language + Bioconductor package
* Heterogenous data = many dimensions to biological data (ex) incubation time, temperature, condition)
<center><img src="http://web.stanford.edu/class/bios221/book/images/FisherParadigm.png" width="300" height="40" alt="Fig0.1"></center>
* **Exploratory data analysis** = Data used for statistical anlysis to summarize -> ex) simple plotting
  * **Confirmatory data analyses** = robust inferrence which don't need assumptions for a conclusion -> ex) hypothesis testing, regression analysis, and variance analysis
* Large number of columns/features (p), small sample size/rows (n) problem
  * To predict an outcome from high #features, model parameters have to be magnitues higher than samples
  * Solution = use sparsity
    * Use parameters of ~0
    * Emperical Bayes = Don't need to know parameters associated w/ each feature, just infer some/all features/groups share similar/same parammters
  * Bottom up approach to statistics = what model explains the data best? (work backwards)  
<button data-toggle="collapse" data-target="#demo">Click to see chapters of the book</button>
<div id="demo" class="collapse">
<br>◆ CH1 (Generative models - building blocks to draw conclusion of data)
<br>◆ CH2 (Choose model to explain data)
<br>◆ CH3 (Visualization)
<br>◆ CH4 (Mixture modeling)
<br>◆ CH5 (Clustering - distance reliance)
<br>◆ CH6 (Hypothesis test workflow)
<br>◆ CH7 (PCA + multivariate), CH8 (variance analysis (ANOVA))
<br>◆ CH9 (combination of data types), CH10 (networks + trees)
<br>◆ CH11 (feature extraction from images + spatial stats)
<br>◆ CH12 (traing algorithm - machine learning)
<br>◆ CH13 (experimental design)
</br></div>

* **

<p id="2"><b><font size="5">Chapter 1: Generative Models for Discrete Data</b></font><a href="#0"><sup>Return</sup></a></p>
* Counting events (ex) #codons, #reads, #GC to calculate %GC)
  * <mark style='background-color:yellow'>Counts = discrete variables</mark>
    * **Discrete variables** = countable variable up to a **limit** (ex) money)
  * <mark style='background-color:yellow'>Quantities (mass/intensity) = continous variables </mark>
    * **Continous variables** = if counted it could be **limitless** (ex) age of universe may be endless)
      * Age can be discrete if unit is specified (ex) months, or years)
      * Therefore, continous variables may be discrete under certain circumstances (ex) time on clock)
* Studies usually constricted by rules -> can predict probability of outcome == **Top-down approach of deduction**
  * Prior knowledge manipulates probabilities
* Parametric vs non-parametic data
  * <mark style='background-color:yellow'>**Parametric** = modelling w/ known facts(paramteters) about population</mark>
    * ex) normal distribution (parameters = mean(μ) and standard deviation(σ))
      * where mean = 0, and stdev = 1 in normal distribution
    * Should be used over nonparametric if applicable (more accurate, more statistical power = find true significance)
  * <mark style='background-color:yellow'>**Nonparametric(distribution-free)** = No assumptions about distribution, usually <u>assumes distribution is not normally distributed</u></mark>
    * Less accurate vs parametric tests 

* **

<p id="3"><b><font size="5">Chapter Summary</b></font><a href="#0"><sup>Return</sup></a></p>
* Calculating probabilities of **discrete events**
  * Model for basic distributions
    * **Bernoulli distribution** = represent single binary trial (ex) coin flip\), as 0 and 1
      * _p_ (probability of success) == 1
    * **Binomial distribution** = used for 1's in _n_ binary trial, create probabilities of _k_ success (via R dbinom)
      * Can stimulate _n_ trial binomial (via R rbinom)
    * **Poisson distribution** = used when _p_ is small (rare 1's - success)
      * One parameter _λ_
        * Poisson formula λ=np -> approximately same as binomial distribution (n,p) IF _p_ is small
      * model #randomly occuring false +ves in assay (epitopes in sequence) -> assumed pre-positon false +ve rate _p_ is small
      * Parametric model to find proabilities of extreme events (requires parameters - parametric)
    * **Multinomial distribution** = used for discrete events w/ >2 possible outcomes(levels)
      * Monte Carlo stimulations can help decide how much data is needed to test w/ multinomial model w/ equal probabilities and see if consistent w/ data
      * Probabilistic models to evaluate hypotheses of how data was generated
        * Assumptions of generative models
      * **Probability of seeing data w/ given hypothesis == p-value**

* **


<p id="4"><b><font size="5">Example</b></font><a href="#0"><sup>Return</sup></a></p>
* ex) Mutations along HIV genome occur at random rate of 5x10<sup>-4</sup>mutations/nt/rep cycle 
  * If rep cycle = 1, then mutations in genome is about 10<sup>4</sup> = 10,000
    * (0.0005mutations\*rep cycle/1nt) = (x mutations*1 cycle/10000 nt)
      * (0.0005mutations\*rep cycle/1nt) * (10000 nt/1 cycle) = (x mutations)
      * (0.0005mutations * 10000 nt/1nt) = x mutations
      * 5 mutations = x
  * Poisson Distribution
    * of rate 5 mutations
    * This model can be used to predict #of mutations in one replication cycle (predicts ~5)
      * Variability of this estimate is √5 (standard error)
      
<center><span class="math display">\[\begin{equation*}
P(X=k)= \frac{\lambda^k\;e^{-\lambda}}{k!}.
\end{equation*}\]</span></center>
* **Poisson Formula, where λ=μ**
  * Probability of seeing _x_ value 
  * <mark style='background-color:lightgreen'> good model for rare events (ex) mutations)</mark>
  * Explained in "Discrete Probability Models" below

```{r}
# If predict probability of 3 mutations using the 5 mutation model then
  # x=3 events
  # rate parameter(λ) = 5
dpois(x=3, lambda=5)
```
Therefore, chance of seeing 3 mutations is ~14%

```{r}
# To find probability of x=0 to 12 mutations
n=0:12
x = dpois(x=n, lambda=5)
x
```

```{r}
#Plot
  # names.arg = vector names to be plotted under each bar
barplot(x, names.arg = 0:12, col = "red")
```

* **

<p id="5"><b><font size="5">Discrete Probability Models</b></font><a href="#0"><sup>Return</sup></a></p>
* Point mutations in binary = occurance(1 or yes) or nonoccurance(0 or no)
  * Yes/No = **levels** of categorical variable
    * <mark style='background-color:yellow'>**Categorical Variable** = variables that can be categorized into levels (ex) toothpaste brands)</mark>
  * Some events are nonbinary (ex) genotypes of diploid -> 3 levels AA, Aa, aa)
  * **To measure categorical variables of sample**
    * **Tally frequencies @each level as counts** 
      * R categorical variables = factors
      * R automatically detects levels when making factors
```{r}
#ex) Blood genotypes for n=19 people
  # Insert vector of data, then use table to categorize + count frequencies of levels

genotype <- c("AA","AO","BB","AO","OO","AO","AA","BO","BO",
             "AO","BB","AO","BO","AB","OO","AB","BB","AO","AO")
table(genotype)
```
```{r}
# To access levels of a R factor (category)
genotype1=factor(genotype) #turn genotype vector into a factor
levels(genotype1) #pull out the levels of the factor
```

* If making factor w/ levels not in data
  * If order of datapoints independent, then the random variable == exchangeable
  * Information of factor is summarized via counts(frequencies) in each level
    * The vector frequencies sufficient to capture all relevant info in data

* <mark style='background-color:lightblue'>**Bernoulli Distribution**</mark>
  * ex) Tossing a coin results in 2 possible outcomes (simple experiment - single trial)
      * for this example, use ```rbinom``` (where r is for random, binom is for binomial).
        * rbinom [(Useful video)](https://www.youtube.com/watch?v=CM7ncRGlViE)
          * n = number of replicates of experiment **(Number of times a trial is performed)**
          * size = #of trials in each replicate **(Number of actions performed per trial)**

```{r}
#  ex) 15 fair coin tosses (Bernoulli trial) w/ probability of success (_p_) = 0.5  -> flip coin once (size=1)
  #rbinom = random generation of success/failure
rbinom(n=15, prob=0.5, size=1)
```

Parameters/Arguements = subset of functions within a function  
&emsp;  ◆ n=15 is parameter 1 (#trials to observe (replicates))  
&emsp;  ◆ prob is parameter 2 (probability of success)  
&emsp;  ◆ size is parameter 3 (means each individual replicate has _one_ (trial) coin toss)  

**Note** The answer using binomial equation always changes because two levels (success or failure)  
&emsp; ◆ outcome differs per run due to probability

Success and failure can have unequal probailities in Bernoulli distribution  
&emsp; ◆ But the probabilities must sum = 1 (complementary)
```{r}
# ex) for 12 throws(trial replicates) of 1 ball into 2 boxes (one throw per replicate (size=1))
  # Probability left = 1/3, right = 2/3

rbinom(n=12, prob=2/3, size=1)
```

Where 1 = successfully landed in right box, 0 = landed in left box

* <mark style='background-color:lightblue'>**Binomial Distribution**</mark>  
  ![**Binomial Formula**](https://www.onlinemathlearning.com/image-files/binomial-distribution-formula.png)  
  * x is often denoted k
  * q = 1-p
  * B(n=20, p=1/6) want to know Prob(X=3) ------ [3 success]
    * ```dbinom(x=3, size=20, p=1/6)```
       * Formula corresponding to above figure
       * size = n
       * x = k
       * p = p
      
        
> The number of trials is the number we input in R as the size parameter and is often written n, while the probability of success is p. Mathematical theory tells us that for X distributed as a binomial distribution with parameters (n, p) written X∼B(n,p), the probability of seeing X = k successes is

<span class="math display">\[\begin{equation*}
\begin{aligned}
P(X=k) = \frac{n\times (n-1)... (n-k+1)}{k\times(k-1)... 1}\; p^k\, (1-p)^{n-k} \\
= \frac{n!}{(n-k)!k!}\;p^k\, (1-p)^{n-k}\\
= { n \choose k}\; p^k\, (1-p)^{n-k}.\end{aligned}
\end{equation*}\]</span>

  * <mark style='background-color:yellow'>**Bernoulli vs Binomial**</mark>
    * 2 outcomes (success/fail)
      * size (action per trial) = 1 Bernouli (One action per multiple Bernoulli trials)
      * size (action per trial) = >=1 then binomial (Many actions per one Bernoulli trial = sum)
      
<center><button data-toggle="collapse" data-target="#explain">Click for better explaination</button>
<br><div id="explain" class="collapse">
<br>[Source](https://math.stackexchange.com/questions/838107/what-is-the-difference-and-relationship-between-the-binomial-and-bernoulli-distr/838122)
<br> ◆ Bernoulli random variable has two possible outcomes: 0 or 1. <u>A binomial distribution is the sum of independent and identically distributed Bernoulli random variables</u>.
<br> ◆ In general, if there are n Bernoulli trials, then the sum of those trials is binomially distributed with parameters n and p.
<br> ◆ Note that a binomial random variable with parameter n=1 is equivalent to a Bernoulli random variable, i.e. there is only one trial.
</br></div></center>
  
  * ex) Only care about #balls go into right box & #throws doesn't matter (independence)
    * sum cells in output vector of Bernoulli
    * Instead of using a binary vector, as in Bernoulli Distribution, <u>the output is a single number (because sum)</u>
```{r}
#ex) For 1 coin (AKA 1 Bernoulli trial), toss coin 12 times (size=12) -> sum outcome of 1 Bernoulli trial
  # Probability tails = 1/3, heads = 2/3
rbinom(1, prob=2/3, size=12)
```
Output is interpreted as 8 coins landing on heads (with probability 2/3)  

Two level model for two possible outcomes (ex) h/t, success/fail, yes/no, CpG/non-CpG, M/F, **diseased/healthy**)  
&emsp; ◆ Probability _p_ of success or complementary event failure (1-p) -> only for independent events (exchangeable)  
&emsp; ◆ ex) in n=15, if SSSSSFSSSSFFFSF, then #success=10, fail=5 -> n15, x=10

```{r}
# 1 Bernoulli trial filp a coin 15 times w/ success probability of 0.3 == binomial form variable B(15, 0.3)  -> B(n,p)
  # Set seed is a state where random numbers are always the same for that seed (so that results can be replicated if same seed used) -> useful for binomial results b/c success/fail probabilities variable
set.seed(235569515)
rbinom(n=1, prob = 0.3, size = 15)  # Binomial form (sum of 15 Bernoulli trials)
```

```{r}
#Q) What is the most common outcome number after 0 to 15 Bernoulli trials? 
  # Check probability mass distribution

#0 to 15 filps of coin per trial(15), success probability of heads is 0.3
prob <- dbinom(0:15, prob=0.3, size=15)
round(prob, 2)
```
```{r}
#Visualize previous question with a barplot
  # names.arg = vector names to be plotted under each bar
barplot(prob, names.arg=0:15, col="red")
```

```{r}
#Q) output of the formula for k=3, p=2/3, n=4? B(4,2/3)
  # per 1 Bernoulli trial
    # n = flip a coin 4 times (trial/size)
    # p = probability of heads is 2/3
    # k = expected success (heads) of 3
  # dbinom will find success rate of 3 heads when flipped 4 times in one trial
dbinom(x=3, prob=2/3, size=4)
```


* <mark style='background-color:lightblue'>Poisson Distribution</mark>  
    ![**Poisson Formula, where λ=μ**](https://www.onlinemathlearning.com/image-files/xpoisson-distribution-formula.png.pagespeed.ic.s5RE-oEm0q.png)
  * **Used when small probability of success (_p_) with large #of trials (_n_)**
    * Can use in place of binomial distribution B(n,p) - reason = Poisson is simpler to calculate
  * Poisson rate parameter λ=np
```{r}
# Q) Probability of mass distribution 
  # Observe 0 to 12 mutation in genome, where n= 10^4 nucleotides, and probability of mutations is p=5x10^-4 per nt

x = 0:12

#Binomial B(n,p)
dbinom(x, prob=0.0005, size=10000)

#Poisson (λ=np)
dpois(x, lambda=((10000*0.0005)))
```

Very similar output, binomial depends on 2 parameters (n,p) while poisson is a product of n*p

```{r}
# Another example using poisson formula and not the function (λ=5 and P(X=3))
(5^3 * exp(-5))/factorial(3)  #same as dpois(x=3, lambda=5)
```

```{r}
# Stimulate 300,000 mutation process (Bernoulli trials/generations) on 10,000nt sequence (size)
  # Mutation rate 5x10^-4 (probability)

rbinom(1, prob=5e-4, size=10000)
stimulation = rbinom(n=300000, prob=5e-4, size=10000)
barplot(table(stimulation), col="red")
```

* Example of using distributions: Epitope detecion
  * Testing pharmaceuticals requires finding proteins that provoke unwated reactions (molecular sites of protein = epitopes)
  * ELISA assay error model
    * Known parameters
      * False positive rate/baseline noise level per position = 0.01 (_p_)
        * P(declare epitope | no epitope) -> Probability of declaring a hit, we think we have an epitope, when there is none
          * The vertical bar in expressions means “X happens conditional on Y being the case”.
      * Tested at total 100 different positions (Bernoulli trials _n_ when size=1 (condition of Bernoulli)) of the protein (independently)
        * AKA: Experiment performed on 100 times on independent protein positions (replicate) == 100 positions
      * From 50 patients
        * Experiment performed on 50 patients (_size_)

```{r}
# Data from one patient
  # 1 = potential epitope present (potential rxn), 0 = no potential epitope (no rxn)

#p1 <- c(rep(0, times=21), 1, rep(0,times=78))
#p1

rbinom(n=100, prob=0.01, size=1) #Bernoulli for one patient (repeat test for 1 patient(size) x100 times(n) by independently sampling different protein positions)
```
```{r}
# Stimulate sum of 50 independent Bernoulli variables (stimulate binomial/poisson when performed on 50 different people)
  # n=100 protein positions (performed on 100 positions (re-sampled 100 times) per person)
  # size(n)=50 patients
  # p = 0.01 is low enough to use for Poisson (lambda=(0.01*50))
  #If no allergic rxns, false+ve rate of one patient @each nt position -> prob 1/100 of being true(binary 1=epitope not present)
    # sum 50 paitients (size), expect any nt position no epitope == sum 50 observed Bernoulli trials(each will output either 0 or 1 if epitope present/not present) should have Possion distribution parameter of 0.5 (n=50*p=0.01)

s <- rpois(100, lambda=0.5) # or rbinom(100, prob=0.01, size=50)
s
barplot(s, ylim=c(0,7), width=0.7, xlim=c(-0.5,100.5), names.arg=seq(from=1, to=100, by=1), xlab="Assay Number", ylab="Patients") # or barplot(s, name.arg=1:100)
```

```{r}
#Real data results from 50 assays summed (50 Bernoulli patients each replicatively tested for total 100 protein positions) using actual data
  #n=100 positions on protein tested independently (replicates)
  #size(n)=50 patients
  #p=0.01

load(here("BookStuff", "data", "e100.RData")) #here defaults directory of workspace, quotes are subdir
e100
barplot(e100, ylim=c(0,7), width=0.7, xlim=c(-0.5,100.5), names.arg=seq(along=e100), col="darkolivegreen")
```

So for assay 1/100, 2/50 people have a 0.01% probability of getting a false positive at a 1% probability (that is no epitope is actually present)  

**Note** the y-axis bar of 7, what are the chances of seeing a value as large as 7, if no epitope present?  
&emsp;  ◆  AKA what are the chances of seeing 7 patients test true for a false positive rate of 0.01%? (that is, we think a epitope is present but it's actually not)  
&emsp;  ◆  If looking for a probability of seeing a number >=7 when considering one Poisson(size=50*prob=0.01) then calculate using

<span class="math display">\[\begin{equation*}
P(X\geq 7)= \sum_{k=7}^\infty P(X=k).
\end{equation*}\]</span>
Same as 1-P(X<=6)  
&emsp;  ◆  The probability P(<=6) == Cumulative distribution function at 6  [more here](https://newonlinecourses.science.psu.edu/stat414/node/69/)  
&emsp;  ◆  Use R function ```ppois```

```{r}
#2 methods of ppois

#Method 1
1-ppois(6,(50*0.01))

#Method 2
  #If lower.tail = TRUE(default) then probabilities are P[X ≤ x], meaning probability of 6, 5, 4, 3, 2, 1, 0 people == proability of smaller than 7
  #If lower.tail = FALSE then probabilities are  P[X > x] so P(X>6) meaning probability of 7,8,9,10,11, ..., n people from 7 to 100 == proability of larger than or equal to 7
ppois(6, 0.5, lower.tail=FALSE)
```

probability of seeing a count as large as 7, assuming no epitope reactions (false positive), is:
<span class="math display">\[\begin{equation*}
\epsilon=P(X\geq 7)=1-P(X\leq 6)\simeq10^{-6}.
\end{equation*}\]</span>  

<mark style='background-color:red'>No clue what's happening in the below</mark>  

Which is actually not correct in this case because shouldn't ask chances of seeing Poisson as large as 7  
&emsp;  ◆  Should be asking, "what are the chances that the maximum of 100 Poisson(0.5) trials is as large as 7?"  
&emsp;&emsp; ◆ Use **extreme value analysis (max/min)** of Poisson distribution  
&emsp;&emsp;&emsp; ◆ Order data values x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>100</sub> and rename as x<sub>(1)</sub>, x<sub>(2)</sub>, ..., x<sub>(100)</sub>  
&emsp;&emsp;&emsp;&emsp; ◆ Where x<sub>(1)</sub> is the smallest count within the 100 positions sampled  
&emsp;&emsp;&emsp; ◆ x<sub>(1)</sub> & x<sub>(100)</sub>, the smallest and largest values, are the rank statistic over 100 values  
&emsp;&emsp;&emsp; ◆ In this case, max value >=7 is the complementary event of the 100 counts being <=6 (7:100 & 6:0)  
&emsp;&emsp;&emsp;&emsp; ◆ Complementary event probability sums to 1  
&emsp;&emsp; ◆ Assumption -> positions are independen (the 100 events/positions are independent)    

Proper calculation:

<span class="math display">\[\begin{equation*}
\begin{aligned}
 P(x_{(100)}\geq 7)
=1-P(x_{(100)} \leq 6)\\
=1-P(x_{(1)}\leq 6 )\times P(x_{(2)}\leq 6 )\times \cdots \times P(x_{(100)} \leq 6 )\\
=1-P(x_1\leq 6 )\times P(x_2\leq 6 )\times \cdots \times P(x_{100}\leq 6 )\\
=1-\prod_{i=1}^{100} P(x_i \leq 6 ).\end{aligned}
\end{equation*}\]</span>
<center>The ∏ represents <u>product of a series of numbers</u>, analogous to that ∑ for sum</center>

So using ϵ from above calculated by ```ppois```
<span class="math display">\[\begin{equation*}
\prod_{i=1}^{100} P(x_i \leq 6)=
\left(P(x_i \leq 6)\right)^{100}=
\left(1-\epsilon\right)^{100}.
\end{equation*}\]</span>  

**Or more generally (1-ppois)<sup>n trials</sup>**  

Theoretical Proper calculation
&emsp; Since ϵ ≃10<sup>−6</sup> is <1, approximating(1-ϵ)<sup>100</sup> using binomial theorem

<span class="math display">\[\begin{equation*}
(1-\epsilon)^n = \sum_{k=0}^n {n\choose k} \, 1^{n-k} \, (-\epsilon)^k  =
1-n\epsilon+{n\choose 2} \epsilon^2 - {n\choose 3} \epsilon^3 + ... \simeq
1-n\epsilon \simeq 1 - 10^{-4}
\end{equation*}\]</span>  

&emsp; or approximate by <span class="math inline">\(e^{-\epsilon} \simeq 1-\epsilon\)</span>, which is the same as <span class="math inline">\(\log(1-\epsilon)\simeq -\epsilon\)</span>.  

<span class="math display">\[\begin{equation*}
(1-\epsilon)^{100} = e^{\log\left((1-\epsilon)^{100}\right)} = e^{ 100 \log (1-\epsilon)} \simeq e^{-100 \epsilon}
\simeq  e^{-10^{-4}} \simeq 1 - 10^{-4}.
\end{equation*}\]</span>

> Thus the correct probability of seeing a number of hits as large or larger than 7 in the 100 positions, if there is no epitope, is about 100 times the probability we wrongly calculated previously.

> Both computed probabilities 10^−6 and 10^−4 are smaller than standard significance thresholds (say, 0.05, 0.01 or 0.001). The decision to reject the null of no epitope would have been the same. However if one has to stand up in court and defend the p-value to 8 significant digits as in some forensic court cases, that is another matter. The adjusted p-value that takes into account the multiplicity of the test is the one that should be reported, and we will return to this important issue in Chapter 6.


Proper calculation using R stimulation  
&emsp;  ◆ Things are more complex in real life, so we use **Monete Carlo** to compute probabilities  
&emsp;  ◆ <mark style='background-color:yellow'>Monte Carlo = computer simulation using generative modelling to find probabilities of events</mark>

```{r}
# ex) generate 100,000 instqances of picking maxima from 100 Poisson distributed numbers (ex) 100 positions)
  #lambda=0.5
PoisMax = replicate(100000, {max(rpois(100,0.5))})
table(PoisMax) #table helps categorize things and counts frequencies
```

For this generated example, a sum of frequency 7 is observed over 100,000 trials (max 7 & 8)
```{r}
#The approximation of this is P(Xmax >=7)

#(PoisMax >= 7) #evaluates into logical TRUE/FALSE vector
mean(PoisMax >=7) #converts TRUE/FALSE into 1/0 -> result of calculation is fraction of 1's == fraction of TRUE
```

**This output is approximately close to the theoretical calculation of 10<sup>-4</sup> as above**  
<mark style='background-color:yellow'>&emsp;  ◆ Though the limitation is clear because Monte Carlo stimulations are never precise - only estimated close to precise (limited by 1/#simulations = lowest probability encountered)</mark>

> Everything we have done up to now is only possible because we know the false positive rate per position (p=0.01), we know the number of patients assayed (size=50) and the length of the protein (n=100), we suppose we have identically distributed independent draws from the model, and there are no unknown parameters. **This is an example of probability or generative modeling: all the parameters are known and the mathematical theory allows us to work by deduction in a top-down fashion.**

> If instead we are in the more realistic situation of knowing the number of patients and the length of the proteins, but don’t know the distribution of the data, **then we have to use statistical modeling. This approach will be developed in Chapter 2.** We will see that if we have only the data to start with, we first need to fit a reasonable distribution to describe it. However, before we get to this harder problem, let’s extend our knowledge of discrete distributions to more than binary, success-or-failure outcomes.

* **

<p id="6"><b><font size="5">Multinomial distributions</b></font><a href="#0"><sup>Return</sup></a></p>
* Modelling for more than 2 outcomes (beyond Heads/Tails)
  * ex) 4 possible outcomes - 4 nucleotides A, C, G, T
  * In binomial model, can assign probability as
    * p = P(1) = p<sub>1</sub>  == 1 binary
    * 1-p = p(0) = p<sub>0</sub> == 0 binary
  * In the case of nucleotides, each with different outcomes/probabilities
    * P<sub>A</sub>, P<sub>G</sub>, P<sub>C</sub>, P<sub>T</sub>
    * And sum of P<sub>A</sub>+P<sub>G</sub>+P<sub>C</sub>+P<sub>T</sub> = 1
```{r}
# random number generator for multinomial distributions
  # ex) A,G,C,T  (n=4) with pA=1/8, pG=3/8, pC=3/8, pT=1/8

runif(n=4, min=1/8, max=3/8)


```

<mark style='background-color:red'>No clue what's happening in the below</mark>  

> <strong>Mathematical formulation.</strong> Multinomial distributions are the most important model for tallying counts and R uses a general formula to compute the probability of a <strong>multinomial</strong> vector of counts <span class="math inline">\((x_1,...,x_m)\)</span> for outcomes that have <span class="math inline">\(m\)</span> boxes with probabilities <span class="math inline">\(p_1,...,p_m\)</span>:
<span class="math display">\[\begin{equation*}
\begin{aligned}
P(x_1,x_2,...,x_m \;|\; p_1,...,p_m)
  =\frac{n!}{\prod x_i!}\prod p_i^{x_i}\\
  ={{n}\choose{x_1,x_2,...,x_m}} \; p_1^{x_1}\,p_2^{x_2} \cdots p_m^{x_m}.\end{aligned}
\end{equation*}\]</span>


> The first term reads: the joint probability of observing count <span class="math inline">\(x_1\)</span> in box 1 and <span class="math inline">\(x_2\)</span> in 2 and … <span class="math inline">\(x_m\)</span> in box m, given that box 1 has probability <span class="math inline">\(p_1\)</span>, box 2 has probability <span class="math inline">\(p_2\)</span>, … and box <span class="math inline">\(m\)</span> has probability <span class="math inline">\(p_m\)</span>.</span></p>
<span class="marginnote">The term in brackets is called the multinomial coefficient and is an abbreviation for <span class="math display">\[{{n}\choose{x_1,x_2,...,x_m}}=\frac{n!}{x_1!x_2!\cdots x_m!}.\]</span> So this is a generalization of the binomial coefficient – for <span class="math inline">\(m=2\)</span> it is the same as the binomial coefficient. </span></p>

```{r}
#ex) Assume A,G,C,T are all equally likely (0.25 ea), what is the probability of observing 4A, 2G, 0C, 0T? -> 8 characters (4A, 2G) of 4 equally likely types(AGCT 0.25 ea)
  # Use stimulation experiments to check if data is consistent (each nt w/ equal probability)
pvec <- rep(0.25, times=4) #repeat probability 4 times in a vector
t(rmultinom(1, prob=pvec, size=8)) #transpose t(col<->row) the matrix made by rmultinom(1 trial, prob, 8 samples)
```

Here, you can see that outcomes using equal probabilities don't have equal outcomes

```rmultinom(n = 8, prob = pvec, size = 1)``` vs ```rmultinom(n = 1, prob = pvec, size = 8)```  
&emsp; size=1 represents 1 trial w/ n=8 observations/repeats (output =1 or 0), size=8 represents 8 trials in n=1 observation (sum of 1's per row of probabilities)

### Simulating for power
  * Monte Carlo for stimulation of multinomial distribution
  * ex) How big of a sample size is needed for my experiment?
    * **Power in statistics = probability of detecting something if present == True positive rate**
    * Typically aim for >=80% power when planning exps
      * Interpretation: Run experiment many times, then 20% of the times will fail to yield significant results
        * **H<sub>0</sub>** = null hypothesis (DNA data collected presents each of the 4nts at equal probability ea0.25)
          * Null = baseline, nothing is happening
          * Want to reject Null hypothesis, because alternative hypothesis (**H<sub>a</sub>**) means something is occuring (non-equal probabilities)
          
```{r}
# ex) Seq length n=20, want to see if original distribution of nts is fair or comes from alternative process
  # Stimulate 1000 times using null hypothesis (equal probs = 0.25 each nt)

pvec1=c(0.25, 0.25, 0.25, 0.25)
ObsUnder0 <- rmultinom(n=1000, prob=pvec1, size=20)
dim(ObsUnder0) #find dimensions of the matrix - 4 rows with 1000 cols
ObsUnder0[,1:11] #subset to view only first 11 columns
mean(ObsUnder0)
```

Each column is a stimulation per the 4nucleotides (row)  
&emsp; high variation per cell (expected value is 20/4 = 5 -- size 20/4 at equal probability should = 5)  

Make a test  
&emsp; Need to measure variability expcted and upper limits of variability  
</span></p>
<span class="math display" id="eq:Gen-stat">\[\begin{equation}
{\tt stat}=\frac{(E_A-x_A)^2}{E_A}+
\frac{(E_C-x_C)^2}{E_C}+
\frac{(E_G-x_G)^2}{E_G}+
\frac{(E_T-x_T)^2}{E_T}
=\sum_i\frac{(E_i-x_i)^2}{E_i}
\tag{1.1}
\end{equation}\]</span>
Sum of squares of differences between observed values (x<sub>A</sub>) & expected values (E<sub>A</sub>) relative to expected values (E<sub>A</sub>)  
&emsp; This measure weights each of the square residuals relative to their expected values.

```{r}
#Check first 3 columns of stimulated data - see how much they differ from expected

expected0 = pvec1*20 #seq length = 20 --> this gives EA, EC, EG, ET
sum((ObsUnder0[,1]-expected0)^2/expected0) #column 1 in all rows
sum((ObsUnder0[,2]-expected0)^2/expected0) #column 2 in all rows
sum((ObsUnder0[,3]-expected0)^2/expected0) #column 2 in all rows
```

```{r}
#Check all 1000 columns via making a function

stat = function(obsvd, exptd) {
  sum((obsvd - exptd)^2 / exptd)
}
#function(multinomial stimulated matrix, expected values)

stat(ObsUnder0[, 1], expected0)

```

> To get a more complete picture of this variation, we compute the measure for all 1000 instances and store these values in a vector we call S0: it contains values generated under H0. We can consider the histogram of the S0 values an estimate of our null distribution.

```{r}
S0 <- apply(ObsUnder0, 2, stat, expected0) #apply function stat to margin=2=column of matrix ObsUnder0, need to specify expected0 the other parameter of function stat()
summary(S0)
hist(S0,breaks=25,col="blue",main="Title")
```

summary() shows that once every coulmn of the multinomial matrix has applied stat() calculation, the spread is variable  
&emsp; From this data stimulation, can approximate a 95% quartile (separate 95% of values from the top 5%)
```{r}
q95 <- quantile(S0, probs=0.95)
q95
```

Interpretation: 5% of S0 values are larger than 7.6  
&emsp; Use this for critical value (0.05) of data test - reject H0 came from a fair process w/ equally likely probability of nucleotides (if weighted sum of squares ```stat``` is > 7.6)  

Determining test power
&emsp; Find probability that the test (based on weighted sum-of-square difs) will detect data not from H0  
&emsp; To do this, find probability that rejects H0 via stimulation
```{r}
# Generate 1000 stimulations of HA (alternarive) via paramaterized pvecA
pvecA <- c(3/8, 1/4, 3/12, 1/8)
observed <- rmultinom(1000, prob=pvecA, size=20) #length of seq = 20
dim(observed)
observed[,1:7] #plot cols 1 to 7
apply(observed, 1, mean)  #apply mean function to margin 1(rows) of matrix
expectedA <- pvecA*20
expectedA
```

Similar to H0 stimulation, HA stimulation also has observed values which vary  

How often (out of 1000 instances) does the test detect data differing (HA) from H0  

The test doesn't reject in the first observation/stimulation (col1 -> 7,7,3,3) b/c statistic value within 95th percentile

```{r}
stat(observed[,1], expectedA) #check statistic w/ function on column1 of HA stimulation
S1 <- apply(observed,2,stat,expectedA)
q95 #95th percentile of H0
sum(S1>q95) #Sum if stat of HA stimulations > H0 95th percentile
power <- mean(S1>q95)
power
```

In 1000 stimulations of HA, the test found 56 data values from HA distribution --> P(Reject H0|HA) = 0.051 or 5.1%  

Interpretation: With a seq length of n=20, the power to reject H0 for HA is ~5.1%  

Usually aim for a power of >=0.8
```{r}
#attempt to simulate p=0.8 using a higher sequence length 
pvecA1 <- c(3/8, 1/4, 3/12, 1/8)
observed1 <- rmultinom(1000, prob=pvecA1, size=31) #length of seq = 31
dim(observed1)
observed1[,1:7] #plot cols 1 to 7
apply(observed1, 1, mean)  #apply mean function to margin 1(rows) of matrix
expectedA1 <- pvecA1*20
expectedA1
stat(observed1[,1], expectedA1) #check statistic w/ function on column1 of HA stimulation
S11 <- apply(observed1,2,stat,expectedA1)
q95 #95th percentile of H0
sum(S11>q95) #Sum if stat of HA stimulations > H0 95th percentile
power1 <- mean(S11>q95)
power1
```

Another way  to calculate the 95th percentile without Monte Carlo stimulation = Q-Q plots in chapter 2  
&emsp; * Note the fuction ```stat``` is actually the Chi-square distribution w/ 3 degreees of freedom (X<sup>2</sup><sub>3</sub>)  
&emsp; * Monte Carlo stimulations more useful for low parametric probabilities w/ counts near 0 (which chi-square doesnt work for)

* **

<p id="7"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>
* 1.1 - Cumulative distribution functions (pXXX)
  * ```ppois``` and ```pbinom```

* 1.2
```{r}
#B(size=10, p=0.3) where X=2
dbinom(x=2, size=10, prob=0.3)

#For cumulative distribution where P(X<=2) (everything less than/equal to X=2)
sum(dbinom(0:2, size=10, prob=0.3))
```

* 1.3
```{r}
#????

#PosMax <- function(maxprob, n, lambda){
  #maxprob <-  dpois(n, lambda)
#}

#1-x to find maximum
#(m-1 to find complement of max) -> probability of maximum
```

* 1.4

```{r}
#?????

#PosMax <- function(maxprob=1, n=2, lambda=3){
  #maxprob <-  dpois(n, lambda)
#}
```

* 1.5
```{r}
#epitope problem
  # p=0.01
  # n=100
  # size=50
aa <- rbinom(n=100, prob=0.01, size=50)

#Prob of >=9
1-ppois(6, lambda=(0.01*50))

#With stimulations
aa <-  replicate(100000, {
  max(rpois(100, 0.5))
})
table(aa)

mean( aa >= 9 )

#as extreme as 1000 obs by inverse (1/x)
#result_vector >= max doesn't work because result_vector is 0's and 1's which doens't evaluate >=max -> need to do max((result_vector) >=1)
```

* 1.6
```{r}
?Distributions
#beta distribution - unequal probability mothers & babies
  #then input into binomial
  # x= probabilities
```


* 1.7
```{r}
#Probability of having 
dd <- rpois(n=100, lambda=(3))
mean(dd)
var(dd)
```

* 1.8
```{r}

```

* **

<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*

[Batch Download Images from Directory](https://stackoverflow.com/questions/23446635/how-to-download-http-directory-with-all-files-and-sub-directories-as-they-appear)

> wget -r -np -nH --cut-dirs=3 -R index.html http://hostname/aaa/bbb/ccc/ddd/  
  * -r : recursively  
  * -np : not going to upper directories, like ccc/…  
  * -nH : not saving files to hostname folder  
    * --cut-dirs=3 : but saving it to ddd by omitting first 3 folders aaa, bbb, ccc  
  * -R index.html : excluding index.html files

```{code}
mkdir Bookimages
cd Bookimages
wget -r -np -nH --cut-dirs=3 -R SmoothLineP134h7.png http://web.stanford.edu/class/bios221/book/images/
# Seems "SmoothLineP134h7.png" is missing so had to exclude 
```

> **Renaming**  
  * sudo apt install rename  #need to download b/c perl version not on ubuntu
  * rename -v 's/<pattern>/<replacement>/g'  
  * -n is to view change, -v is permanent, /g is recursive replacement, s/ is substitute
  
![**Markdown spacing**](https://i.stack.imgur.com/bMNFi.png)

* Here package defaults at your working directory

```{r}
test <- rbinom(100, prob=1, size=50) #resampled for 100 protein positions, probability of false positive = 100%, testing 50 patients (trials)
barplot(test, ylab = "Number of people tested", xlab = "test on protein position", names.arg =1:100)
```

* [More on Statistical Power](https://www.statisticshowto.datasciencecentral.com/statistical-power/)

<button data-toggle="collapse" data-target="#demo1">Click for info</button>
<div id="demo1" class="collapse">
> The statistical power of a study (sometimes called sensitivity) is how likely the study is to distinguish an actual effect from one of chance. It’s the likelihood that the test is correctly rejecting the null hypothesis (i.e. “proving” your hypothesis). For example, a study that has an 80% power means that the study has an 80% chance of the test having significant results.  
> &emsp; > A high statistical power means that the test results are likely valid. However, as the power increases, so does the possibility of a Type II error.  
  &emsp; > A low statistical power means that the test results are questionable.
  
> Statistical power helps you to determine if your sample size is large enough.
It is possible to perform a hypothesis test without calculating the statistical power. If your sample size is too small, your results may be inconclusive when they may have been conclusive if you had a large enough sample.

> Power analysis is a method for finding statistical power: the probability of finding an effect, assuming that the effect is actually there. To put it another way, power is the probability of rejecting a null hypothesis when it’s false. Note that power is different from a Type II error, which happens when you fail to reject a false null hypothesis. So you could say that power is your probability of not making a type II error.

> A Type I error is the incorrect rejection of a true null hypothesis. Alpha is the size of the test. A Type II error is where you don’t reject a false null hypothesis. This is the β. Beta( β) is the probability that you won’t reject the null hypothesis when it is false. The statistical power is the complement of this probability: 1- β

[TypeI & II Errors](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/statistics-definitions/type-i-error-type-ii-error-decision/)
</div>


# Presenter [notes](https://github.com/kozakb/STAT_4600/blob/master/Chapter_One/generative_models_for_discrete_data_merge.pdf)
* Discrete data and is associated w/ labels (factors)
  * Convert vector to factor via ```factor()```
    * Sort via specifying level orders as a vector within factor ```factor(levels=c())```
* Distributions in R
  * d - probability distribution function P(X = x)
  * p - cumulative distirbution function P(X <= x)
  * q - quantile function -> gives x when a cumulative probability == p
  * r - random generator -> gives random sample size n
* Order statistics
  * Random sample size (n) -> order each random var from smallest exptected success to largest (x<sub>(1)</sub> < x<sub>(2)</sub>, x<sub>(n)</sub>)
* Complementary events
  * Ex) P(X>=2) is equivalent to P(X=2) + P(X=3) + P(X=x)
    * If large number of x successes expected then long calculation
      * To simplify P(X>=2) is also equivalent/complementary to 1-P(X<2) or just 1-P(X<=1) so, 1-(P(X=0) + P(X=1)) cumulative
* Probability of maximum
  * Find probability of seeing value as large as (m) when taking from sample size (n)
    * Also stated as, probability that largest value in sample is <u>as large</u> as m
    * P(x<sub>n</sub> >= m)
      * = 1-P(x<sub>n</sub> <= (m-1))
      * = 1-(P(x<sub>1</sub> <= (m-1)) x P(x<sub>2</sub> <= (m-1)) x P(x<sub>n</sub> <= (m-1)))
        * x expected successes are independent events but identically distributed
      * **Simplifed as: 1-P(x<=(m-1))<sup>n</sup>**
* Poisson = rate calculation ```dpois(x, lambda)```
  * X~Poisson(lambda = (rate 10e-5)*(10000nts mutated per cycle))
    * Lambda = X(expected value) = 5
    * Varience = 5
      * std dev = sqrt(5)
  * Used instead of binomial distribution when low probability with high sample size (n)
  * Questions possibly answered
    * Prob exactly x mutations/cycle
    * Prob >= x mutation/cycle
    * Prob <= x mutation/cycle
    * Prob seeing between x1:x2 mutations/cycle (x1<x2)
  * Function A = [0,1]
    * Integer input -> returns probability as output
    * **Probability mass function** = any formula that outputs probability
      * Probability of observable X has a value x P(X=x) - where X follows Poisson distriub 
      * [more](https://stattrek.com/probability-distributions/probability-distribution.aspx)
        * Random variable (Capital X)- outcome of stats experiment
          * lowercase (x) represents value of outcome
          * P(X) means probability of X
          * P(X=x) means probability random var X is equal to a particular value x
            * P(X=1) probability random var X=1
      * Probability distribution = random var X matched with a probability of outcome within a table/equation
        * Cumulative proability distibution = Probability random var X falls within a range (X<=1) == P(X=0) + P(X=1)
        * Uniform probability disribution = Values (x) of random var X occurs w/ equal probability P(X=x<sub>k</sub>) = 1/k
          * Each outcome is represented by random var (X = k)
          
```{r}
# Probability (X) of having exactly 5 mutations (x)
dpois(x=5,lambda=5)

# Probability (X) of multiple multations (x = 0:20)
dpois(x=0:20, lambda=5)
```

* Bernoulli ```rbinom(n,prob,size=1)```
  * Binary w/ 2 outcomes (assigned 1 or 0)
  * Bernoulli prob(p) == binomial distribution w/ size(n) & prob(p)
```{r}
# Outcome is male(1)/female(0), X random variable represents sex of newborn is male at prob 0.5
  # n = sample 20 mothers
  # size = 1 boy
rbinom(n=20,prob=0.5,size=1)
```
* Binomial = Two outcomes (success/fail) alike Bernoulli, but perform multiple Bernoulli trials 
  * Find success in multiple Bernoulli trials
  * rbinom()
    * n = number of samples taken
    * size = number of Bernoulli trials (denoted n for binomial distribution formula B(n,p))
```{r}
# Outcome is male(>1)/female(0), X random var follows binomial distrib and represents sex of newborn is male per mother at prob 0.5
  # Binomial formula B(3,0.5)
    # n = total number of trials
  # rbinom formula
    # n = sample 10 mothers
    # size = 3 boys in 3 births (total number of trials)
rbinom(n=10, prob=0.5, size=3)
```

```{r}
# Probability of exatcly (P(X=2) 2 boys (x)
dbinom(x=2,prob=0.5, size=3)
```
* Multinomial = Multioutcome 
  * When bernoulli/binomial has more than 2 outcomes
```{r}
# pick 8 people (n Rbase), prob biologist(0.5), prob staticians(0.3), prob engineeers(0.2)
  # outcome exactly 4 biologist, 3 staticians, 1 engineer (x)
dmultinom(x=c(4,3,1), prob=c(0.5,0.3,0.2))
```

* Monte Carlo Stimulations
  * Uses randomness to solve multifactorial questions
    * Used for: optimization, numerical integration, <u>generate/stimulate probability distribution outcome</u>
  * ex) stimulate power for hypothesis test
    * power = probability to reject H0 when H0 is false
      * AKA **True positive rate**
    * Ex) Test H0 of DNA data is actually from a fair process
      * H0 = A,C,T,G is all equal probability of 0.25 vs HA = A,C,T,G do not have equal probabilities
      * Randomly sample a sequence of length 20 using multinomial distribution (size=20) & assume equal probability per nt 
        * stimulate 1000 times (n=1000 in rbase == 1000 observations)
```{r}
set.seed(20190919)
rmultinom(n=10, size=20, p=rep(0.25,4))
```
* In these 20 trials (length of 20 nts), there is variability
* **Use Chi-squared test for goodness of fit**
  * each individual observed(x) - take squared difference from expected value & divide by expected value  
![](http://www.geography-site.co.uk/pages/skills/fieldwork/statimage/chisqu.gif)
```{r}
# Make new Monte Carlo simulated matrix of observed AGCT values
set.seed(1234)
dna <- rmultinom(n=1000, prob=rep(0.25, times=4), size=20) #1000 stimulations(observation)s in length 20nts (trials)
dna[1:4,1:20]
```

```{r}
# Find expected values for each x using the probabilities & the formula E[X]=p*n -> n in terms of binomial formula B(n,p) = 20(size)
expt <- (rep(0.25,times=4)*20)
expt
```

```{r}
# Chi-square starts here
# Subtract expected vector from the observed vector in this case - subtract vector from each row element (4 x 4)
dnasub <- dna-expt
dnasub[1:4,1:20]
```
```{r}
# (O-E) done, now square each value of the matrix
dnasubsq <- dnasub^2
dnasubsq[1:4,1:20]
```
```{r}
# (O-E)^2 done, now divide each row by the expected
dnasubsqE <- dnasubsq/expt
dnasubsqE[1:4,1:20]
```

```{r}
# A single chi-squared function preventing the need to do the previous steps
ChiSqStat <- function(obsmatrix,expected){
  return((obsmatrix-expected)^2/expected)
}
```

```{r}
# Test function
set.seed(1234)
dna1 <- rmultinom(1000, prob=rep(0.25, times=4), size=20)
dna1_ObsmatrixH0 <- ChiSqStat(dna1, expt)
dna1_ObsmatrixH0[1:4, 1:20]
```

* Now find the overall variablity for each position along the 20nt length
```{r}
dna1_colsum <- colSums(dna1_ObsmatrixH0) #sum ATCG chi-square values
hist(dna1_colsum, breaks=25, col="green") #visualize
# View 95th percentile for overall variability among positions
p95th <- quantile(dna1_colsum, probs=0.95)
p95th
```

* **The 95th percentile means that 5% of positions vary by value of 7.6**
  * This number can be used to detect if individual positions come from different (non fair HAlternative) process
```{r}
# Simulate unfair observations (Halternative = probabilities for nt differ)
set.seed(1234)
dnaNF <- rmultinom(1000,prob=c(0.375,0.25,0.25,0.125), size=20)
dnaNF[1:4,1:20]
```

```{r}
# Generate expected values from formula E[x]=n*p
exptNF <- c(0.375,0.25,0.25,0.125)*20
exptNF
```
```{r}
# Calculate Chi-squared statistic
  # Note use probability from H0 for chi-square calculation
dnaNF_ObsmatrixH0 <- round(ChiSqStat(dnaNF,expt), 1)
dnaNF_ObsmatrixH0[1:4, 1:20]
```
```{r}
# Find overall variability
dnaNF_colsums <- colSums(dnaNF_ObsmatrixH0)
dnaNF_colsums[1:20]
```

* When colsums of H0 (equal probabilities-fair) and HA (differing probabilities-unfair) is calculated, then can dertermine power of test
  * That is, how likely is the test correctly rejects the null hypothesis (Ideally ~80%)
  * Statistical Power = total number of samples varying by >= 95th percentile H0 (7.6)/total number of simulations performed
    * total number of samples varying by >= 95th percential H0 (7.6) == if >=7.6 then reject H0
    * number of simulations in this case is 1000
```{r}
power <- mean(dnaNF_colsums>p95th)
power
```
* The resulting power is 4.7% when DNA length of 20(trials) is used for test, which is not great
  * What is the minimum trials needed for a power of 80%?
  
```{r}
# Simulate using different multinomial n values (size in R) then plot power(y) & trial(x)
  # Make function to calculate power by combining all steps
calcPower <- function(Obsdata,probs,crit_v){
  dataMatrixH0 <- ChiSqStat(Obsdata, (1/nrow(Obsdata)) * colSums(Obsdata[])[1])  #??????
  data_col_sum <- colSums(dataMatrixH0)
  power <- mean(data_col_sum>crit_v)
  return(power)
}
```

```{r}
# Generate 100 matricies of observed values (100 different length DNA(trials)) - each simulated 1000 times
  # purrr::map(.x, .f, ...) function - in tidyverse package
    # .x = vector w/ parameter which can vary per call
    # .f = function to use on the vector
    # ... any other parameters of Rfunction

size <- c(1:100)
dna1_to_100 <- map(size, rmultinom, n=1000, prob=c(0.375,0.25,0.25,0.125)) #n=1000 simulations
```

```{r}
# First matrix where each trial has length 1
dna1_to_100[[1]][,1:10]

# Last matrix where each trial has length 100
dna1_to_100[[100]][,1:10]
```
```{r}
# Use map() again but using calcPower() function applied to each matrix - for H0 first (equal probabilities)
  # NOTE CRITICAL VALUE FROM 95TH PERCENTILE IS CALCULATED FROM COLSUMS OF H0 PROBABILITIES
Obsdata <- dna1_to_100
power1_to_100 <- map(Obsdata, calcPower, probs=rep(0.25,times=4), crit_v=p95th)
```

```{r}
# Plot relationship
qplot(x=1:100, y=as.numeric(power1_to_100),geom="line")
```

```{r}
# Now find what n/size is best for getting a power of 80%
detect_index(as.numeric(power1_to_100), ~.x >= 0.8)
```

* So need a minimum length of ~84nts to have a power of 80% (note Monte Carlo simulation changes so only close approximation)

```{r}
#Check 

set.seed(1234)
nsize=84
dna1.1 <- rmultinom(1000, prob=rep(0.25, times=4), size=nsize)
dna1.1_ObsmatrixH0 <- ChiSqStat(dna1.1, (rep(0.25,times=4)*nsize))
dna1.1_colsum <- colSums(dna1.1_ObsmatrixH0) #sum ATCG chi-square values
p95th1.1 <- quantile(dna1.1_colsum, probs=0.95)
p95th1.1

set.seed(1234)
dnaNF1.1 <- rmultinom(1000,prob=c(0.375,0.25,0.25,0.125), size=nsize)
dnaNF1.1_ObsmatrixH0 <- ChiSqStat(dnaNF1.1, (rep(0.25,times=4)*nsize))
dnaNF1.1_colsums <- colSums(dnaNF1.1_ObsmatrixH0)

power1.1 <- mean(dnaNF1.1_colsums>p95th1.1)
power1.1
```

</body>
  