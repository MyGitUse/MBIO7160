---
title: "Chapter 5: Clustering"
output: html_notebook
---

```{r, hidecode=T}

```


<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> Clustering  
<a href="#2">[2]</a> Chapter Summary  
<a href="#3">[3]</a> What are the data and why do we cluster them?  
<a href="#4">[4]</a> How do we measure similarity?  
<a href="#5">[5]</a> Nonparametric mixture detection  
<a href="#6">[6]</a> Clustering examples: flow cytometry and mass cytometry  
<a href="#7">[7]</a> Hierarchical clustering  
<a href="#8">[8]</a> Validating and choosing the number of clusters  
<a href="#9">[9]</a> Clustering as a means for denoising  
<a href="#11">[10]</a> Excercises</br> 
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">Clustering</b></font><a href="#0"><sup>Return</sup></a></p>
  * Making categories within a set of scientific data
    * Can model data as mixtures from different groups/populations w/ parametric model (CH4) -> EM algorithm to assign components (distributions)
    * Making groups via clustering -> no elliptical shapes
      * elliptical shapes generated by mixture models w/ multivaraibles in normal distribution -> implies elliptic cluster boundries
  * Clustering is for continous/quasi-continous data
    * Makes categorical group variable -> helps simplify decisions made
    * ex) Medical decisions simplified by replacing high-dimension diagnostic measurements via simple groupings
      *numbers w/ glucose lvls, hemoglobin lvls, etc -> grouped via assigning as diabetes mellitus
  * This CH -> find meaningful clusters/groups in low or high dimension non-parametric data
    * <mark style='background-color:yellow'>problem: Clustering algorithms designed to find clusters, so forced to cluster even if no grouping actually exists</mark>
      * Need to validate clusters manually (esp. if no prior knowledge to support clusters)
      
* **

<p id="2"><b><font size="5">Chapter Summary</b></font><a href="#0"><sup>Return</sup></a></p>

* **How to compare observations?**
  * First step is to find the right distance for clustering analysis
  * Garbage in = Garbage out
    * Choose distance that is scientifically meaningful & compare output of other possible distance combinations
    * Same data may require different distances if different scientific objectives
* **Two ways of clustering**
  1. Iterative partitioning approach -> Kmeans & Kmedoids (PAM): Alternate between estimating cluster centers & assiging points to clusters
  2. Hierarchical clustering approach -> Collect points & growing clusters, then nest sequences into sets represented by hierarchical clustering trees
* **Biological examples**
  * Clustering useful for finding latent classes in single cell measurements (Immunology & single cell data)
  * <mark style='background-color:yellow'>Density-based clustering useful for low-dimension data w/ no sparsity issue</mark>
* **Validating clusters**
  * Cluster algorithms always asign clusters, so need to assess #of clusters & choose if representitive of data
  * Use visualization tools & repeating clustering w/ resamples of data (self note- bootstrap?)
    * Statistical tests (WSS/BSS) or (log(WSS)) calibrated w/ simulations of data
      * Used to understand group structure
      * Provide benchmark for choosing #of clusters on new data
    * Use of biological prior knowledge is best approach for validating clusters
* **Distances & probabilities**
  * Instead of distances, can account for _baseline frequencies & local densities for clustering_
    * Essential clustering to denoise 16S rRNA seq reads (true class/taxa group occur at different freqs)

* **
<p id="3"><b><font size="5">What are the data and why do we cluster them?</b></font><a href="#0"><sup>Return</sup></a></p>
* Clustering may lead to discoveries
  * ex) map cases of outbreaks - may find clusters of cases
    * collect info about situation within clusters, proximity of cases within clusters -> may reveal something latent, such as source
  * WWII bombing locations data show that targets were random
  * <mark style='background-color:yellow'>Clustering useful for complex multivariable data</mark>
    * Clustering is unsupervised = all variables have same status based on info from explanatory variables (not predicting/learning value of one variable)
    * Exploratory techniques -> grouping is important to interpret data
      * ex) Understanding cancer biology -> tumors clustered based upon anatomical location & histopathology via molecular data (expression) -> clusters may refine disease types
        * Relevance of clusters via evidence (associated w/ different outcomes)
  * CH4 mainly based upon EM algorithm for clustering latent groups
    * CH5 clustering techniques are more general and can be used for complex data
      * <mark style='background-color:yellow'>Most are based on distances beteween pairs of obs (distances vs all, or some)</mark>
        *Does not make assumptions about generative mechanisms of data distributions (normal, gamma-Poisson, etc.)
        * Many clustering algorithms used in softwares (based on diversity of data types & objectives of study)

![](http://web.stanford.edu/class/bios221/book/images/ClusteringA.png)

> We decompose the choices made in a clustering algorithm according to the steps taken:  
starting from an observations-by-features rectangular table X -> choose an observations-to-observations distance measure and compute the distance matrix, here schematized by the triangle.  
The distances are used to construct the clusters.  
On the left, we schematize agglomerative methods, that build a hierarchical clustering tree;  
on the right, partitioning methods that separate the data into subsets.  
Both types of methods require a choice to be made: the number k of clusters.  
&emsp; For partitionning approaches such as k-means this choice has to be made at the outset;  
&emsp; for hierarchical clustering this can be deferred to the end of the analysis.

* **
<p id="4"><b><font size="5">How do we measure similarity?</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="5"><b><font size="5">Nonparametric mixture detection</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="6"><b><font size="5">Clustering examples: flow cytometry and mass cytometry</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="7"><b><font size="5">Hierarchical clustering</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="8"><b><font size="5">Validating and choosing the number of clusters</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="9"><b><font size="5">Clustering as a means for denoising</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="11"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>


* **

<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*

</body>
  