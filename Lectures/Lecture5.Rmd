---
title: "Chapter 5: Clustering"
output: html_notebook
---

```{r, message=FALSE, warning=F}
library(here)
library(vegan)
library(scRNAseq) #bioconductor
  # In windows conda R "BiocManager::install("scRNAseq", INSTALL_opts=c("--no-multiarch"))"
library(clusterExperiment) #bioconductor
  # requires gsl, so on linux terminal "sudo apt-get install libgsl-dev" to allow installation of gsl on R (install.packages("gsl")) or BiocManager::install("clusterExperiment") might install it w/ other packages
#browseVignettes("clusterExperiment")
library(limma)
library("flowCore") ; library("flowViz")
library(Biobase)  #bioconductor
  # In windows conda R "BiocManager::install("Biobase", INSTALL_opts=c("--no-multiarch"))"
library(readr)
library(flowPeaks) #bioconductor
library(ggcyto) #bioconductor
library(labeling)
library(dbscan)
library(pheatmap)
library(dplyr)
library(fpc) ; library(cluster)
library(Hiiragi2013)
library(clue)
library(ggbeeswarm)
library(mixtools)
library(vcd)
library(dada2) #bioconductor
library(kernlab)
```


<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> Clustering  
<a href="#2">[2]</a> Chapter Summary  
<a href="#3">[3]</a> What are the data and why do we cluster them?  
<a href="#4">[4]</a> How do we measure similarity?  
<a href="#5">[5]</a> Nonparametric mixture detection  
<a href="#6">[6]</a> Clustering examples: flow cytometry and mass cytometry  
<a href="#7">[7]</a> Hierarchical clustering  
<a href="#8">[8]</a> Validating and choosing the number of clusters  
<a href="#9">[9]</a> Clustering as a means for denoising  
<a href="#11">[10]</a> Excercises</br> 
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">Clustering</b></font><a href="#0"><sup>Return</sup></a></p>
  * Making categories within a set of scientific data
    * Can model data as mixtures from different groups/populations w/ parametric model (CH4) -> EM algorithm to assign components (distributions)
    * Making groups via clustering -> no elliptical shapes
      * **elliptical shapes generated by mixture models w/ multivaraibles in normal distribution -> implies elliptic cluster** boundries
  * Clustering is for continous/quasi-continous data
    * Makes categorical group variable -> helps simplify decisions made
    * ex) Medical decisions simplified by replacing high-dimension diagnostic measurements via simple groupings
      *numbers w/ glucose lvls, hemoglobin lvls, etc -> grouped via assigning as diabetes mellitus
  * This CH -> find meaningful clusters/groups in low or high dimension non-parametric data
    * <mark style='background-color:yellow'>problem: Clustering algorithms designed to find clusters, so forced to cluster even if no grouping actually exists</mark>
      * Need to validate clusters manually (esp. if no prior knowledge to support clusters)
      
* **
<p id="2"><b><font size="5">Chapter Summary</b></font><a href="#0"><sup>Return</sup></a></p>
* **

* **How to compare observations?**
  * First step is to find the right distance for clustering analysis
  * Garbage in = Garbage out
    * Choose distance that is scientifically meaningful & compare output of other possible distance combinations
    * Same data may require different distances if different scientific objectives
* **Two ways of clustering**
  1. Iterative partitioning approach -> Kmeans & Kmedoids (PAM): Alternate between estimating cluster centers & assiging points to clusters
  2. Hierarchical clustering approach -> Collect points & growing clusters, then nest sequences into sets represented by hierarchical clustering trees
* **Biological examples**
  * Clustering useful for finding latent classes in single cell measurements (Immunology & single cell data)
  * <mark style='background-color:yellow'>Density-based clustering useful for low-dimension data w/ no sparsity issue</mark>
* **Validating clusters**
  * Cluster algorithms always asign clusters, so need to assess #of clusters & choose if representitive of data
  * Use visualization tools & repeating clustering w/ resamples of data (self note- bootstrap?)
    * Statistical tests (WSS/BSS) or (log(WSS)) calibrated w/ simulations of data
      * Used to understand group structure
      * Provide benchmark for choosing #of clusters on new data
    * Use of biological prior knowledge is best approach for validating clusters
* **Distances & probabilities**
  * Instead of distances, can account for _baseline frequencies & local densities for clustering_
    * Essential clustering to denoise 16S rRNA seq reads (true class/taxa group occur at different freqs)

* **
<p id="3"><b><font size="5">What are the data and why do we cluster them?</b></font><a href="#0"><sup>Return</sup></a></p>
* **
* Clustering may lead to discoveries
  * ex) map cases of outbreaks - may find clusters of cases
    * collect info about situation within clusters, proximity of cases within clusters -> may reveal something latent, such as source
  * WWII bombing locations data show that targets were random
  * <mark style='background-color:yellow'>Clustering useful for complex multivariable data</mark>
    * Clustering is unsupervised = all variables have same status based on info from explanatory variables (not predicting/learning value of one variable)
    * Exploratory techniques -> grouping is important to interpret data
      * ex) Understanding cancer biology -> tumors clustered based upon anatomical location & histopathology via molecular data (expression) -> clusters may refine disease types
        * Relevance of clusters via evidence (associated w/ different outcomes)
  * CH4 mainly based upon EM algorithm for clustering latent groups
    * CH5 clustering techniques are more general and can be used for complex data
      * <mark style='background-color:yellow'>Most are based on distances beteween pairs of obs (distances vs all, or some)</mark>
        *Does not make assumptions about generative mechanisms of data distributions (normal, gamma-Poisson, etc.)
        * Many clustering algorithms used in softwares (based on diversity of data types & objectives of study)

<img src="http://web.stanford.edu/class/bios221/book/images/ClusteringA.png"  width="500" height="500"> 

> We decompose the choices made in a clustering algorithm according to the steps taken:  
starting from an observations-by-features rectangular table X -> choose an observations-to-observations distance measure and compute the distance matrix, here schematized by the triangle.  
The distances are used to construct the clusters.  
On the left, we schematize agglomerative methods, that build a hierarchical clustering tree;  
on the right, partitioning methods that separate the data into subsets.  
Both types of methods require a choice to be made: the number k of clusters.  
&emsp; For partitionning approaches such as k-means this choice has to be made at the outset;  
&emsp; for hierarchical clustering this can be deferred to the end of the analysis.

* **
<p id="4"><b><font size="5">How do we measure similarity?</b></font><a href="#0"><sup>Return</sup></a></p>
* **
* First, decide what similar means
  * ex) ways to compare birds - distance of size vs weight = one way to cluster
    * or distance of diet vs habitat = different way to cluster
  * **Choose relevant features & combine differences between multiple features into a single number**
    * How distances measured & similarities between obs defined have heavy impact on clustering result

<center><img src="http://web.stanford.edu/class/bios221/book/images/FourDistances_a.png"  width="120" height="120">
<img src="http://web.stanford.edu/class/bios221/book/images/FourDistances_b.png"  width="120" height="120">
<img src="http://web.stanford.edu/class/bios221/book/images/FourDistances_c.png"  width="120" height="120">
<img src="http://web.stanford.edu/class/bios221/book/images/FourDistances_d.png"  width="120" height="120"></center> 

> Equal-distance contour plots according to four different distances: points on any one curve are all the same distance from the center point.

* Similarity selection Choices
  * **Euclidean** = distance between 2 points (A=(a1,...,an)) & (B=(b1,...,bn)) in p-dimensional space (where p=features) -> square root of sum of squares of differences in all p-coordinate directions  
  <button data-toggle="collapse" data-target="#c1">Euclidean</button>
<div id="c1" class="collapse">
\begin{equation*}
d(A,B)=\sqrt{(a_1-b_1)^2+(a_2-b_2)^2+... +(a_p-b_p)^2}.
\end{equation*}
</div>
  * **Weighted Euclidean** = distance by generalizing ordinary Euclidean distance. -> Assign different weights to different directions in feature space
    * Seen in ch2. -> Chi-squared distance
      * Used to compare rows in contingency tables & weight of each feature = inverse of expected value
    * Mahalanobis distance -> another weighted Euclidean distance
      * Accounts for different features having different dynamice range
        * Some features -ve/+ve correlation w/ eachother
      * Weights derived via covarience matrix of features (seen in first question below)
  * **Manhattan/City Block/Taxicab/L1** = distance by sum of absolute differences in all coordinates  
  <button data-toggle="collapse" data-target="#c2">Manhattan</button>
<div id="c2" class="collapse">
\begin{equation*}
d(A,B)=|a_1-b_1|+|a_2-b_2|+... +|a_p-b_p|.
\end{equation*}
</div>
  * **Maximum/L∞** = distance by max absolute differences between coordinates  
  <button data-toggle="collapse" data-target="#c3">Manhattan</button>
<div id="c3" class="collapse">
\begin{equation*}
d_\infty(A,B)= \max_{i}|a_i-b_i|.
\end{equation*}
</div>
  * **Minkowski** = Euclidean distance without the square -> exponent 2 is replaced with m  
  <button data-toggle="collapse" data-target="#c4">Minkowski</button>
<div id="c4" class="collapse">
\begin{equation}
d(A,B) = \left( (a_1-b_1)^m+(a_2-b_2)^m+... +(a_p-b_p)^m \right)^\frac{1}{m}.
\tag{5.1}
\end{equation}
</div>
  * **Edit/Hamming** = Simplest distance to compare character sequences
    * Counts #differences between 2 character strings
      * ex) nucleotide or AA seqs -> different character substitutions associated w/ different contributions to distance (accounts ofr physical/evolutionary similarity)
      * Allows for InDels
  * **Binary** = Distance by two binary vectors, where binary values are coordinates
    * ex) Non-zero elements = on-coordinates & zero elecments = off-coordinates
    * Proportion of features w/ only one binary bit "on-coordinate" among features w/ at least one binary bit "on-coordinate"
  * **Jaccard Distance** = Trait occurenence or features in ecological mutation data -> translated into presence/absence (1's or 0's)
    * Co-occurence more informative than co-abscence
      * ex) compare mutation patterns in HIV
        * Co-existence of mutation in 2 different strains is more important observation vs co-absence of mutation
        * Use **Jaccard Index**
          * Two observation vectors (S & T)
          * f<sub>11</sub> = #times features co-occur in both vectors
          * f<sub>10</sub> = #times features occurs in vector S but not in T (and vice versa f<sub>01</sub>)
          * f<sub>00</sub> #times features co-absent in both vectors  
    <button data-toggle="collapse" data-target="#c5">Jaccard Index(Aka Jaccard Similarity)</button>
<div id="c5" class="collapse">
\begin{equation}
J(S,T) = \frac{f_{11}}{f_{01}+f_{10}+f_{11}},
\tag{5.2}
\end{equation}
Note that the Jaccard Index ignores co-absence (f<sub>00</sub>)
</div>
    <button data-toggle="collapse" data-target="#c6">Jaccard Dissimilarity</button>
<div id="c6" class="collapse">    
\begin{equation}
d_J(S,T) = 1-J(S,T) = \frac{f_{01}+f_{10}}{f_{01}+f_{10}+f_{11}}.
\tag{5.3}
\end{equation}
</div>
  * **Correlation based distance** = distance based upon the correlation of A&B  
    <button data-toggle="collapse" data-target="#c7">Correlation based distance</button>
<div id="c7" class="collapse">
\begin{equation*}
d(A,B)=\sqrt{2(1-\text{cor}(A,B))}.
\end{equation*}
</div>  

  
* Q) Which of the two cluster centers is the red point closest to?
<center><img src="http://web.stanford.edu/class/bios221/book/figure/chap5-Mahalanobis-1.png"  width="500" height="500"></center>
* A) Can use Euclidean distance metric to decide that red point is closest to left cluster
  * Problem = features have different ranges & correlations which differ between both clusters
  * Solution = cluster using specific Mahalanobis distances (Weighted Euclidean)
    * Figure shows contour lines for both clusters -> obtained via density estimate
    * Mahalanobis distance approximates contours w/ ellipses
      * Distance between red point & each cluster centers = #contour lines crossed
      * Can see right group is more spread (red point is actually closer to the right)
      
* Distances computation using R
  * `dist()` designed to use less space vs full n<sup>2</sup> positions (complete n x n distance matrix between n objects)
    * Function computes 1/6 choices of distances (euclidean, maximum, manhattan, canberra, binary, minkowski) -> output vector of values for reconstructing complete distance matrix
    * Function returns object of class "dist" = has relevant vector size n x (n-1)/2  
    
<button data-toggle="collapse" data-target="#E1">ex) 3x3 matrix</button>
<div id="E1" class="collapse">
```{r eval=T}
mx <- c(0,0,0,1,1,1)
my <- c(1,0,1,1,0,1)
mz <- c(1,1,1,0,1,1)

mat <- rbind(mx, my, mz)
dist(mat)
```

```{r eval=T}
dist(mat, method="binary")
```
</div>

* To **access particular distance** (ex) distance between obs 1 &2) -> need to **turn class "dist" to "matrix"**

```{r eval=T}
load(here("BookStuff", "data", "Morder.RData"))
sqrt(sum((Morder[1,]-Morder[2,])^2)) #Euclidean formula
```
```{r eval=T}
as.matrix(dist(Morder))[2, 1]
```

* Going back to the Jaccard distance example as explained above (distance between 2 HIV strains w/ co-mutations)

```{r eval=T}
mut <- read.csv(here("BookStuff", "data", "HIVmutations.csv"))
mut[1:3, 10:16]
```

* Q) Compare Jaccard distance between mutations in HIV data "mut" to correlation based distance (use function `vegdist()` in R-package vegan)  

* A) 
```{r eval=T}
mutJ <- vegdist(mut, "jaccard")
mutC <- sqrt(2*(1-cor(t(mut)))) #Correlation based distance formula
mutJ
```
```{r eval=T}
as.dist(mutC)
```

* Interesting to compare complex objects not in vectors or real numbers via dissimilarities or distances
  * Grower's distance for data of mixed modalities (categorical factors+continuous variables) -> computed using `daisy()`
  * Distances can be defined between pairs of objects (not just points in R or character seqs)
    * ex) `shortest.paths()` from igraph package (ch10) computes distance between vertices on graph
    * `cophenetic()` computes distances betweens leaves of a tree (seen on below image)
    * `dist.multiPhylo()` from distory package can compute distance between trees  
<center><img src="http://web.stanford.edu/class/bios221/book/images/birds_and_dinosaurs.png"  width="500" height="500"></center>

* Jaccard index between graphs can be computed via looking at two graphs built on same nodes -> count #co-occuring edges
  * use `similarity()` from igraph package
  * Distances & dissimilarities used to compare images, sounds, maps, & documents
    * Distance encompasses domain knowledge (if chosen carefully) -> can lead to solution for hard problems w/ heterogenous data
    * Ask waht is relevant closeness/similairty for data -> find usefuly way to represent similarity (Ch9)  
    
<center><img src="http://web.stanford.edu/class/bios221/book/images/DistanceTriangle.png"  width="500" height="500"></center>

> Figure 5.7: The lower triangle of distances can be computed by any of a hundred different functions in various R packages (`vegdist()` in vegan, `daisy()` in cluster, `genetic_distance()` in gstudio, `dist.dna()` in ape, `Dist()` in amap, `distance()` in ecodist, `dist.multiPhylo()` in distory, `shortestPath()` in gdistance, `%dudi.dist()` and `dist.genet()` in ade4).
    

* **
<p id="5"><b><font size="5">Nonparametric mixture detection</b></font><a href="#0"><sup>Return</sup></a></p>
* **

* k-methods: k-means, k-medoids, and PAM (centers of groups = medoids = PAM = partitioning around medoids)
  * Partitioning/iterative relocation methods on high-dimensional data which can't easily apply probability densities (due to dimensionality curse)
    * Can't apply probability densities w/ EM algorithm & parametric mixture modeling
  * After the distance measure, we have to decide on the number of clusters k
    * <mark style='background-color:yellow'>PAM (Partitioning around medoids) works as follows:</mark>
      1. Matrix of _p_ features measured on set of _n_ observations
      2. Randomly pick _k_ distinct cluster centers from the _n_ observations = "seeds"
      3. Assign remaining observations to group based on closest center
      4. In each group, choose a new center from the observations within the group -> sum of distances of group members to new center must be minimized = **medoid**
      5. Repeat steps 3-4 until groups stabilize
  * Each time the PAM algorithm is ran, different inital seeds will be picked in step 2 (same as EM algorithm)
    * This can led to difference in final results
    * `pam()` function in the cluster package
  * Slight variation of the PAM method <u>replaces the medoids to arithmetic means of clusters (centers of gravity) = **k-means**</u>
    * In PAM algorithm, the centers are observations (usually not the case w/ k-means algorithm b/c arithmetic mean medoid)
    * `kmeans()` from the stats package  

<center><img src="http://web.stanford.edu/class/bios221/book/figure/chap5-clust-kmeansastep1-1.png"  width="120" height="120">
<img src="http://web.stanford.edu/class/bios221/book/figure/chap5-clust-kmeansastep1-2.png"  width="120" height="120">   
<img src="http://web.stanford.edu/class/bios221/book/figure/chap5-clust-kmeansastep1-3.png"  width="120" height="120"></center>
> An example run of the k-means algorithm.  
&emsp; The initial, randomly chosen centers (black circles) and groups (colors) are shown in the top panel. The group memberships are assigned based on their distance to centers. At each iteration, the group centers are redefined, and the points reassigned to the cluster centers.

* k-methods are most common methods used for clustering
  * works well when clusters are comparable in size & convex(blob-shape)
    * If clusters are different in size, larger clusters will be broken up (same w/ groups in non-spherical or non-elliptic shapes)
    
* Q) k-means algorithm alternates by computing average pt & assigning points to clusters. How does this alteration, iteration method differ from EM-algorithm
* A) Each point in EM algorithm is used for computing mean of all groups via probabilitically assigned weight
  * Each point in k-means algorithm is assigned to a cluster or not, so points participate only in computation of center of a single cluster
  
#### <mark style='background-color:yellow'>Tight clusters w/ resampling</mark>
  * Some schemes repeat process many times via different intial centers (starting seeds) or resampling datasets
    * Repeating clustering procedure multiple times on the same data, but using different starting points = strong forms
    * Repeated subsampling of dataset then clustering, results in groups of obs which are "almost always" grouped = tight clusters
  * Strong forms vs Tight clusters <u>will determine the number of clusters</u>
  * clusterExperiment package can be used to combine & compare output from many different clusters
  * vignette package on single-cell RNA-seq experiment (which outputs counts of reads representing gene transcripts from single cells)
    * Single cell resolution allows cell lineage dynamic determination
    * Can use clustering for analyzing this data
    
* Q) Use vignette of package clusterExperiment
  * Use ensemble clustering function `clusterMany()` which utilizes PAM algorithm for individual clustering
  * Set choice of genes to include at 60, 100, or 150 most variable genes
  * Plot clustering result for _k_ varying between 4 & 9 -> what do you notice?
* A)
```{r eval=T}
data("fluidigm", package="scRNAseq") #needs scRNAseq package
se <- fluidigm[, fluidigm$Coverage_Type == "High"] #subset the df fluidigm to have all rows but only column "Coverage_Type" where values = "High"
assays(se) <- list(normalized_counts=round(limma::normalizeQuantiles(assay(se)))) #normalizeQuantiles requires the limma package
ce <- clusterMany(se, clusterFunction="pam", ks=5:10, run=TRUE, isCount=TRUE, reduceMethod="var", nFilterDims=c(60,100,150))
clusterLabels(ce) <- sub("FilterDims", "", clusterLabels(ce)) #replace the "FilterDims" values from the cluster labels of ce w/ blank
plotClusters(ce, whichClusters="workflow", axisLine=-1)
```
> Comparison of clustering results (rows), for different numbers of included genes and for varying numbers of clusters, k. Each column of the heatmap corresponds to a celXl, and the colors represent the cluster assignments.
  
* **
<p id="6"><b><font size="5">Clustering examples: flow cytometry and mass cytometry</b></font><a href="#0"><sup>Return</sup></a></p>
* **

* Studying measurements on signle cells improves focus & resolution -> allows analysis of cell types & dynamics
  * Flow cytometry allows simultaneous measurements of ~10 different cell markers
  * Mass cytometry expands collection of measurements to max 80 proteins per cell
  * These techniques allow studying immune cell dynamics
* Flow & mass cytometry 
  * During immune cell devlopment -> unique combos of proteins @surfaces expressed = markers called CD (clusters of differentiation)
    * These are collected by flow cytometry (via fluorescence) or mass cytometry (single cell mass spec)
    * ex) CD4 of Th cells (CD4+)
      * Use bioconductor package for cytometry data (flowCore & flowWiz) to read fcsB dataset
```{r eval=T}
fcsB <- read.FCS(here("BookStuff", "data", "Bendall_2011.fcs"))
slotNames(fcsB)
```

<center><img src="http://web.stanford.edu/class/bios221/book/figure/chap5-ObviousClusters-1.png"  width="250" height="250"></center>
> Scatterplot of two of the variables available in the fcsB data. We can see clear bimodality and clustering in these two dimensions.

* Qa) Look at structure of fcsB (`colnames()`) - how many variables measured?
* Aa) 
```{r eval=T}
length(colnames(fcsB))
```

* Qb) Subset the data to look at first few rows (`Biobase::exprs(fcsB)`) - how many cells were measured?
* Ab)
```{r eval=T}
Biobase::exprs(fcsB[1:3,])
```

#### Data Preprocessing
  * Load table of data which shows mapping between isotopes & markers(antibodies)
    * replace isotope names in columns of fcsB w/ marker names -> easier to read code
```{r eval=T}
markersB <- readr::read_csv(here("BookStuff", "data", "Bendall_2011_markers.csv"))
mt <- match(markersB$isotope, colnames(fcsB))
stopifnot(!any(is.na(mt))) #for this to work, need to rerun fcsB load in 3 code blocks above
colnames(fcsB)[mt] <- markersB$marker
flowPlot(fcsB, plotParameters = colnames(fcsB)[2:3], logy=TRUE) #this is the same as the above figure
```

* Plotting data in 2D shows that cells can be grouped into subpopulations
  * Sometimes, one marker can be used to define populations -> **simple rectangular gating** used to separate populations
    * ex) CD4+ cells can be gated via taking subpopulations w/ high CD4 marker values
  * Cell clustering improved by carefully choosing transformations of data
    * Distibution transformations seen below
    
* **Data Transformation: hyperbolic arcsin (asinh)** - Standard to transform flow & mass cytometry data via special functions (use one)
  * ex) Inverse hyperbolid sine (asinh)
    * Large values of x: asinh(x) behaves like a log and is equal to log(x)+log(2)
    * Small values of x: asinh(x) close to linear in x
  * **This is another example of variance stablizing transformation (n.b. CH4 beeswarm, also in CH8)**

\begin{equation*}
\operatorname{asinh}(x) = \log{(x + \sqrt{x^2 + 1})}.
\end{equation*}

* Run the following code to see two main regimines of asinh(x) transformation (when large values or small values)
```{r eval=T}
v1 = seq(0, 1, length.out = 100)
plot(log(v1), asinh(v1), type = 'l') #when small values, the log(x) is not equal to asinh(x)
plot(v1, asinh(v1), type = 'l') #when small values, asinh(x) behaves linearly
v3 = seq(30, 3000, length = 100)
plot(log(v3), asinh(v3), type= 'l') #when large values, the log(x) is euqal to asigh(x)
plot(v3, asinh(v1), type = 'l') 
```

```{r eval=T}
#This is produced by flowCore package
asinhtrsf = arcsinhTransform(a = 0.1, b = 1)
fcsBT = transform(fcsB, transformList(colnames(fcsB)[-c(1, 2, 41)], asinhtrsf))
densityplot( ~`CD3all`, fcsB)
densityplot( ~`CD3all`, fcsBT)
```

> The top plot shows a simple one dimensional histogram before transformation; on the bottom we see the distribution after transformation. It reveals a bimodality and the existence of two cell populations.

* Q) How many dimensions does the following code use to split the data into 2 groups using k-means algorithm?
```{r eval=T}
kf = kmeansFilter("CD3all" = c("Pop1","Pop2"), filterId="myKmFilter")
fres = flowCore::filter(fcsBT, kf)
summary(fres)

fcsBT1 = flowCore::split(fcsBT, fres, population = "Pop1")
fcsBT2 = flowCore::split(fcsBT, fres, population = "Pop2")
```

* A)
```{r eval=T}
# This is a naive projection of the data into 2D spanned by CD4 & CD56 markers -> so answer is 4 dimensions by k-means

fp <- flowPeaks(Biobase::exprs(fcsBT)[,c("CD3all", "CD56")])
plot(fp)
```

```{r eval=T}
# When plotting points densly populating an area -> avoid overplotting (some techniques in ch3)
  #can use contours & shading as follows (more informative than the previous graph)
flowPlot(fcsBT, plotParameters = c("CD3all", "CD56"), logy = FALSE)
contour(fcsBT[, c(40, 19)], add = TRUE)
```

* The bioconductor package ggcyto allows plotting each patient in a different facet via ggplot

```{r eval=T}
ggcd4cd8 <- ggcyto(fcsB,aes(x=CD4, Y=CD8))
ggcd4 <- ggcyto(fcsB,aes(x=CD4))
ggcd8 <- ggcyto(fcsB,aes(x=CD8))
p1 <- ggcd4+geom_histogram(bins=60)
p1b <- ggcd8+geom_histogram(bins=60)
asinhT <- arcsinhTransform(a=0,b=1)
transl <- transformList(colnames(fcsB)[-c(1,2,41)], asinhT)
fcsBT <- transform(fcsB, transl)
p1t <- ggcyto(fcsBT,aes(x=CD4))+geom_histogram(bins=90)
p2t <- ggcyto(fcsBT,aes(x=CD4, y=CD8))+geom_density2d(colour="black")
p3t <- ggcyto(fcsBT,aes(x=CD45RA, y=CD20)) + geom_density2d(colour="black")

p2t ; p3t
```

#### Desnity-based clustering
* Flow cytometry datasets w/ few markers but large # cells can under-go density-based clustering
  * Clustering would look for regions of high density separated via sparse regions
  * Can work with non convex clusters
  * ex) dbscan method

```{r eval=T}
mc5 <- Biobase::exprs(fcsBT)[,c(15,16,19,40,33)] #markers into clusters
res5 <- dbscan::dbscan(mc5, eps=0.65, minPts=30)
mc5df <- data.frame(mc5, cluster=as.factor(res5$cluster))
table(mc5df$cluster)
```

```{r eval=T}
ggplot(mc5df, aes(x=CD4,    y=CD8,  col=cluster))+geom_density2d()
ggplot(mc5df, aes(x=CD3all, y=CD20, col=cluster))+geom_density2d()
```

>  results of clustering with dbscan using five markers. Here we only show the projections of the data into the CD4-CD8 and C3all-CD20 planes. - overlaps = show multidimensionality of data

* Q) Increase the dimension to 6 by adding one CD marker-variables form input data
  * Vary eps, & find 4 clusters where >= clusters have more than 100 points
  * Repeat w/ 7 CD marker-variables (what is the difference?)
* A) 
```{r eval=T}
# 6 marker example
mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 40, 33, 25)] #6 markers
res = dbscan::dbscan(mc6, eps = 0.65, minPts = 20)
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)
```
Increase #markers = decrease #clusters

```{r eval=T}
# Changing eps= in dbscan()
mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 40, 33, 25)] #6 markers
res = dbscan::dbscan(mc6, eps = 0.55, minPts = 20) #if eps decreased 
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)

# Changing eps= in dbscan()
mc6 = Biobase::exprs(fcsBT)[, c(15, 16, 19, 40, 33, 25)] #6 markers
res = dbscan::dbscan(mc6, eps = 0.75, minPts = 20) #if eps increased
mc6df = data.frame(mc6, cluster = as.factor(res$cluster))
table(mc6df$cluster)
```
Decrease eps = decrease clusters, increase eps = increase clusters

```{r eval=T}
mc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)] #7 markers/dimensions
res = dbscan::dbscan(mc7, eps = 0.65, minPts = 20) #unchanged eps
mc7df = data.frame(mc7, cluster = as.factor(res$cluster))
table(mc7df$cluster)

mc7 = Biobase::exprs(fcsBT)[, c(11, 15, 16, 19, 25, 33, 40)] #7 markers/dimension
res = dbscan::dbscan(mc7, eps = 0.95, minPts = 20) #increased eps
mc7df = data.frame(mc7, cluster = as.factor(res$cluster))
table(mc7df$cluster)
```
* <mark style='background-color:yellow'>If increased dimensions (#makers), need to increase eps to increase #clusters</mark>
  * **This is known as the curse of dimensionality**
  
  > eps	size of the epsilon neighborhood.
  
* dbscan density-based clustering algorithms
  * Clusters pts in dense regions based on _density-connectedness criteria_
    * Looks for small neighbourhood spheres of radius (ϵ) -> views if points are connected
  * Density-reachability concept
    * point q is density-reachable from point p if not further than a threshold (ϵ)
    * if point p is surrounded by many pts (such that p & q may be part of a dense region) -> then q is density-reachable from p if there is a seq of points p1,...,pn w/ p1=p & p<sub>n</sub>=q -> so each p<sub>i+1</sub> is density-reachable from p<sub>i</sub>
    * Cluster is a subset of points w/ following properties
      1. All pts in clusters are mutually density-connected
      2. If a point is density-connected to any point of cluster, then it is part of the cluster
      3. Groups of points must have MinPts points to count as a cluster

> It is important that the method looks for high density of points in a neighborhood. Other methods exist that try to define clusters by a void, or “missing points” between clusters. But these are vulnerable to the curse of dimensionality; these can create spurious “voids”.

* **
<p id="7"><b><font size="5">Hierarchical clustering</b></font><a href="#0"><sup>Return</sup></a></p>
* **

* This a bottom-up approach (start from obs to infer top/common ancestor)
  * Similar observations & subclasses assembled iteratively
  * ex) Linnaeus nested clusters of organisms based on specific characteristics, seen in below image 
<center><img src="http://web.stanford.edu/class/bios221/book/images/LinnaeusClass-01.png"  width="200" height="200"></center>

* **Dendogram ordering**
  * Order of labels doesn't matter within sibling pairs
  * Horizontal distances usually meaningless, vertrical distances have some info for interpretations about neighbours
    * non-monophyletic = not from same subtree/clade -> but can appear as neighbours in a plot (ex) B & D in the below image = non-monophyletic)

<center><img src="http://web.stanford.edu/class/bios221/book/images/SameTree-01.png"  width="400" height="200">
</center>

> Three representations of the same hierarchical clustering tree.

* **Top-down hierarchies**
  * Alternative to top-down approach
  * Takes all objects & spits into sequence based on chosen criteria
    * **Recursive partitioning method** -> makes _decision trees_
  * Useful for predicting things (ex) survival time of diseased)
    * split heterogenous population into homogenous subgroups via partitioning
  * This chapter will focus on bottom-up approaches (top-down partitioning in supervised learning & classification CH12)

#### Computing (dis)similarities between aggregated clusters
* Hierarchical clustering by aggregation requires more than distances between pairs of individual objects
  * **need to also calculate distances between aggregates**
* How to define aggregates?
  * Based on object-obeject distances -> each choice results in different hieratchical clustering type
* Hierarchical clustering algorithms
  * starts by grouping most similar observations
    * If aggregation occurs -> need to determine what distance between newly formed cluster & all other points OR distance between two clusters

* Choices to build hierarchical clustering trees
  * **Minimal jump & Single Linkage/Nearest neighbour method** = finds distance between two clusters using smallest distance between any 2points in the two cluster
    * This method creates clusters looking like continous strings of points (trees are comb-shaped)  
      <button data-toggle="collapse" data-target="#h1">Nearest Neighbour method</button>
<div id="h1" class="collapse">    
\begin{equation*}
d_{12} = \min_{i \in C_1, i \in C_2 } d_{ij}.
\end{equation*}
<center><img src="http://web.stanford.edu/class/bios221/book/images/ClusterStepChoiceSingle1b.png"  width="200" height="200"></center>
> In the single linkage method, the distance between groups C1 and C2 is defined as the distance between the closest two points from the groups.
</div>
      
  * **Maximum jump/Complete linkage method** = Defines distance betwen clusters via largest distance between any 2objects in two clusters  
            <button data-toggle="collapse" data-target="#h2">Complete linkage method</button>
<div id="h2" class="collapse">    
\begin{equation*}
d_{12} = \max_{i \in C_1, i \in C_2 } d_{ij}.
\end{equation*}
<center><img src="http://web.stanford.edu/class/bios221/book/images/ClusterStepChoiceComplete1b.png"  width="200" height="200"></center>
> In the complete linkage method, the distance between groups C1 and C2 is defined as the maximum distance between pairs of points from the two groups.
</div>
        
  * **Average linkage method** = halfway between the nearest neighbour method & complete linkage method  
        <button data-toggle="collapse" data-target="#h3">Average Linkage Method</button>
<div id="h3" class="collapse">    
\begin{equation*}
d_{12} = \frac{1}{|C_1| |C_2|}\sum_{i \in C_1, i \in C_2 } d_{ij}
\end{equation*}
</div>
  
  * **Ward's method** = Analysis of variance approach to minimize variance _within_ clusters
    * Very efficient method
    * Problem = breaks clusters into smaller sizes  
        <button data-toggle="collapse" data-target="#h4">Ward's Method</button>
<div id="h4" class="collapse">    
<center><img src="http://web.stanford.edu/class/bios221/book/images/BetweenWithinb.png"  width="200" height="200"></center>
  > The Ward method maximizes the between group sum of squares (red edges), while minimizing the sums of squares within groups (black edges).
</div>

* Complete list of methods to make hierarchical clustering trees in below dropdown
  * advantage of hierarchical clustering(bottom-up) vs partitioning methods(top-down) = graphical diagnostic of strength of groupings (length of inner edges of tree)
  * Prior knowledge of clusters being approximately the same size -> use average linkage or Ward's method to minimize winithin class variance
  <button data-toggle="collapse" data-target="#h5">Hierarchical Method List</button>
<div id="h5" class="collapse">
|**Method**|**Pros**|**Cons**|
|---|---|---|
|Single linkage|#of clusters| comb-like trees|
|Complete linkage|compact classes| single obs can alter groups|
|Average linkage|similar size & var|not rombust|
|Centroid|rombust to outliers| smaller #clusters|
|Ward|minimize inertia|classes small if high variability|
</div>

<center><img src="http://web.stanford.edu/class/bios221/book/images/single14heatmap.png"  width="200" height="200">
<img src="http://web.stanford.edu/class/bios221/book/images/average14heatmap.png"  width="200" height="200">
<img src="http://web.stanford.edu/class/bios221/book/images/complete14heatmap.png"  width="200" height="200"></center>
  > Three hierarchical clustering plots made with different agglomeration choices. Note the comb-like structure for single linkage in the left. The average and complete linkage trees only differ by the lengths of their inner branches.

* Q) Hierarchical clustering for cell populations (Morder data - gene expression measurements for 156 genes of 3 types of T-cells (naive, effector, memory) from 10 patients)
  * pheatmap package -> make 2 simple heatmaps w/o dendogram or reordering for Euclidean & Manhattan distances
* A)
```{r eval=T}
MorderE <- vegdist(Morder, "euclidean")
pheatmap(MorderE)

MorderM <- vegdist(Morder, "manhattan")
pheatmap(MorderM)
```


```{r eval=T}
# Authors code

## celltypes=factor(substr(rownames(Morder),7,9))
## status=factor(substr(rownames(Morder),1,3))
## ##Just the Euclidean distance
## pheatmap(as.matrix(dist(Morder)),cluster_rows=FALSE,
##         cluster_cols=FALSE,cellwidth=10,cellheight=10)
## ###Manhattan
## pheatmap(as.matrix(dist(Morder,"manhattan")),cluster_rows=FALSE,
##         cluster_cols=FALSE,cellwidth=10,cellheight=10)
```


* Q) Look at differences in orderings in the hierarchical clustering trees w/ the Euclidean & Manhattan distances
  * What differences are noticeable?
* A) <mark style='background-color:red'>Dunno</mark>

* Q) Hierarchical clustering tree is like the Calder mobile (image below) - can swing around many internal pivot pts (nodes) = different orderings of the leafs which are actually still the same order with each tree.
  * In the right tree image, how many ways to order the leaf labels but maintain consistent tree?
<center><img src="http://web.stanford.edu/class/bios221/book/images/CalderHand.png"  width="200" height="200">
<img src="http://web.stanford.edu/class/bios221/book/images/apeclust14.png"  width="200" height="200"></center>

> Left:  
Hierarchical clustering output has similar properties to a mobile: the branches can rotate freely from their suspension points.  
Right:  
This tree can be drawn in many different ways. The ordering of the leaves as it is appears here is (8,11,9,10,7,5,6,1,4,2,3).

* A) each internal connection node can swing, so there are 9 nodes 
  * nPr? order is important
  
* Common to see heatmaps w/ rows and/or columns ordered based on hierarchical clustering tree
  * Makes clusters look stronger than it actually is
  * Alternative ways to ordering rows/columns in heatmaps via NeatMap package (Oridination methods to find orderings CH9)
   
* **
<p id="8"><b><font size="5">Validating and choosing the number of clusters</b></font><a href="#0"><sup>Return</sup></a></p>
* **

* Clustering methods used to give good groupings of data based on various constraints
  * Problem = clustering methods will give good groupings even if no group should exist
  * If there are no real clusters in data -> hierarchical clustering tree may show short branches (difficults to quantify)
  * Need to validate cluster choices w/ criteria, such as
    * **What extent the cluster maximizes the between group differences but keeps within-group distances small**
      * ex) maximize length of red lines & minimize length of black lines in Ward's method figure above  
<button data-toggle="collapse" data-target="#v1">within-groups sum of squared distances (WSS<sub>k</sub>)</button>
<div id="v1" class="collapse">    
\begin{equation} 
\text{WSS}_k=\sum_{\ell=1}^k \sum_{x_i \in C_\ell} d^2(x_i, \bar{x}_{\ell})
\tag{5.4}
\end{equation}
Where:
k= #clusters, C<sub>l</sub>= set of objects in the l-th cluster, x(hat)<sub>l</sub>=center of mass(avrg pt) in l-th cluster
</div>

> dependence on k of the WSS in Equation (5.4) as we are interested in comparing this quantity across different values of k, for the same cluster algorithm. Stated as it is, the WSS is however not a sufficient criterion: the smallest value of WSS would simply be obtained by making each point its own cluster. The WSS is a useful building block, but we need more sophisticated ideas than just looking at this number alone.  
One idea is to look at WSS<sub>k</sub> as a function of k. This will always be a decreasing function, but if there is a pronounced region where it decreases sharply and then flattens out, we call this an elbow and might take this as a potential sweet spot for the number of clusters.  

<button data-toggle="collapse" data-target="#v2">Alternative WSS<sub>k</sub></button>
<div id="v2" class="collapse">    
\begin{equation} 
\text{WSS}_k=\sum_{\ell=1}^k \frac{1}{2 n_\ell} \sum_{x_i \in C_\ell} \sum_{x_j \in C_\ell} d^2(x_i,x_j),
\tag{5.5}
\end{equation}
Where:
k= #clusters, C<sub>l</sub>= set of objects in the l-th cluster, x(hat)<sub>l</sub>=center of mass(avrg pt) in l-th cluster, n<sub>l</sub> is the size of the l-th cluster
</div>

* Q) Use R to compute sum of distances between all pairs of pts in a cluster & compare to WSS<sub>k</sub>
* A) <mark style='background-color:red'>Skipped</mark>

* **The above question was suppose to show within-cluster sum of squares (WSS) measures distances of points in cluster to the center & average distances between all pairs of points in cluster**

* When looking at deciding how many clusters are appropriate for data -> useful to look at cases where we know the actual answer
  * Simulate data coming from 4groups (pipe %>% and `bind_rows()` of dplyr to concatenate 4 tibbles matching in each cluster into one tibble)
  
```{r}
simdat = lapply(c(0, 8), function(mx) { #to list consiting of 0 & 8 - apply the function which intakes mx created in above code code (similarity section)
  lapply(c(0,8), function(my) { #do the same to my
    tibble(x = rnorm(100, mean = mx, sd = 2), #make a tibble of rnorm() numbers
           y = rnorm(100, mean = my, sd = 2),
           class = paste(mx, my, sep = ":"))
   }) %>% bind_rows
}) %>% bind_rows
simdat
```
```{r}
simdatxy = simdat[, c("x", "y")] # without class label
ggplot(simdat, aes(x = x, y = y, col = class)) + geom_point() +
  coord_fixed()
```

> The simdat data colored by the class labels. Here, we know the labels since we generated the data – usually we do not know them.

* Compute within-groups sum of squares for clusters obtained via k-means method

```{r}
wss = tibble(k = 1:8, value = NA_real_)
wss$value[1] = sum(scale(simdatxy, scale = FALSE)^2)
for (i in 2:nrow(wss)) {
  km  = kmeans(simdatxy, centers = wss$k[i])
  wss$value[i] = sum(km$withinss)
}
ggplot(wss, aes(x = k, y = value)) + geom_col()
```

> The barchart of the WSS statistic as a function of k shows that the last substantial jump is just before k=4. This indicates that the best choice for these data is k=4.

* Q) The Calinski-Harabasz index uses WSS & BSS (between group sums of squares) - inspired by F statistic for variance anlysis(F stat = ratio of mean sum of squares via factor to mean residual sum of squares)
  * x(hat) = overall center of mass (avrg pt)
  * Plot CH index for simdat data
  
\begin{equation*}
\text{CH}(k)=\frac{\text{BSS}_k}{\text{WSS}_k}\times\frac{N-k}{N-1}
\qquad \text{where} \quad \text{BSS}_k = \sum_{\ell=1}^k n_\ell(\bar{x}_{\ell}-\bar{x})^2,
\end{equation*}

* A)
```{r}
CH = tibble(
  k = 2:8,
  value = sapply(k, function(i) {
    p = pam(simdatxy, i)
    calinhara(simdatxy, p$cluster)
  })
)
ggplot(CH, aes(x = k, y = value)) + geom_line() + geom_point() +
  ylab("CH index")
```

> The Calinski-Harabasz index, i.,e., the ratio of the between and within group variances for different choices of k, computed on the simdat data.

#### Using the gap statistic
* Take log of WSS (log(WSS<sub>k</sub>)) and compare to averages from simluated data w/ less structure = good way of choosing k (Gap statistic)
  * Compute log(WSS<sub>k</sub>) for range of values of k & #clusters
  * Compare to reference data w/ similar #dimensions & various non-clustered distributions
  * Can use uniform distributed data or data simulated w/ same co-variance structure as original data
  
* Gap statistic algorithm (Uses monte carlo) -> compares gap statistic (log(WSS)) for observed data to average over simulations of data w/ similar structure
  1. Cluster data w/ k clusters -> compute WSS for various k choices
  2. Make B plausible reference data sets via Monte Carlo sampling from homogenous distribution (redo step1 using simulated data) -> Results in B new WSS fo simulated data W*<sub>kb</sub> for b=1,...,B
  3. Compute gap(k)-statistic (first term expected to be larger than 2nd if clustering is good (smaller WSS)- gap statistic should be positive b/c looking for highest value) (g3)
  4. Use standard deviation (g4) to choose best k -> Choose best smallest k based on gap(k) >= gap(k+1) formula below (g4.1)
  
  
\begin{equation*}
\text{gap}(k) = \overline{l}_k - \log \text{WSS}_k
    \quad\text{with}\quad
    \overline{l}_k =\frac{1}{B}\sum_{b=1}^B \log W^*_{kb}
    \tag{g3}
\end{equation*}

\begin{equation*}
\text{sd}_k^2 = \frac{1}{B-1}\sum_{b=1}^B\left(\log(W^*_{kb})-\overline{l}_k\right)^2
\tag{g4}
\end{equation*}

\begin{equation*}
\text{gap}(k) \geq \text{gap}(k+1) - s'_{k+1}\qquad \text{where } s'_{k+1}=\text{sd}_{k+1}\sqrt{1+1/B}.
\tag{g4.1}
\end{equation*}

* Packages cluster & clusterCrit have gap statistic algorithm

* Q) Make a function plotting gap statistic. Show output for simdat example dataset clusted w/ `pam()`
* A)
```{r}
pamfun = function(x, k)
  list(cluster = pam(x, k, cluster.only = TRUE))

gss = clusGap(simdatxy, FUN = pamfun, K.max = 8, B = 50,
              verbose = FALSE)
plot_gap = function(x) {
  gstab = data.frame(x$Tab, k = seq_len(nrow(x$Tab)))
  ggplot(gstab, aes(k, gap)) + geom_line() +
    geom_errorbar(aes(ymax = gap + SE.sim,
                      ymin = gap - SE.sim), width=0.1) +
    geom_point(size = 3, col=  "red")
}
plot_gap(gss)
```

* Using gap statistic on real example to cee how cells cluster 
```{r}
data("x")
selFeats = order(rowVars(Biobase::exprs(x)), decreasing = TRUE)[1:50] #choosing the 50 most variable genes (features) -> reduces influence of tachnical/batch effects - if this is not done, may suppress biological signal
embmat = t(Biobase::exprs(x)[selFeats, ])
embgap = clusGap(embmat, FUN = pamfun, K.max = 24, verbose = FALSE)
k1 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"])
k2 = maxSE(embgap$Tab[, "gap"], embgap$Tab[, "SE.sim"],
           method = "Tibs2001SEmax")
c(k1, k2)
```

* Default choice for #of clusters k1 = first value of k which gap is not > ((first local max)-(standard error(s)))
  * `clausGap()`
  * Gives number of clusters k=9
    * if compared to gap(k) >= gap(k+1)-s`<sub>k+1</sub> -> this would give k=7
    
```{r}
plot(embgap, main = "")
cl = pamfun(embmat, k = k1)$cluster
table(pData(x)[names(cl), "sampleGroup"], cl)
```
    
> we see the comparison between the clustering that we got from pamfun with the sample labels in the annotation of the data.

* Q) How do results change if all feature in x used (rather than subsetting top 50 most variable genes?)
* A)
```{r}
#data("x")
#selFeats1 = order(rowVars(Biobase::exprs(x)), decreasing = TRUE) #not choosing #the 50 most variable genes (features) 
#embmat1 = t(Biobase::exprs(x)[selFeats1, ])
#embgap1 = clusGap(embmat1, FUN = pamfun, K.max = 24, verbose = FALSE)
#k11 = maxSE(embgap1$Tab[, "gap"], embgap1$Tab[, "SE.sim"])
#k21 = maxSE(embgap1$Tab[, "gap"], embgap1$Tab[, "SE.sim"],
#           method = "Tibs2001SEmax")
#c(k11, k21)
#plot(embgap1, main = "")
#cl1 = pamfun(embmat1, k11 = k11)$cluster
#table(pData(x)[names(cl1), "sampleGroup"], cl1)
```
<mark style="background-color:red">skipped this Q</mark>

#### Cluster validation via bootstrap
* Bootstrapping presented in CH4 - want to use subset of samples from existing data
  * Want to apply a cluster method to each subset of data & view how stable the clustering is (or how variable)
    * This uses an index (as explained above for validation) to compare clusters
  * Create new datasets via taking random subsamples from data to look at different clusterings, then compare
    * **Useful to use bootstrap resampling to infer #of clusters via gap statistic index**
    
* Dataset for this example - investigate hypothesis that inncer cell mass (ICM) of mouse blastocyst in embryotic day 3.5 is separated into 2 clusters, pluripotenet epiblast (EPI) & primitive endoderm (PE)
  * The embryotic day 3.25 should not show these 2 clusterings
  * Don't use true group labels in clusters (only use in final interpretation)
  * Use bootstrap on 2different data sets (E3.5 & E3.25)
    * each bootstrap will generate random subset of data to be clustered
    * compare via consensus of many clusters
    * use the clue package `clusterResampling()`
    
```{r}
clusterResampling <- function(x, ngenes=50, k=2, B=250, prob=0.67){
    mat = Biobase::exprs(x)
    ce = cl_ensemble(list = lapply(seq_len(B), function(b) {
    selSamps = sample(ncol(mat), size = round(prob * ncol(mat)),
                      replace = FALSE)
    submat = mat[, selSamps, drop = FALSE]
    sel = order(rowVars(submat), decreasing = TRUE)[seq_len(ngenes)]
    submat = submat[sel,, drop = FALSE]
    pamres = pam(t(submat), k = k)
    pred = cl_predict(pamres, t(mat[sel, ]), "memberships")
    as.cl_partition(pred)
  }))
  cons = cl_consensus(ce)
  ag = sapply(ce, cl_agreement, y = cons)
  list(agreements = ag, consensus = cons)
}
```

* This is how `clusterResampling()` works
  1. Clue1 = take random subset of data (from original E3.25 or E3.5 data) via selecting 67% of samples _w/o replacement_
  2. Pick top ngenes features (in subset data) based on overall variance
  3. Clue2 = apply k-means clustering & predict cluster memberships of samples not found in subset data via distance to cluster centers (uses `cl_predict()` of clue package)
  4. Repeat steps above B times (seq_len(B))
  5. Use consensus clustering (`cl_consensus()`)
  6. clueagree = Measure agreement w/ consusus via `cl_agreement()` for each B clustering. -> Good agreement value = 1, <1 = less agreement value, >1 = cluster into k classes & considered to be stable cluster which is reproducible (if low value, then no stable partition of samples into k clusters)
  

* Measure between-cluser distance for consensus clustering via **Euclidean dissimilarity of membership (sqrt of min sum of squared differences between _u_ and column permutations _v_)**
  * u & v = cluster membership matracies
  * For agreement measurement in step 6 (clueagree) -> quantity 1-d/m used
    * d = Euclidean dissimilarity
    * m = upper bound for max Euclidean dissimilarity
    
```{r}
iswt <- (x$genotype == "WT")
cr1 <- clusterResampling(x[, x$Embryonic.day == "E3.25" & iswt])
cr2 <-clusterResampling(x[, x$Embryonic.day == "E3.5"  & iswt])
```

```{r}
# These plots confirm hypothesis that E3.5 data has 2 clusters
ag1 = tibble(agreements = cr1$agreements, day = "E3.25")
ag2 = tibble(agreements = cr2$agreements, day = "E3.5")
ggplot(bind_rows(ag1, ag2), aes(x = day, y = agreements)) +
  geom_boxplot() +
  ggbeeswarm::geom_beeswarm(cex = 1.5, col = "#0000ff40")
mem1 = tibble(y = sort(cl_membership(cr1$consensus)[, 1]),
              x = seq(along = y), day = "E3.25")
mem2 = tibble(y = sort(cl_membership(cr2$consensus)[, 1]),
              x = seq(along = y), day = "E3.5")
ggplot(bind_rows(mem1, mem2), aes(x = x, y = y, col = day)) +
  geom_point() + facet_grid(~ day, scales = "free_x")
```

> Cluster stability analysis with E3.25 and E3.5 samples. Top: beeswarm plots of the cluster agreements with the consensus, for the B clusterings; 1 indicates perfect agreement, lower values indicate lower degrees of agreement. Bottom: membership probabilities of the consensus clustering. For E3.25, the probabilities are diffuse, indicating that the individual clusterings often disagree, whereas for E3.5, the distribution is bimodal, with only one ambiguous sample.  

* Computational & memory issues
  * Computation of all comparitive distances of n objects is O(n<sup>2</sup>) opertation of time/memory
  * Classical hierarchical clustering (`hclust()` in stats package) is O(n<sup>3</sup>) in time/memory
  * When large n, these methods are impractical
    * Computational complexity -> algorithm is O(n<sup>k</sup>) 
      * if n increases = larger resource consumption (CPU time/memory) grows proportional to n<sup>k</sup>
      * This also increases baseline costs (but lowering n also lowers power) -> this is negligible compared to when n->infinity
    * ex) distance matrix for 1million objects stored at 8-bit would take up 4tb (if hclust algorithm ran = ~30 years process)
  * Avoid computation of all-vs-all distance matrix via
    * k-means uses only O(n) computations -> only keeps track of distances between each object & cluster centers (#always remains same even w/ increase n)
    * fastclust & dbscan packages optimized to deal w/ large n observations 

* **
<p id="9"><b><font size="5">Clustering as a means for denoising</b></font><a href="#0"><sup>Return</sup></a></p>
* **

* Ex) A set of measurements reflecting some hidden true values (ex) spp. represented by DNA seqs via genomes)
  * What if the signal of these measurements have been degraded by noise?
  * Clustering used to remove noise

* Noisy observations w/ different baseline freqs
  * Ex) bivariate distribution of obs w/ same error variances
    * Sampling from two groups w/ different baseline frequencies
    * Errors are continous independent bivariate normally distributed
    * 10<sup>3</sup> of seq1 & 10<sup>5</sup> of seq2 generated by the following code
    
```{r}
seq1 = rmvnorm(n = 1e3, mu = -c(1, 1), sigma = 0.5 * diag(c(1, 1)))
seq2 = rmvnorm(n = 1e5, mu =  c(1, 1), sigma = 0.5 * diag(c(1, 1)))
twogr = data.frame(
  rbind(seq1, seq2),
  seq = factor(c(rep(1, nrow(seq1)),
                 rep(2, nrow(seq2))))
)
colnames(twogr)[1:2] = c("x", "y")
ggplot(twogr, aes(x = x, y = y, colour = seq,fill = seq)) +
  geom_hex(alpha = 0.5, bins = 50) + coord_fixed()
```

> observed values  
although both groups have noise distributions with the same variances, the apparent radii of the groups are very different. The 10<sup>5</sup> instances in seq2 have many more opportunities for errors than what we see in seq1, of which there are only 10<sup>3</sup>. Thus we see that frequencies are important in clustering the data.

* Q) take seq1 & seq2 data then cluster into 2 groups based on distance from group center
  * Do you think results should depend on the frequencies of each of the 2 seqs?
* A) <mark style='background-color:yellow'>This approach is used in taxonomic clustering (OTU's) which is not optimal</mark>
  * Because this method relies on similarities which have inherent biases in the representiveness heuristic
    * Why is representiveness & distance-based heuristics in clustering & taxonomic assignment biased?
      * We assign groups by looking for similar representitives (clustering/group assignment = assign new seq to group based on distance to center)
        * ex) take balls w/ same radius (but have differences in other forms that are not considered)
      * Problem = grouping based on distance but what about other considerations to group?

* Simulate n=2000 binary variables w/ length=200 which indicates the quality of n sequencing reads
  * Assume sequencing errors occur independent & uniformly w/ prob perr=0.001
  * Only care if base called is correct (TRUE) or not (FALSE)
  
```{r}
n <- 2000
len <- 200
perr <- 0.001
seqs <- matrix(runif(n*len) >= perr, nrow=n, ncol=len)
```

* Compute pairwise distances between reads
```{r}
dists <- as.matrix(dist(seqs, method="manhattan"))
```

* Various values of number of reads(k from 2 to n), max distance in this set of reads computed by below code

```{r}
dfseqs = tibble(
  k = 10 ^ seq(log10(2), log10(n), length.out = 20),
  diameter = vapply(k, function(i) {
    s = sample(n, i)
    max(dists[s, s])
    }, numeric(1)))
ggplot(dfseqs, aes(x = k, y = diameter)) + geom_point()+geom_smooth()
```

>The diameter of a set of sequences as a function of the number of sequences.

#### Denoising 16S rRNA seqs
* Improve 16S read clustering via denoising mechanism to incorporate error probabilities
* Uses an iterative alternating apporach (similar to EM algorithm in CH4) to build probability noise model from data 
  * <mark style='background-color=yellow'>This is known as the de novo method</mark> = Uses clustering, cluster centers used to denoise sequence variances (Amplicon sequence variants ASVs)
    * Once all denoised variants found -> make contingency table to of counts per different samples (CH10 shows how these tables can be used to infer properties of bacerial communties via networks & graphs)
* Improve data quality, start w/ raw data & model all sources of variation (starting from scratch)

* Q) If we have 2 seqs w/ length 200 in out sample w/ different abundances
  * Told about technological sequencing errors occruing as independent Bernoulli(0.0005) random events for each nt
  * What is the distribution of #of errors per seq? 
* A) Probability theory tells that sum of 200 indepdendent Poisson(lambda=0.0005) will be Poisson(lambda=0.1)  
  * Verify via Monte Carlo simulation
```{r}
simseq10K <- replicate(1e5, sum(rpois(200,0.0005)))
mean(simseq10K)
```
```{r}
vcd::distplot(simseq10K, "poisson")
```

> displot shows us how close the distribution is to being Poisson distributed.  

#### Infer sequence variants
* <mark style='background-color:yellow'>DADA method (Diverse Amplicon Denoising Algorithm) uses parametrized model for substitution errors</mark>
  * This distinguishes seq errors from real biological variation
  * The model computes probabilities of base substituitons (ex) seeing A instead of C)
    * Assumes probabilties are independent of the position along sequence
    * Error rates vary substantially betwen seq runs & PCR protocols -> model parameters estimated form data using EM-type approach
      * Read classified as noisy or extact based upon given parameters -> updates noise model parameters per iteration
        * In large data set, noise mdoel estimation step doesn't have to be done on the complete set [large data tips here](https://benjjneb.github.io/dada2/bigdata.html)
        
* Dereplicated sequences (Fwd & reverse) are read in then denoising & estimation is used w/ dada function via the following code
```{r}
derepFs = readRDS(here("BookStuff", "data", "derepFs.rds"))
derepRs = readRDS(here("BookStuff", "data", "derepRs.rds"))

ddF = dada(derepFs, err = NULL, selfConsist = TRUE)
ddR = dada(derepRs, err = NULL, selfConsist = TRUE)
```

* Varify error transition rates are resonably well estimated via fitting observed error rates (black points) & fitted error rates (black lines)

```{r}
plotErrors(ddF)
```

>  Forward transition error rates as provided by plotErrors(ddF). This shows the frequencies of each type of nucleotide transition as a function of quality.

* Once errors estimated -> algorithm reran on data to find ASVs
```{r}
dadaFs = dada(derepFs, err=ddF[[1]]$err_out, pool = TRUE)
dadaRs = dada(derepRs, err=ddR[[1]]$err_out, pool = TRUE)
```

* **Note**
  * Sequence inference function has two different modes
    * Independent inference by sample (pool=FALSE)
      * Two advantages
        * Functions of the number of samples - linear computation time & constant memory useage
    * Pooled inference from sequencing reads combined from all samples
      * More computationally taxing but can improve detection of singletons (rare variants) which occur once/twice in sample but more often accross all samples
      * Works well for small datasets
      
* **Sequence inference removes almost all substition and InDel errors from data**
* Merge the inferred fwd & reverse seqs by removing paired seqs that don't have perfect overlaps = another control from residual errors 

```{r}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs)
```

* Now produce a contingency table of counts of ASVs (A higher resoluton analogue of the traditional OTU table)
  * Contingency table = samples by features table -> each cell has the number of times a sequence variant is observed in each sample
  
```{r}
seqtab.all <- makeSequenceTable(mergers[!grepl("Mock",names(mergers))])
```

* Q) Explore the components ofthe objects dadaRs & mergers
* A) 
```{r, message=F, warning=F}
#glimpse(c(dadaRs, mergers))
```

```{r}
#length(dadaRs)
#length(dadaFs)
#class(dadaRs)
#names(dadaRs)
#class(mergers)
#length(mergers)
```


> dadaRs is a list of length 20. Its elements are objects class dada that contain the denoised reads. We will see in Chapter 10 how to align the sequences, assign their taxonomies and combine them with the sample information for downstream analyses.

* Chimera sequences are artifically made during PCR amplification (concatenating >=2 sequences)
  * Need to remove with the function `removeBimeraDenovo()` for a clean contengency table

```{r}
seqtab <- removeBimeraDenovo(seqtab.all)
```

* Q) Why are chimera sequences easy to recognize?
  * What proportion of reads were chimeric in the seqtab.all data?
  * What proportion of unique seq variants are chimeric?
* Easy to recognize b/c of excessive length/merge of expected fwd & reverse reads
  * Only 7% of all reads were observed as chimeric from sequence variants

* **
<p id="11"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>
* **

#### Excercise 5.1

> We can define the average dissimilarity of a point xi to a cluster Ck as the average of the distances from xi to all points in Ck . Let A(i) be the average dissimilarity of all points in the cluster that xi belongs to. Let B(i) be the lowest average dissimilarity of xi to any other cluster of which xi is not a member. The cluster with this lowest average dissimilarity is said to be the neighboring cluster of xi , because it is the next best fit cluster for point xi . The silhouette index is
\begin{equation*}
S(i)=\frac{B(i)-A(i)}{\max_i(A(i),B(i))}.
\end{equation*}

* 5.1a Compute the silhoutte index for the simdat data previously simulated

```{r}
pam4 = pam(simdatxy, 4)
sil = silhouette(pam4, 4)
plot(sil, col=c("red","green","blue","purple"), main="Silhouette")
```
Seems the average index caluclated from k=4 clusters is 0.51


* 5.1b
```{r}
#pam4 = pam(simdatxy, 3)
#sil = silhouette(pam4, 3)
#plot(sil, col=c("red","green","blue","purple"), main="Silhouette")

#pam4 = pam(simdatxy, 5)
#sil = silhouette(pam4, 5)
#plot(sil, col=c("red","green","blue","purple"), main="Silhouette")
```
Increasing or decreasing k clusters seems to give worse index scores, so it seems 4 is the best

* 5.1c Repeat for groups w/ uniform (unclustered) data distributions over a whole range of values 

```{r}
set.seed(1234)
simdatU = lapply(c(0, 8), function(mx) { #to list consiting of 0 & 8 - apply the function which intakes mx created in above code code (similarity section)
  lapply(c(0,8), function(my) { #do the same to my
    tibble(x = runif(100, min=0, max=100), #make a tibble of rnorm() numbers
           y = runif(100, min=0, max=100),
           class = paste(mx, my, sep = ":"))
   }) %>% bind_rows
}) %>% bind_rows
simdatUXY <- simdatU[,c("x","y")]
```

```{r}
pamU = pam(simdatUXY, 4)
silU = silhouette(pamU, 4)
plot(silU, col=c("red","green","blue","purple"), main="Silhouette")
```

#### Excercise 5.3
* 5.2a Make a "character" representation of distances between the 20 locations in the dune data w/ vegan package using function `symnum()`

```{r}
data(dune)
duned <- dist(dune, method="binary") 
symnum(as.matrix(duned))
```

* 5.2b Make a heatmap of these distances
```{r}
pheatmap(as.matrix(duned))
```

#### Excercise 5.3

* 5.3a Load the spiral data from kernlab package & plot results using k-means on data
```{r}
data(spirals)
spiralk <- kmeans(spirals,2)
plot(spirals, col = c("blue", "red")[spiralk$cluster])
```

> An example of non-convex clusters. On the left we show the result of k-means clustering with k=2.

* 5.3b Clustering above is unsatisfactory, use a different method from package specc or dbscan to cluster spiral data

```{r}
spiralss <- specc(spirals, centers=2)
plot(spirals, col = c("blue", "red")[spiralss])
```

* 5.3c Repeat specc clustering w/ different #groups/clusters
```{r}
spiralss5 <- specc(spirals, centers=5)
plot(spirals, col = c("blue", "red", "green", "orange", "black")[spiralss5])
```
How rombust? Idk

#### Excercise 5.4

> Looking at graphical representations in simple two-dimensional maps can often reveal important clumping patterns. We saw an example for this with the map that enabled Snow to discover the source of the London cholera outbreak. Such clusterings can often indicate important information about hidden variables acting on the observations. Look at a map for breast cancer incidence in the US [here](http://www.huffingtonpost.com/bill-davenhall/post_1663_b_817254.html) (Mandal et al. 2009); the areas of high incidence seem spatially clustered. Can you guess the reason(s) for this clustering and high incidence rates on the West and East coasts and around Chicago?

* Skipped

#### Excercise 5.5 

>Amplicon bioinformatics: from raw reads to dereplicated sequences. As a supplementary exercise, we provide the intermediate steps necessary to a full data preprocessing workflow for denoising 16S rRNA sequences. We start by setting the directories and loading the downloaded data:

```{r}
base_dir = "../BookStuff/data"
miseq_path = file.path(base_dir, "MiSeq_SOP")
filt_path = file.path(miseq_path, "filtered")
fnFs = sort(list.files(miseq_path, pattern="_R1_001.fastq"))
fnRs = sort(list.files(miseq_path, pattern="_R2_001.fastq"))
sampleNames = sapply(strsplit(fnFs, "_"), `[`, 1)
if (!file_test("-d", filt_path)) dir.create(filt_path)
filtFs = file.path(filt_path, paste0(sampleNames, "_F_filt.fastq.gz"))
filtRs = file.path(filt_path, paste0(sampleNames, "_R_filt.fastq.gz"))
fnFs = file.path(miseq_path, fnFs)
fnRs = file.path(miseq_path, fnRs)
print(length(fnFs))
```


> The data are highly-overlapping Illumina Miseq 2×250
 amplicon sequences from the V4 region of the 16S rRNA gene (Kozich et al. 2013). There were originally 360 fecal samples collected longitudinally from 12 mice over the first year of life. These were collected by Schloss et al. (2012) to investigate the development and stabilization of the murine microbiome. We have selected 20 samples to illustrate how to preprocess the data.  
We will need to filter out low-quality reads and trim them to a consistent length. While generally recommended filtering and trimming parameters serve as a starting point, no two datasets are identical and therefore it is always worth inspecting the quality of the data before proceeding. We show the sequence quality plots for the two first samples in Figure 5.36. They are generated by:

```{r}
plotQualityProfile(fnFs[1:2]) + ggtitle("Forward")
plotQualityProfile(fnRs[1:2]) + ggtitle("Reverse")
```

> Quality scores. The lines show positional summary statistics: green is the mean, orange is the median, and the dashed orange lines are the 25th and 75th quantiles.

> Note that we also see the background distribution of quality scores at each position in Figure 5.36 as a grey-scale heat map. The dark colors correspond to higher frequency.

#### Excercise 5.6
* Generate similar plots for four randomly selected sets of forward and reverse reads. Compare forward and reverse read qualities; what do you notice?

```{r}
plotQualityProfile(fnFs[3:4]) + ggtitle("Forward")
plotQualityProfile(fnRs[3:4]) + ggtitle("Reverse")
#plotQualityProfile(fnFs[5:6]) + ggtitle("Forward")
#plotQualityProfile(fnRs[5:6]) + ggtitle("Reverse")
#plotQualityProfile(fnFs[7:8]) + ggtitle("Forward")
#plotQualityProfile(fnRs[7:8]) + ggtitle("Reverse")
#plotQualityProfile(fnFs[9:10]) + ggtitle("Forward")
#plotQualityProfile(fnRs[9:10]) + ggtitle("Reverse")
```

The quality of the reverse reads always suck compared to the forward. Why? I would also like to know. 

#### Excercise 5.7

> Here, the forward reads maintain high quality throughout, while the quality of the reverse reads drops significantly at about position 160. Therefore, we truncate the forward reads at position 240, and trimm the first 10 nucleotides as these positions are of lower quality. The reverse reads are trimmed at position 160. Combine these trimming parameters with standard filtering parameters remember to enforce a maximum of 2 expected errors per-read. (Hint: Trim and filter on paired reads jointly, i.e., both reads must pass the filter for the pair to pass. The input arguments should be chosen following the dada2 vignette carefully. We recommend filtering out all reads with any ambiguous nucleotides.)

* **
<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>
* **

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*

* Linux
  * `find . -name *.fastq` & to remove everything found `find -name *fastq -exec rm {} \;`


* **arithmetic mean** = average of set of numerical values

* From wiki

>Silhouette refers to a method of interpretation and validation of consistency within clusters of data. The technique provides a succinct graphical representation of how well each object has been classified.[1]  
The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). **The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters. If most objects have a high value, then the clustering configuration is appropriate.** If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.  
The silhouette can be calculated with any distance metric, such as the Euclidean distance or the Manhattan distance.

```{r}
# Wanted to know how this works - here is authors solution
out = filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(240,160),
        maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,  trimLeft=10,
        compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
head(out)
```
```{r}
derepFs = derepFastq(filtFs, verbose = FALSE)
derepRs = derepFastq(filtRs, verbose = FALSE)
names(derepFs) = sampleNames
names(derepRs) = sampleNames
```


#### Excercise 5.8
>Use R to create a map like the one shown in Figure 5.2. Hint: go to the [website of the British National Archives](http://bombsight.org/#15/51.5050/-0.0900) and download street addresses of hits, use an address resolution service to convert these into geographic coordinates, and display these as points on a map of London.

* skipped

</body></font>
  