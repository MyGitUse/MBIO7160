---
title: "Chapter 8: High-Throughput Count Data"
output: html_notebook
---


```{r libraries, message=F, warning=F}
library(pasilla) #bioconductor
library(tibble)
library(DESeq2)
library(ggplot2)
library(matrixStats)
library(dplyr)
library(pheatmap)
library(vsn) #bioconductor
library(edgeR) #bioconductor
```

<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> High-Throughput Count Data  
<a href="#2">[2]</a> Summary  
<a href="#3">[3]</a> Concepts  
<a href="#4">[4]</a> Count data  
<a href="#5">[5]</a> Modeling count data  
<a href="#6">[6]</a> A basic analysis  
<a href="#7">[7]</a> Critique of default choices and possible modifications  
<a href="#8">[8]</a> Multi-factor design and linear models  
<a href="#9">[9]</a> Generalized linear models  
<a href="#10">[10]</a> Two-factor analysis of the pasilla data  
<a href="#11">[11]</a> Further statistical concepts  
<a href="#12">[12]</a> Excercises  
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">High-Throughput Count Data</b></font><a href="#0"><sup>Return</sup></a></p>
* Many biolology machines based on parallel sampling & counting of molecules
  * ex) high-throughput DNA seq
* 2 classes of data output
  1) Output are sequences (ex) polymorphisms or seq differences)
  2) Output is abundance of different sequence regions (based on assembled+annotated reference genome alignment)
* In RNA-Seq = sequence RNA moleucles (cDNA) in population of cells/tissues
* In ChIP-Seq = sequence DNA regions bound to DNA-binding proteins (via immuno-precipitation)
* In RIP-seq = RNA molecules/regions bound by RNA-binding protein
* In DNA-seq = Sequence gneomic DNA and want to know SNPs in heterogenous population
* In HiC (high-throughpput chromatin conformation capture) = map 3D apatial arrangement of DNA
* In Genetic screens (RNAi/CRISPR-Cas9) = want tot know proliferation/survival of cells after gene knockdown/knockout/modification
* In microbiome analysis = want to know aboundance of microbial species in complex habitats

* Ideal situation is to seq and count all molecules of interest in sample
  * This is usually impossible b/c protocols are not 100% efficient
  * Some molecules lost during each step
  * Solution
    * Sequence & count a **staticical sample**
      * Sample size depends on complexity of sequence pool assayed
      * Hope that sample size is enough to represent all trends + patterns

* **

<p id="2"><b><font size="5">Chapter Summary</b></font><a href="#0"><sup>Return</sup></a></p>

* Analyzed count tables from high-throughput seq (& analagous data types) for difference in abundance
  * Used a framework of linear models
    * Can be used to analyze basic 2group comparison + complex multifactorial designs/experiments w/ covariates >2 levels or continous
    * Ordinary linear models -> sampling distribution of data around expected value is **assumed independent+normal w/ 0 mean+same variances**
      * count data -> distributions discrete & skewed (asymmetric) w/ high variances across a range
        * Used generalization of oridinary linear models (Generalized linear models GLM)
          * Considered gamma-Poisson distributed data w/ dispersion parameters needed to estimate from data
          
* Sampling depth differs from different seq runs (replicates) -> estimate effect of this variable parameter & account in model
  * Used size factor (si)
  * This part of the analysis = normalization
  
* In experiments, #replicates is usually too small to estimate dispersion parameter from data for each gene 
  * Use shrinkage/emperical Bayes technique (large gains in percision w/ low bias)
  
* Generalized linear models allows modeling data on original scale
  * Sometimes need to transform data scale to where data is more homoskedastic (less variance) & fill range more uniform
    * ex) plot data/subject to general purpose clustering/dimesnion reduction/learning methods
      * Used variance stabilizing transformation
      
* Critique of differential expression testing = null hypothesis -> effect size is exactly 0
  * This is almost never true, so apporach is not consistent estimator of differentially expressed gene
    * Can overcome by considering effect size + statistical significance
    * Can use "banded" null hypothesis 

* **
<p id="3"><b><font size="5">Concepts</b></font><a href="#0"><sup>Return</sup></a></p>

* Key terminology
  * Seq library = colelction of DNA inputted into machine
  * Fragments = molecules sequenced via framenting DNA/cDNA (max length = 300-100 bp)
  * Read = sequence outputted from fragment sequencing (covers one or both ends ~150 bp)
  
* During Sequencing & counting, there is a aggregation (clustering) step which groups all sequences together
  * ex) all reads from same gene (RNA-seq)/from same binding region (ChIP-seq)
  * Several approaches/choices depending on aim of experiment (best to check literature)
  * Methods use alignment or hash-based mapping to reference seq (ex) RNA-seq genome+annotation of transcript) or Reference-independent seq similarity based clustering of reads (no obvious reference -> metagenomics/metatranscriptomics)
    * Choose to consider different alleles/isoforms separately or merge into equivalence class

* Use the term **gene** in this chapter to represent operational aggregates (clustering)

* **
<p id="4"><b><font size="5">Count data</b></font><a href="#0"><sup>Return</sup></a></p>

* Load data from pasilla package
```{r}
fn <- system.file("extdata", "pasilla_gene_counts.tsv", package="pasilla",mustWork=T) # system.file() locates file in pasilla package

counts <- as.matrix(read.csv(fn, sep="\t",row.names = "gene_id"))
```

* Data stored in rectangular table (tab-delimited file) -> matrix counts

```{r}
dim(counts)
```

```{r}
counts[2000+(0:3),] #prints data from beginning + random point in middle
```

* The counts matrix tallies #reads for each gene in sample = **count table**
  * 14,599 rows = genes
  * 7 columns = samples
  * Matrix w/ integer value
    * value in ith row + jth column of matrix = how many reads mapped to gene i in sample j
    * Statistical sampling models discussed will rely on values of direct "raw" counts of seq reads (not derived quantity (ex) normalized counts/counts of covered bp) -> leads to nonsense results)
    
#### Challenges of count data 
* Data have large range (0 to millions)
  * varience & distribution shape of data in different parts of range differs = **heteroskedasticity**
* Data are non-negative integers & distribution not symmetric
  * Log-normal distribution models can be poor fit
* Need to understand sampling bias & adjust
  * This is called **normalization**
    * ex) total seq depth of experiment (even if true abundance of a gene in 2 libraries are same -> expect different #reads - depends on total#reads seq'd)
    * ex) differing sampling probabilities (even if true abundance of two genes in sample are same -> expect different #reads if biophysical properties differ (%GC, 2ndary structure, binding partners))
* Need to understand stochatic properties of sampling + other sources of stochastic experiment variation
  * Studies w/ large #biological samples = resampling/permutation based methods
  * Harder for designed experiments w/ limited sample size
  * ex) 4 replicates from untreated & treated condition in pasilla data
    * resampling/permutation based methods don't have enough power
    * Need to make distribution assumptions
      * what allows for computation of **probabilities of rare events in tails of distribution** -> really high/low counts from small #of distribution parameters
* Estimation of dispersion parameters is difficult w/ small sample sizes
  * need to make further assumptions -> genes w/ similar locations have similar dispersions = sharing of info across genes
    * Dispersion parameters = distributions parameterized in various ways
      * parameters = measure of location & dispersion
      * ex) mean is a measure of location & variance/stdev is a measure of dispersion

#### RNA-seq: what about gene structures, splicing, isoforms?
* Euk genes are complex -> multi-exon & mRNAs from concatenated exons via splicing
  * Alternative splicing & choices of start/stop sites -> forms many alternative isoforms from the same gene locus
    * Can use high-throughput seq to detect isoform structures of transcripts
* Fragments characteristic for specific isoform -> can detect isoform specific abundances 
* Current RNA-seq data -> short fragments of full-length isoforms = hard to assemble full length isoform structures/abundances
  * Procedures have modest aim of making local statements (inclusion/exclusion of individual exons) = more rombust

* **
<p id="5"><b><font size="5">Modeling count data</b></font><a href="#0"><sup>Return</sup></a></p>

#### Dispersion
* ex) seq library w/ n1 fragments for gene1, n2 fragments for gene2, etc
  * Total library size of n=n1+n2+...
  * Sequence this library and determine identity of r randomly sampled fragments
* Look at orders of magnitude of these #'s
  * #genes in tens of thousands
  * values of n depends on amount of cells used
    * bulk RNA-seq = billions-trillions
  * #reads r usually in tens of millions (much smaller vs n)
* Conclude probability given read maps to ith gene is p=ni/n
  * independent of outcomes from all other reads
  * Model #reads for gene i via Poisson distribution = rate of Poisson is product of pi (initial proportion of framents for ith gene * r)
    * \begin{equation}\lambda_i=rp_i\end{equation}
    
* Usually not interested in modeling read counts w/ single library
  * Want to compare counts between libraries (see differences between biological conditions)
    * ex) same cell w/ or w/o drug treatment -> larger than expected by chance
      * larger than expected between biological replicates
* **Replicate experiments vary more than what Poisson distribution predicts**
  * pi (& lambdai) vary between biological replicates
    * ex) temp to grow cells, drug concentration varies, incubation time
    * solution: need to add another layer of modeling
      * gamma-Poisson (negative bionomial) distribution is good for this
        * Instead of a single lambda (represents mean & variance)
        * gamma-Poisson has 2 parameters
          * can be different for each gene
* Want to consider **sampling w/o replacement** & multinomial distribution
  * probability of sampling read for ith gene depends on #times same genes/other genes already sampled
  * Dependencies are negligibly small so can ignore
    * B/c n much larger than r -> #genes large and each individual ni is small vs summed total (n)
    
* n=total library size (total #fragemnts)
    
#### Normalization
* Systematic biases affect data generation - should account for
* The term **normalization** commonly used for this (often misleading - > nothing to do w/ normal distibution or data transformation)
  * Want to identify nature & magnitude of systematic biases -> account in model-based analysis

* Important systematic bias from variations in total #reads for each sample
  * More reads in library1 vs library2 -> may assume counts are proportional to eachother w/ proportional factor (s)
    * can naively propose estimate of s for each sample based on sum of counts of all genes
    * There is a better method to this
    
* Ex) Dataset w/ 5 genes + 2 samples
  * If estimate s for each of the 2 samples via sum of counts -> slope of blue line is the ratio (see below image)
    * This means, gene C is downregulated in sample2 vs sample1 & other genes lowly upregulated
  * If we estimate s so that ratios -> slope of red line
    * Still conclude C is downregulated & other genes are unchanged
    * This estimate is more parsimonious & perferred
      * Slope via rombust regression via DESEQ2

<center><img src="http://web.stanford.edu/class/bios221/book/figure/chap7-rnaseq-normalization-1.png"  width="250" height="250"></center>
> Size factor estimation. The points correspond to hypothetical genes whose counts in two samples are indicated by their x- and y-coordinates. The lines indicate two different ways of size factor estimation explained in the text.

* Q) For example dataset count, how does output of DESeq2 `estimateSizeFactorsForMatrix()` compared to `colSums()`?
* A) Not much different, results are nearly proportional
```{r}
ggplot(tibble(`size factor` = estimateSizeFactorsForMatrix(counts), `sum`=colSums(counts)), aes(x=`size factor`, y=`sum`)) + geom_point() #note ` vs ' generates different plots
```
> Size factors versus sums for the pasilla data.

* Task: Locate the R sources for this book and have a look at the code that produces Figure 8.1.

```{r}
szfcDemo = data.frame(
  x = c(2, 4, 6, 6,  8) * 10,
  y = c(3, 6, 2, 9, 12) * 10,
  name = LETTERS[1:5],
  check.names = FALSE)
slopes =  c(
  blue = with(szfcDemo, sum(y) / sum(x)),
  red = szfcDemo[, c("x", "y")] %>% as.matrix %>%
    (DESeq2::estimateSizeFactorsForMatrix) %>% (function(x) x[2]/x[1]) %>% as.vector)
ggplot(szfcDemo, aes(x = x, y = y, label = name)) + geom_point() +
  coord_fixed() + xlim(c(0, 128)) + ylim(c(0, 128)) + xlab("sample 1") + ylab("sample 2") +
  geom_text(hjust= 0.5, vjust = -0.6) +
  geom_abline(slope = slopes[1], col = names(slopes)[1]) +
  geom_abline(slope = slopes[2], col = names(slopes)[2])
```

* Q) Plot mean-variance relationship for biological replicates in pasilla dataset
* A)
```{r}
sf = estimateSizeFactorsForMatrix(counts)
ncounts  = counts / matrix(sf,
   byrow = TRUE, ncol = ncol(counts), nrow = nrow(counts))
uncounts = ncounts[, grep("^untreated", colnames(ncounts)),
                     drop = FALSE]
ggplot(tibble(
        mean = rowMeans(uncounts),
        var  = rowVars( uncounts)),
     aes(x = log(mean), y = log(var))) +
  geom_hex() + coord_fixed() + theme(legend.position = "none") +
  geom_abline(slope = 1:2, color = c("forestgreen", "red"))
```
> Variance versus mean for the (size factor adjusted) counts data. The axes are logarithmic. Also shown are lines through the origin with slopes 1 (green) and 2 (red).

* Green line(slope1) = expected if variance(v) = mean(m)
  * Case for Poisson-distributed random variable (v=m)
  * This approximately fits data in the lower range
* Red line(slope2) = quadratic mean-variance relationship (v=m<sup>2</sup>)
  * Lines parallel (not shown) would represent v=cm<sup>2</sup> for various values of c
  * This approximately fits data in the upper range
    * Quadratic relationship approximately fits data for some value of c<1

* **
<p id="6"><b><font size="5">A basic analysis</b></font><a href="#0"><sup>Return</sup></a></p>

* Using pasilla data (experiment on Drosophila melanogaser cell cultures w/ RNAi knockdown of splicing factor pasilla to see effect on transcriptome)
  * Two experimental conditions (treated & untreated on header of count table)
  * Corresponds to -ve control & siRNA against pasilla gene
  * Experimental metadata of the 7 samples in this dataset is as follows
  
> Load the file pasilla_sample_annotation.csv that comes with the pasilla package. We locate it with the function system.file. When you work with your own data, you will need to prepare an analogous file, or directly a dataframe like pasillaSampleAnno.

```{r}
annotationFile = system.file("extdata",
  "pasilla_sample_annotation.csv",
  package = "pasilla", mustWork = TRUE)
pasillaSampleAnno = readr::read_csv(annotationFile)
pasillaSampleAnno
```

* Overall dataset produced in 2 batches
  * 1st has 3 seqs libraries subjected to single end read sequencing
  * 2nd batch has 4 seq libraries subjected to paired end sequencing
* Need to data wrangle to replace hypens in type column w/ underscores
  * Arithmetic operators in factor levels (such as hypens (-)) are discouraged by DESeq2
    * Convert type & condition columns into factors
      * Explicitly specify perfered order of levels (default = alphabetical)
```{r}
pasillaSampleAnno = mutate(pasillaSampleAnno, condition = factor(condition, levels = c("untreated", "treated")), type = factor(sub("-.*", "", type), levels = c("single", "paired")))
```

* Note, design is approximately balanced between factor of interest (condition) & nuisance factor (type)

```{r}
with(pasillaSampleAnno, table(condition, type))
```

* DESeq2 uses data contianer (DESeqDataSet) to store datasets
  * Use of data containers (**or classes in R**) -> common for bioconductor packages (helps keep related data together)
    * Requries user to understand classes unlike base R matrix & dataframes
    * Helps avoid bugs due to loss of syncronization between related parts of data
    * Allows abstraction & encapsulation of common operations
  * Data container DESeqDataSet by DESeq2 is an extension of SummarizedExperiment class in bioconductor
  * <mark style='background-color:blue'> Ask class about these classes</mark>
    * SummarizedExperiment class used in many packages so learning it is multipurpose
    
    > Another advantage is that classes can contain validity methods, which make sure that the data always fulfill certain expectations, for instance, that the counts are positive integers, or that the columns of the counts matrix align with the rows of the sample annotation dataframe.
    
* Use constructor function `DESeqDataSetFromMatrix()` -> creates a DESeqDataSet class from count data matrix counts & sample annotation dataframe pasillaSampleAnno (metadata)

>Note how in the code below, we have to put in extra work to match the column names of the counts object with the file column of the pasillaSampleAnno dataframe, in particular, we need to remove the "fb" that happens to be used in the file column for some reason. Such data wrangling is very common. One of the reasons for storing the data in a DESeqDataSet object is that we then no longer have to worry about such things.

```{r}
mt = match(colnames(counts), sub("fb$", "", pasillaSampleAnno$file))
stopifnot(!any(is.na(mt)))

pasilla = DESeqDataSetFromMatrix(
  countData = counts,
  colData   = pasillaSampleAnno[mt, ],
  design    = ~ condition)
class(pasilla)

is(pasilla, "SummarizedExperiment")
```

* SummarizedExperiment class = DESeqDataSet
  * Has storage for annotation of rows of count matrix
  
* Q) How can we access the row metadata of SummarizedExperiment object
  * How can we read it/change it?
* A) Check manual page of SummarizedExperiment class & methods rowData + rowData <-

```{r}
?`SummarizedExperiment-class`
```

#### DESeq2 method

* Jump into differential expression analysis
  * Aim -> ID genes that are differnetially abundant between treated/untreated cells
  * Apply test that is similar to t-test (in section6) but more mathematically involved (more in multi-factor designs & linear models of this CH)
    * STandard analysis steps wrapped into single function `DESeq()`

```{r}
pasilla <- DESeq(pasilla)
```

* `DESeq()` is a wrapper that calls:
  * `estimateSizeFactors()` -> normalization as previously discussed
  * `estimateDispersions()` -> dispersion estimation
  * `nbinomWalsTest()` -> hypothesis test for differential abundance
* Test is between 2levels textttuntreated & textttreated of factor condition
  * condition is what was specified when constructing pasilla object via the argument design=\simcondition
    * can call each of these functions individually if wanted to modify behaviour/custom steps
    
```{r}
# Look at results
res <- results(pasilla)
res[order(res$padj),] %>% head #head 6 lines of results
```

#### **Exploring the results**
* 1st step after differential expression analysis = visualize following 3-4 basic plots 
  * Histogram of p-values
  * MA plot
  * Oridination plot
  * Heatmap can be instructive

* <mark style='background-color:lightgreen'>These are essential data quality assesment measures</mark>
  * Advice on quality assessment & control in CH13
  
```{r}
#Histogram of p-values
ggplot(as(res, "data.frame"), aes(x = pvalue)) +
  geom_histogram(binwidth = 0.01, fill = "Royalblue", boundary = 0)
```
> Histogram of p-values of a differential expression analysis.

* This histogram distribution displays 2 main components
  * Uniform background w/ values between 0-1
    * Corresponds to non-differentially expressed genes 
    * Usually the majority of genes
  * Peak of small p-values (left)
    * Left hand peak = differnetially expressed genes (few isolated peaks in the middle towards right = genes w/ small counts & reflect discreteness of data)
* In CH6 saw ratio of level of background to height of peak = rough indication of false discovery rate associated w/ calling genes in left-bin differentially expressed
  * In this case, left-bin has all p-vlaues between 0-0.1 = 993 genes
    * Background level is ~100 so false discovery rate associated w/ calling all genes in left-bin = ~10%
    * <mark style='background-color:blue'>Note didn't go over ch6 but look into it</mark>
    
* **If background distribution is not uniform & has tilted shape w/ increase at the right -> indicates batch effects (underlying systematic variation making replicates look more different than expected)**

* Q) If histogram for data is indicative of batch effects, what can you do?
  * <mark style='background-color:blue'>Come back to this</mark>
  
* Produce MA plot using `plotMA()` in DESeq2 package

```{r}
plotMA(pasilla, ylim=c(-2,2))
```
> fold change versus mean of size-factor normalized counts. Logarithmic scaling is used for both axes. By default, points are colored red if the adjusted p-value is less than 0.1. Points which fall out of the 
y-axis range are plotted as triangles.

* Produce PCA plot similar to CH7 - use DESeq2 function `plotPCA()`
```{r}
pas_rlog <- rlogTransformation(pasilla)
plotPCA(pas_rlog, intgroup=c("condition", "type")) + coord_fixed()
```
> PCA plot. The 7 samples are shown in the 2D plane spanned by their first two principal components.

* This plot is useful for visualizing overall effect of experimental covariates and/or detecting batch effects
  * PC1 mostly aligned w/ experimental covariate of interest (untreated/treated)
  * PC2 roughly aligned w/ seq protocol (single/paired)
* Used data transfromation (regularized log AKA rlog) - talked about in further statisitcal concepts section of this CH

* Q) Do axes of PCA plots always have to align w/ specific experimental covariates? 
  * In terms of single or paired-end data
    * Treated should always align w/ treated paired/single
    * Untreated should always align w/ untreated paired/single
    * This is because paired end sequencing should contain the single end sequence
  * In terms of paired end treated vs untreated
    * No these should not align b/c different sequences expected
  
* Heatmaps can quickly show an overview of a matrix-like dataset (count tables)
  * Below -> heatmap from rlog transformed data
  * A matrix as large as counts(pasilla) -> not practical to plot all so plot subset of most variable genes
  
```{r}
select <-  order(rowMeans(assay(pas_rlog)), decreasing = TRUE)[1:30]
pheatmap( assay(pas_rlog)[select, ],
     scale = "row",
     annotation_col = as.data.frame(
        colData(pas_rlog)[, c("condition", "type")] ))
```
> Heatmap of regularized log transformed data of the top 30 genes.

* Default of `pheatmap()` -> arranges rows & columns of matrix by dendogram (unsupervised culstering)
  * In above figure - clustering of of the columns (samples) dominated by type factor
    * Shows differential expression analysis above was too naive -> should adjust for strong "nuisance" factor for testing differentially expressed genes between conditions (shown in Two-factor analysis of pasilla data section of this CH)

#### Export the results
* HTML report of results + plots & sortable/filterable columns can be exported via ReportingTools package on DESeqDataSet class processed via `DESeq()`
  * ex) see RNA-seq differential expression vignette of ReportingTools package or manual page for the publish method of DESeqDataSet class
  
* CSV flile of results can be exported via `write.cvs()` (or counterpart from readr package)

```{r}
# write.csv(as.data.frame(res), file = "treated_vs_untreated.csv")
```






* **
<p id="7"><b><font size="5">Critique of default choices and possible modifications</b></font><a href="#0"><sup>Return</sup></a></p>

#### Few changes assumption
* Assumption by default normalization & dispersion estimate by DESeq2 (& other differential expression methods) = most genes not differnetially expressed
  * Assumption often reasonable (in well-designed experiments ask specific question so not everything changes at once)
    * What to do if assumption is not correct?
    * Solution: Don't apply operations on data w/ all genes
      * ID subset of -ve control genes which we belive the assumption holds
        * b/c prior knowledge or controlled abundance as external "spike in" (PhiX)
        
> For the normalization, although not for the dispersion estimation, one can slightly relax this assumption: it is still valid if many genes are changing, but in a way that is balanced between up- and downward directions

* Task: Run DESeq2 workflow w/ size factors & dispersion parameters estimated only from a predefined subset of genes
  * <mark style='background-color:red'>skipped</mark>
  
#### Point-like null hypothesis
* Default of `DESeq()` tests vs null hypothesis (each gene has same abundance across conditions)
  * If sample size limited, the statistical significance is strong enough
  * If sample size increases, the statistical significance in the tests present w/o much biological relevance
    * ex) many genes may be downregulated by downstream indirect effects
      * can modify test to use a more permissive, interval-based null hypothesis (seen in further statistical concepts section of this chapter)

* **
<p id="8"><b><font size="5">Multi-factor design and linear models</b></font><a href="#0"><sup>Return</sup></a></p>

#### What is multifactorial design?
* In addition to siRNA knockdown of pasilla gene, want to ttest effect of a drug.
  * Can use experiment to treat -ve control cells and/or cells w/ siRNA against pasilla gene using drug
  * Use the notaion 
    * where y = experimental measure of interest (transformed expression level of gene in this example)
    * Coefficient β0 = base lvel of measurement in -ve control (also called the intercept)
    * Design factors x1 & x2 = binary indicator variables
      * x1 takes value=1 if siRNA was transfected (0 if not)
      * x2 takes value=1 if durg was administered (0 if not)
      * In this exp only siRNA used, no drugs (x1=1 & x2=0)
        * 3rd and 4th term of this eqn simplified to 0
          * so final eqn: y=β0+β1
            * β1 will represent the difference between treatment + control
    
  \begin{equation}
y = \beta_0 + x_1 \beta_1 + x_2 \beta_2 + x_1x_2\beta_{12}.
\tag{8.1}
\end{equation}

> sometimes written w/ additional term x0 multiplied w/ β0 & x0=1 always.  
This makes subsequent notation easier to keep track of b/c intercept easier to handle w/ β1 & β2


* RNA-seq deals with a lot of genes so have to use this eqn multiple times (once per gene)

* If measurements are on log scale then following eqn is on log fold change to treatment w/ siRNA
  * β2 is log fold change due to treatment w/ drug (x2) (as with previous but w/ log)

\begin{align}
\beta_1 = y-\beta_0 &=
\log_2(\text{expression}_{\text{treated}}) -  \log_2(\text{expression}_{\text{untreated}})\nonumber\\
&=\log_2\frac{\text{expression}_{\text{treated}}}{\text{expression}_{\text{untreated}}}
\tag{8.2} 
\end{align}

* What if treating cells w/ both siRNA+drug
  * Then x1 = x2 = 1 and the eqn can be rewritten as

\begin{equation}
\beta_{12} = y - (\beta_0 + \beta_1 + \beta_2).
\tag{8.3}
\end{equation}

* β12 is interpreted as "difference between observed outcome (y) & expected outcome from individual treatments (via adding to baseline the effect of siRNA alone (β1) + drug alone (β2)"
  * β12 = interaction effect of siRNA and drug
    * not associated w/ physical interaction (ex) effects of two different experimental factors don't add up but may combine synergistically)
    
* ex) Target drug of siRNA equivalent -> same side effect on cells then biologically expect
  * β1=β2
  * Also expect that combination of the drug + siRNA lead to no further effects (additive), then β12= -β1
  
* ex) Target drug of siRNA are parallel pathways (ie) buffer each other)
  * Expect β1 & β2 to be small
  * Also expect that the combination of drug + siRNA lead to synergistic effect, and β12 is large

> Note addition is on log scale = multiplication on original scale

* Don't always care about interactions
  * Many exps designed w/ multi-factors & care about individual effects
  * In this case -> combinatorial treatment may not be present in experimental design
    * Model to use for analysis = eqn 8.1 above but remove end right term y=b0+b1 or y=b0+b2
  * Can encode the exp design as a **design matrix**
    * ex) for combinatorial exp. described above, this is the design matrix:
    
|x0|x1|x2|
|---|---|---|
|1|0|0|
|1|1|0|
|1|0|1|
|1|1|1|

* **Columns of the design matrix = experimental factors**
* **Rows of the design matrix = different experimental conditions**
  * 4 in this case

* If combinatorial treatment is not performed, then design matrix reduced to 3 rows of the above matrix (ex) remove the last row where x1=1 & x2=1)

#### What about noise & replicates?
* Eqn 8.1 above = conceptual decomposition of observed data -> effects caused by different exp. variables
* If data (y's) were precise, then can use a linear system of eqns
  * one eqn for each of the 4 exp. conditions (x's /rows) -> can solve for β's
* Usually want to analyze real data affected w/ noise
  * **Need replicates to estimate lvls of noise + assess uncertainty of estimated β's**
    * Then can empirically assess if observed changes between conditions are significantly larger vs those due to exp/natural variation.
    * Use the following eqn:
    
\begin{equation}
y_{j} = x_{j0} \; \beta_0 + x_{j1} \; \beta_1 + x_{j2} \; \beta_2 + x_{j1}\,x_{j2}\;\beta_{12} + \varepsilon_j.
\tag{8.5}
\end{equation}

* Added new term (Ej) & index (j) (Xj1, etc) to eqn 8.1
  * index j = explicitly counts over individual replicates in exp.
    * ex) if replicate 3 times for each of the 4 conditions -> j counts from 1 to 12 (AKA now design matrix has 12 rows & x<sub>jk</sub> is value of the matrix in _j_th row & _k_th column)
  * The new term (Ej) = **residuals**
    * absorbs differences between replicates
    * Needs one additional modeling component
      * the 12 modeling component in eqn 8.5 needs further info 
        * Has more variables (12 epsilons & 4 betas) vs eqns (12, one for each j)
        * Solution: **Ej needs to be small**
          * One method = minimize sum of squared residuals

\begin{equation}
\sum_j \varepsilon_j^2 \quad\to\quad\text{min}.
\tag{8.6}
\end{equation}

> Since β0 = the intercept, then x<sub>j0</sub>=1 for all j

* This requirement is satisfied b/c 
  * β's represent average effects of each experimental factors
  * Residuals Ej reflect experimental fluctuations around mean between replicates
  * This approach =**least sum of squares fitting**
    * straightforward matrix algebra via `lm()`

* Q) Alternative to writting eqn 8.5 is the following:
  * How can this be mapped to eqn 8.5 (what's with the interation term xj1 xj2 β12)?
* A) Notation -> sum extends over k=0,...,3
  * Terms for k=0,1,2 are known
  * Write β3 instead of β12 & xj3 = (xj1)(xj2)
  * eqn 8.7 is better to use in computer softwar for linear models + math proofs
    * Highlights scientific content of linear model (via design matrix)

\begin{equation}
y_{j} = \sum_k x_{jk} \; \beta_k + \varepsilon_j.
\tag{8.7}
\end{equation}

* Task: Show if we fit eqn 8.5 to data, so that 8.6 holds, the fit of residuals (E(hat)j) average = 0
<mark style='background-color:red'>Skipped for now</mark>

#### Analysis of variance
* Model via eqn 8.5 = **linear model**
  * Implies that the criteria of eqn 8.6 -> used to fit data
  * Advantage vs taking averages over replicates (for each distinct exp condition) & comparing these means across conditions
    * Taking averages over replicates per condition -> replicate #s not all the same in different groups (or >=1 x-variables is continous)
      * May end up fitting eqn 8.5 to data
* Think about eqn 8.5 this way
  * **Analysis of variance (ANOVA)**
    * eqn 8.5 decomposes the variabillity of (y) observed during experiments into elementary components
      * β0 = Baseline value -> variability caused via effect of 1st variable (β1)
        * β1 variability caused by effect of 2nd variable β2
        * β2 variability caused by effect of interation β12
        * β12 variability unaccounted for
          * unaccounted variability = noise/systematic variability

#### Rombustness
* **Sum of eqn 8.6 is sensitive to data outliers (because eqn 8.5 is linear system of eqn?)**
  * One measurement (yj) w/ outlier value changes β estimate to be far from values measured by replicates
    * **Methods used on least sum of squares have low breakdown pt**
      * If one single data point is an outlier -> whole result strongly affected
        * ex) Average of set of numbers (n) has breakdown pt of (1/n) -> average can be changed by changing a single number (n)
      * If median is higher vs breakdown pt -> changing single# has no effect
        * if changing single# has effect -> effect limited to range of data points in the middle of the ranking (those adjacent to rank n/2)
        * To change median by high amount -> need to change half of the obs
          * **Median is rombust -> breakdown point is 1/2**
          * median of set of #'s y1,y2,... minimizes the sum
          
          \begin{equation}\sum_j|y_j-\beta_0|\end{equation}

* To get a high degree of rombustness vs outliers, there choices other than sum of squares(eqn 8.6) used for minimization

\begin{align}
R &= \sum_j |\varepsilon_j| & \text{Least absolute deviations} \tag{8.8} \\
R &= \sum_j \rho_s(\varepsilon_j)  & \text{M-estimation}  \tag{8.9} \\
R &= Q_{\theta}\left( \{\varepsilon_1^2, \varepsilon_2^2,... \} \right)
& \text{LTS, LQS}  \tag{8.10}  \\
R &= \sum_j w_j \varepsilon_j^2 & \text{general weighted regression} \tag{8.11} 
\end{align}

* R is the quantity to be minimized
  * eqn 8.8 = least absolute deviations regression - > generalization of median
    * Harder to minimize vs sum of squares eqn (8.6)
    * Less stable & efficient if data is limited/don't fit model
  * eqn 8.9
    * Uses penalization function p8 (least squares regression when ps(E)=E^2 special case)
      * Looks like quadratic function for limited range of e
        * Has a smaller slope + flattens/drops back to 0 for abs values |E| > scale parameter (s)
        * Used to downweight effect of outliers (data pts w/ large residuals)
        * Need to make choice for s -> determines what to call an outlier
        * Can drop requirement that ps is quadratic around 0
          * Second derivative must be positive for this to occur
    * Variety of choices for function ps found in literature
      * Aims to get best estimator for statistical tests (ex) bias & efficiency) when data fits model
        * Also limits/prevents influence of data points not estimating for statistics
        
* Q) Plot graph of function ps(E) for M-estimators
* A)

\begin{equation*}
\rho_s(\varepsilon) = \left\{
  \begin{array}{cc}
\frac{1}{2}\varepsilon^2, \quad\text{for }|\varepsilon|< s\\
s|\varepsilon|-\frac{1}{2}s^2, \quad\text{for }|\varepsilon|\ge s\\
  \end{array}
\right.
\end{equation*}

```{r}
# which is this code
rho = function(x, s)
  ifelse(abs(x) < s, x^2 / 2,  s * abs(x) - s^2 / 2)

df = tibble(
  x        = seq(-7, 7, length.out = 100),
  parabola = x ^ 2 / 2,
  Huber    = rho(x, s = 2))

ggplot(reshape2::melt(df, id.vars = "x"),
  aes(x = x, y = value, col = variable)) + geom_line()
```
> Graph of ρs(ε), for a choice of s=2.

* Eqn 8.10 generalizes eqn 8.6 (least sum of squares method)
  * Uses Least quantile of squares regression (LQS) -> sum over squared individuals replaced w/ quantile
    * ex) Q50 = median, Q90 = 90% quantile
  * The variation Least trimmed sum of squares regression (LTS) -> sum of squared residuals used, but sum doesn't apply for all residuals (only for smallest residuals in the fraction 0<=theta<=1)
  * Both cases -> outlier data points causing large residuals & if rare -> don't affect quantile/trimmed sum
  * Problem:
    * Least sum of squares by eqn 8.6 uses straightforward linear algebra
      * M-estimation (eqn 8.9) & LQS+LTS (eqn 8.10)
      
* Eqn 8.11 is a complex way of weighing down outliers
  * Assumes there is a way to decide the weight (wj) for each observation (which should weigh down outliers)
    * In "Further statistical concepts" of this CH -> DEseq package approach
      * Leverage of each data point on estimated β's assessed via Cook's distance
        * For data points w/ large Cook's distance -> weight (wj) = 0
        * All other data points -> weight (wj) = 1
      * All outlier data points discarded (wj=0) & ordinary regression performed on points where wj=1
  * This uses straightforward linear algebra
  
* All these approaches to outlier rombustness add a subjective component & **rely on sufficient replication**
  * Subjectivness reflected via parameter choices
    * s in eqn 8.9
    * theta in eqn 8.10
    * weights in eqn 8.10
    
* Task: Search for packages implementing the above eqns for rombust regression methods <mark style='background-color:red'>skipped</mark>

* **
<p id="9"><b><font size="5">Generalized linear models</b></font><a href="#0"><sup>Return</sup></a></p>

* Two more theoretical concepts
  * Eqn 8.5 models the expected value of the outcome variable(y) into a linear function of a design matrix -> fit data via least sum of squares criteria (eqn 8.6)
    * These assumptions generalized below

#### Model data on a transformed scale
* Can consider the data not to be on the scale obtained, but on a scale after transformation (ex) log transformation)
  * ex) linear model in eqn 8.5 may not be useful for modeling outcomes bound within an interval for indicating disease risk (ie) 0 to 1)
* For linear models, values of y should cover the whole real axis
  * If transforming the expression on the right w/ a sigmoid function (ex) f(y) = 1/(1+e<sup>-y</sub>) -> the range of the function is bound between 0 & 1 and more useful for the disease risk model above
    * This function is the logistic function associated w/ logistic regression model 

#### Other error distributions
* Generalizing the maximization criterion (eqn 8.6)
  * This criteria can be derived via specific probabilistic model (Maximium likelihood principle - seen i nCH2)
  * Consider the follwing probability model
    
\begin{equation}
p(\varepsilon_j) = \frac{1}{\sqrt{2\pi}\sigma} \exp \frac{\varepsilon_j^2}{2\sigma^2},
\tag{8.12}
\end{equation}

* This model assumes residuals follow a normal distribution (mean 0 & stdev σ)
  * Can demand from a good model (good set of β's) -> these probabilities are large 
  
\begin{equation}
\prod_j p(\varepsilon_j) \quad\to\quad\text{max}.
\tag{8.13}
\end{equation}

* Q) Show that the maximizing the likelihood (criteria 8.13) is = to minimizing the sum of squared residuals (criteria 8.6)
* A) Insert ((8.12)) into ((8.13)) and take the logarithm.

* Core concepts 
  * Left hand side of eqn 8.13 (product of probabilities of residuals) = function of the model parameters β1,β2,... & data y1, y2,... -> this is f(β,y)
    * Think of the model parameters β as fixed
      * Then the collapsed function f(y) indicates the probability fo the data
    * Think of the data given
      * Then f(β) is a function of model parameters - indicates the likelihood
      * We use this to optimize criteria 8.6 (which changes criteria 8.13 also)
        * β's obtained are called maximul-likelihood estimates 
  * Can make a probabilistic model w/ this generalization
    * Use densities from other distributions (except normal eqn 8.12)
      * ex) count data - use a gamma-Poisson distribution
      
> It is good to remember that, while we can use the normal distribution as a convenient argument to motivate least sum of squares regression through the maximum likelihood principle, the data do not have to be distributed according to the normal for least sum of squares regression to provide a useful result. In fact, least sum of squares fitting often provides useful estimates for the 
βs even when the data are non-normal, although that depends on the specific circumstances.

#### Generalized linear model for count data
* Differential expression analysis in DESeq2 uses a generalize linear model

\begin{align}
K_{ij} &\sim \text{GP}(\mu_{ij}, \alpha_i)  \tag{8.14} \\
\mu_{ij} &= s_j\, q_{ij}  \tag{8.15} \\
\log_2(q_{ij}) &= \sum_k x_{jk} \beta_{ik}.  \tag{8.16} 
\end{align}

* Step-by-step
  * Counts (Kij) for gene i & sample j -> modeled via gamma-Poisson (GP) distribution w/ 2 parameters
    * Mean (uij) & disperson (ai)
  * Default dispersion is different for each gene i BUT same across all samples
    * Therefore, no index j
  * Eqn 8.15 states that mean made from sample-specific size factor (sj & qij)
    * This is proportional to the ture expected concentration of fragments for gene i & sample j
  * Value of qij -> given by linear model in 3rd line (link function log2)
  * Design matrix (xjk) same for all genes (therefore, does not depend on i)
    * rows j = smaples
    * columns k = experimental factors
  * For a pairwise comparison, the design matrix has only 2 columns
    * one col filled w/ 1's (=β0 in eqn 8.1)
    * other col contains 1 or 0 (depends on sample group)
    * Coefficients βuk give log2 fold changes for gene i for each col of the design matrix (X)

>The model can be generalized to use sample- and gene-dependent normalization factors (sij) . This is explained in the documentation of the DESeq2 package.


* **
<p id="10"><b><font size="5">Two-factor analysis of the pasilla data</b></font><a href="#0"><sup>Return</sup></a></p>

* Other than treating w/ siRNA (A basic analysis in this CH8)
  * Pasilla data has another covariate (type) -> indicates type of sequencing performed
* In exploratory data analysis plots (A basic analysis in this CH8), type of sequencing has high systematic effect on data
  * The previous plots (histogram, MA plots , ordination, & heatmap) did not account for type of sequencing
  * Accounting for type of sequencing should give a better picture of which differences in data is attributed to the treatment (siRNA)/or confounded or masked by sequencing type
  
```{r}
pasillaTwoFactor = pasilla
design(pasillaTwoFactor) = formula(~ type + condition)
pasillaTwoFactor = DESeq(pasillaTwoFactor)
```

* The two variables _type_ & _condition_
  * More interested in condition
    * DESeq2 puts this at the end of the formula
      * Doesn't affect model fitting
      * Helps simplify some of the results
* Acess the results using `results()` -> returns a dataframe w/ statistics of each gene

```{r}
res2 = results(pasillaTwoFactor)
head(res2, n = 3)
```

* Can also get log2 fold changes, p-values, and adjusted p-values associated w/ the type variable
  * `results()` takes an argument _contrast_ and lets user pick the name of variable for
    * lvl corresponding to numerator of fold change
    * lvl corresponding to denominator of fold change
    
```{r}
resType = results(pasillaTwoFactor,
  contrast = c("type", "single", "paired"))
head(resType, n = 3)
```

* This analysis takes in the variable _type_ as **nuisance factor (AKA blocking factor)**
  * in "A basic analysis section" -> compared 2 groups
  * What is the diifference between analyses?
    * Plot p-values from both analyses against each other
    
```{r}
trsf = function(x) ifelse(is.na(x), 0, (-log10(x)) ^ (1/6))
ggplot(tibble(pOne = res$pvalue,
              pTwo = res2$pvalue),
    aes(x = trsf(pOne), y = trsf(pTwo))) +
    geom_hex(bins = 75) + coord_fixed() +
    xlab("Single factor analysis (condition)") +
    ylab("Two factor analysis (type + condition)") +
    geom_abline(col = "orange")
```
> Comparison of p-values from the models with a single factor (condition) and with two factors (type + condition). The axes correspond to (−log10p)<sup>1/6</sup>, an arbitrarily chosen monotonically decreasing transformation that compresses the dynamic range of the p-values for the purpose of visualization. We can see a trend for the joint distribution to lie above the bisector, indicating that the small p-values in the two-factor analysis are generally smaller than those in the one-factor analysis.

* The above figure shows p-values in 2-factor analysis similar to 1-factor analysis
  * 2-facotr analysis generally smaller
    * leads to a small increase in power (ex) count #genes passing a certain significance threshold in each case)
    
```{r}
compareRes = table(
   `simple analysis` = res$padj < 0.1, #0.1 is FDR treshold of 10%
   `two factor` = res2$padj < 0.1 )
addmargins( compareRes )
```

* 2-factor analysis has found 1325 differentially expressed gene w/ FDR treshold of 10%
  * 1 factor analysis found 1061 genes differentially expressed
  * Therefore, the 2-factor analysis has increased detection power
    * gain can be larger/smaller depending on data
* Proper choice of model depends on experimental design & data quality

* Q) Why do we detect fewer significant genes when we don't account for type variables?
  * Hows does accounting for/not blocking factors benefit?
* A)

> Without modeling the blocking factor, the variability in the data that is due to it has to be absorbed by the ε's (residual). This means that they are generally larger than in the model with the blocking factor (ε's larger in data w/o including type). The higher level of noise leads to higher uncertainty in the β-estimates. On the other hand, the model with the blocking factor has more parameters that need to be estimated. In statistical parlance, the fit has fewer “degrees of freedom”. Both of these effects are counteracting, and which of them prevails, and which of the modeling choices yields more or fewer significant results depends on the data.

* Q) What is confounding?
  * If don't account for blocking factor, does this lead to detection of more genes?
* A)

> Yes. Imagine the variables condition and type were not as nicely balanced as they are, but partially or fully confounded. In that case, differences in the data due to type could be attributed to condition if a model is fit that does not make it possible to absorb them in the type-effect. Scientifically, such an experiment (and analysis) can be quite an embarrassment.

* Q) If a paired experiment (10 different cell lines w/ and w/o drug treatment)
  * How should this be analyzed?
  
> If we just did a simple two-group comparison (treated versus untreated) many of the treatment effects would probably go under in the strong **cell line to cell line variation**. However, we can set up a paired analysis simply by **adding cell line identity as a blocking factor**. (Cell line is then really an R factor with 10 different levels, rather than just a 0 vs 1 indicator variable as with the variables that we looked at so far; R’s linear modeling facilities, and also DESeq2, have no problem dealing with that.)

* Q) What can you do if you suspect "hidden" factors which affect data but not documented? (ex) undocumented covariates such as **batch effects**)
* A) There are methods attempting to identify blocking factors wia unsupervised learning (read Leek and Storey (2007; Stegle et al. 2010))

* **
<p id="11"><b><font size="5">Further statistical concepts</b></font><a href="#0"><sup>Return</sup></a></p>

#### Sharing of dispersion information across genes
* Seen in explaination of Bayesian (or emperical Bayes) analysis in CH6
  * use aditional info to improve estimates (prior information, a priori, or from analysis of other similar data)
  * This idea is useful if the data if noisy
* DESeq uses emperical Bayes approach for estimation of dispersion parameters (a's in eqn 8.16)
  * can also estimate log fold changes (B's)

* Priors in this case is from distributions of maximum-likelihood estimates (MLEs) across all gnees
  * Both of these distributions (a's and B's) are unimodal
    * B's -> peak around 0
    * a's -> peak around particular value (typical dispersion)
* Emperical Bayes algorithm shrinks each per-gene MLE to the peak (amount depends on sharpness of empirical prior distribution & percision of ML estimate)
  * Percision of ML estimate is better -> less shrinkage
  * Below figure visualizes the approach for B's
  
* Task
  * Advanced: check source code producing below figure
```{r}

```

  <mark style='background-color:red'>skipped for now</mark>
  
#### Count data transformations 
* Testing differential exprssion on raw counts -> use discrete distirbutions
* For downstream analyses (visualization/clustering) -> may want to use transfromed count data
  * Log transformations good but count values for a gene can become 0
    * May want to use pseudocounts (transformation by below formula)
      * n = count values
      * n0 = shomehow chosen +ve constant

\begin{equation}
y = \log_2(n + 1)\quad\mbox{or more generally,}\quad y = \log_2(n + n_0),
\tag{8.17}
\end{equation}

* Looking at 2 alternative approaches w/ theoretical justification + rational way of choosing parameter n0
  * Method 1 = incorporates priors to sample differences
  * Method 2 = variance-stabilizing transformations
  
* Variance-stabilizing transformation
  * Can compute piece-wise linear transformation (CH4) for discrete set of random variables 
  * Can use calculus to derive smooth variance-stabilizing transformation for gamma-Poisson mixture
  * Both computations implemented in DESeq2 package
  
```{r}
vsp = varianceStabilizingTransformation(pasilla)
```

* Using first sample as an example, let's see the effect on the data
  * Compare w/ log2 transformation
```{r}
j = 1
ggplot(tibble(
         x    = assay(pasilla)[, j],
         VST  = assay(vsp)[, j],
         log2 = log2(assay(pasilla)[, j])) %>%
             reshape2::melt(id.vars = "x"),
       aes(x = x, y = value, col = variable)) +
  geom_line() + xlim(c(0, 600)) + ylim(c(0, 9)) +
  xlab("counts") + ylab("transformed")
```
> Graph of variance-stabilizing transformation for the data of one of the samples, and for comparison also of the log2 transformation. The variance-stabilizing transformation has finite values and finite slope even for counts close to zero, whereas the slope of log2 becomes very steep for small counts and is undefined for counts of zero. For large counts, the two transformation are essentially the same.

* Regularized logarithm (rlog) transformation
  * 2nd way to transform data 
  * Distinct from variance stabilizaiton
    * Uses shrinkage estimation previously talked about
  * Transform original count data -> log2-like scale via fitting "trivial" model w/ separate term for each sample
    * Prior distribution on coefficients is estimated from data
  * Fitting uses regularaziation as previously discussed
  * Transformed data (qij) = defined in eqn 8.16
    * Design matrix (xij) is of size (K)(K+1)
      * K= #samples w/ the form 

\begin{equation}
X=\left(\begin{array}{ccccc}1&1&0&0&\cdot\\1&0&1&0&\cdot\\1&0&0&1&\cdot\\\cdot&\cdot&\cdot&\cdot&\cdot\end{array}\right).
\tag{8.18}
\end{equation}

* w/o priors -> design matrix leads to non-unique solution
  * w/ prior on non-intercept B's = unique solution
* DESeq implements this in the `rlogTransformation()` function
  * rlog transformation is approximately variance-stabilizing
    * better w/ data where size factor for each sample is distincy
    
* Q) Plot mean against standard deviation between replicates for shifted log (eqn 8.17) - this is the regularized log transformation & variance-stabilizing transformation
* A) 

```{r}
rlp = rlogTransformation(pasilla)

msd = function(x)
  meanSdPlot(x, plot = FALSE)$gg + ylim(c(0, 1)) +
     theme(legend.position = "none")

gridExtra::grid.arrange(
  msd(log2(counts(pasilla, normalized = TRUE) + 1)) +
    ylab("sd(log2)"),
  msd(assay(vsp)) + ylab("sd(vst)"),
  msd(assay(rlp)) + ylab("sd(rlog)"),
  ncol = 3
)
```
> Per-gene standard deviation (sd, taken across samples) against the rank of the mean, for the shifted logarithm log2(n+1), the variance-stabilizing transformation (vst) and the rlog. Note that for the leftmost ≈ 2,500 genes, the counts are all zero, and hence their standard deviation is zero. The mean-sd dependence becomes more interesting for genes with non-zero counts. Note also the high value of the standard deviation for genes that are weakly detected (but not with all zero counts) when the shifted logarithm is used, and compare to the relatively flat shape of the mean-sd relationship for the variance-stabilizing transformation. 

#### Dealing w/ outliers
* Data sometimes has isolated instances of large counts which is unrelated to experiment (sometimes outliers)
* Many reasons for outliers (technical/experimental artifacts/read mapping problems for genetically differing samples/rare biolocial events)
* People usually interested in genes w/ consistent behaviour
  * By defualt, genes affected by outliers are set to the side by DESeq
* Function calculates a diagnostic test for outliers for every gene & every sample via **Cook's distance**
  * Measures of how much a single sample is influencing fitted coefficients for a gene
  * Large Cook's distances = indicates outlier count
    * DESeq2 flags genes above Cook's distance cutoff -> sets p-values & adjusted p-values to NA
    * Default cutoff depends on sample size & #parameters to be estimated 
      * DESeq2 uses 99% quantile of F(p,m-p) distribution (p=#parameters including intercept & m=#samples)
      
* Q) Check documentation to see how default cutoff can be changed & how outlier removal function can be distabled
  * How can computed Cook's distances be accessed?

* w/ many degrees of freedom (more samples than #of parameters to be estimated), may not want to remove entire genes from analysis b/c their data have a single count outlier
  * Alternative strategy is to replace #outlier counts w/ trimemd mean over all samples & adjust by size factor for the sample
    * This is a conservative approach -> won't lead to false +ves (replaces outlier values w/ value predicted by null hypothesis)
    
#### Tests of log2 fold change above/below threshold
* How add requirement to detect effects w/ strong size (but small) vs statistically significant ones?
* 2 arguements to `results()` allow for threshold-based Wald tests
  * lfcTreshold -> takes numeric non-ve threshold value
  * altHypothesis -> specifies test
* Can take 1 of the four following values (B=log2 fold change specifiec by name arguement, theta=represents lfcTreshold)
  * greater: B>theta
  * less: B<(-theta)
  * greaterAbs: |B| > theta (2-tailed test)
  * lessAbs: |B| < theta (p-values are max of upper/lower tests)
* Demonstrated in the following code -> visuallized by MA-plots
  * `plotMA()` in DESeq2 uses base graphics
```{r}
par(mfrow = c(4, 1), mar = c(2, 2, 1, 1))
myMA = function(h, v, theta = 0.5) {
  plotMA(pasilla, lfcThreshold = theta, altHypothesis = h,
         ylim = c(-2.5, 2.5))
  abline(h = v * theta, col = "dodgerblue", lwd = 2)
}
myMA("greaterAbs", c(-1, 1))
myMA("lessAbs",    c(-1, 1))
myMA("greater",          1)
myMA("less",         -1   )
```
>MA-plots of tests of log 2 fold change with respect to a threshold value. From top to bottom, the tests are for , , , and.

* Producing results table instead of MS plots, use the same arguments in `plotMA()` above but exclude ylim to the `results()` function

```{r}
results(pasilla, lfcThreshold = 0.5, altHypothesis = "greaterAbs")
```


* **
<p id="12"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>

# https://www.bioconductor.org/help/course-materials/2016/CSAMA/lab-3-rnaseq/rnaseq_gene_CSAMA2016.html#creating-a-dgelist-for-use-with-edger
# WORK IN PROGRESS - STILL FIGURING THINGS OUT

#### Excercise 8.1

```{r}
#make DGEList class
pasillaEdge <- DGEList(counts=counts, sample = pasillaSampleAnno[mt,], group=pasillaSampleAnno$condition)
class(pasillaEdge)
```

```{r}
is(pasillaEdge, "SummariedExperiment") #not a summarizedexperiment class of bioconductor
```

```{r}
# DESeq2() -> normalization (estimateSizeFactors), dispersion estimate (estimateDispersions), & nbiomWladTest (hypothesis test for differential abundance)

#Equivalent to DESeq2() function in EdgeR is model.matrix() for groups + fit()
design <- model.matrix(~pasillaEdge$samples$group)
pasillaEdge1 <- calcNormFactors(pasillaEdge) # estimate size factor
pasillaEdge1 <- estimateDisp(pasillaEdge, design) #estimate dispersion
fit <- glmFit(pasillaEdge1, design=design, dispersion = pasillaEdge$common.dispersion) #note dispersion is set at NULL default
lrt <- glmLRT(fit, coef=ncol(design))
```

```{r}
#Compare w/ DESeq2 output
pasilla$sizeFactor ; pasillaEdge1$samples$norm.factors #size factor est.
```

```{r}
#Compare edgeR results vs DESeq2 results
tt <- topTags(lrt, n=nrow(pasillaEdge1), p.value=0.1)
tt10 <- topTags(lrt) # just the top 10 by default
tt.all <- topTags(lrt, n=nrow(pasillaEdge1), sort.by="none")
table(DESeq2=results(pasilla)$padj < 0.1, edgeR=tt.all$table$FDR <0.1)
```


#### Excercise 8.2
* skipped b/c not sure how shiny works (this was mentioned in ch3 which we didn't go through)

* **

<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*

</body>
  