---
title: "Chapter 7: Multivariate Analysis"
output: html_notebook
---


```{r libraries, message=F, warning=F}
  library(here)
  library(phyloseq)
  library(SummarizedExperiment)
  library(airway) #bioconductor
  library(GGally)
  library(tidyverse)
  library(pheatmap)
  library(factoextra)
  library(ade4)
```

<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> Multivariate Analysis  
<a href="#2">[2]</a> Chapter Summary  
<a href="#3">[3]</a> What are the data? Matrices and their motivation  
<a href="#4">[4]</a> Dimension reduction  
<a href="#5">[5]</a> The new linear combinations   
<a href="#6">[6]</a> The PCA workflow  
<a href="#7">[7]</a> The inner workings of PCA: rank reduction  
<a href="#8">[8]</a> Plotting the observations in the principal plane  
<a href="#9">[9]</a> PCA as an exploratory tool: using extra information   
<a href="#10">[10]</a> Excercises  
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">Multivariate Analysis </b></font><a href="#0"><sup>Return</sup></a></p>

* Datasets may have multiple variables measured on same set of subjects (multidimension on one set of data)
  * ex) Patients, samples, and organisms
  * May have characteristic biometrics: height, weight, age, clinical variables (blood pressure, blood sugar, heart rate, genetic data) for n patients
* Multivariate analysis allows investigation into connections/associations between different variables measured
  * Data usually in table data structure
    * each row = one subject
    * each column = one variable
      * In this example, variables are numeric (special case) -> use matrix to represent
      * If columns of matrix are independent from each other (no relation)
        * Can look at each column separately & use "univariate" statistics on each column -> therefore the matrix would be pointless
        * Multivariate data = columns are assumed dependent
* In multivariate data, there are patterns which depend on multiple variables
  * ex) biology of cells - proliferation rate influences gene**(s)** expression which is a simultaneous effect.
    * Study expression of 25,000 genes (columns) or many samples (rows) of patient cells = can see many genes act dependently of another (or is correlated/anti-correlated)
    * If genes study independently via "univariate" approaches = miss a lot of info
    * Connections between genes requires looking at the data as a whole
      * rows = measurements made on same observational unit
      * 25,000 columns = genes
        * problem = 25,000 variations -> need to reduce data into smaller #dimensions w/o losing information
* In this chapter - multivariate data matracies seen in high-throughput experiments
  * Focus is on Principle Component Analysis (PCA) = dimension reduction method
    
* **

<p id="2"><b><font size="5">Chapter Summary</b></font><a href="#0"><sup>Return</sup></a></p>

* **Pre-processing matracies** 
  * Multivariate data analysis needs pre-processing
    * Find means, variances, and 1D histograms -> then rescale and recenter data
* **Project onto new variables**
  * Projection onto lower dimensions of high dimensional data (planes & 3D most used)
  * Low loss of information
  * PCA searches for new, informative variables which are linear combos of old ones
* **Matrix decomposition**
  * PCA needs to find decompositions of matrix (X)
    * SVD = decomposition method outputs lower rank approximations = to eigenanalysis of X<sup>t</sup>X
    * Squares of single values are equal to eigen values & variences of new variables
    * Plot these values to decide how much axes are needed to reproduce signal in data
  * 2 Eigenvalues that are close can give high scores or **PC scores which are highly unstable**
    * Need to look at screeplot of eigenvlaues & avoid separating axes of close eigenvalues
    * May need interactive 3D or 4D projections (R packages)
* **Biplot representations**
  * Space of observations is p-dimensional space (p-original variables provide coordinates)
    * Space of variables is in n-dimensions
  * Decomposition methods (singular values/eigenvalue & singular vectors/eigenvectors) -> provide new coordinates for these 2 spaces
    * Sometimes one is the dual of the other
  * Can plot projection of both observations & variables into same eigenvectors
    * Generates a biplot for interpreting PCA output
* **Projecting other group variables**
  * Can interpret PCA by redundant or contiguous data about observations


* **
<p id="3"><b><font size="5">What are the data? Matrices and their motivation</b></font><a href="#0"><sup>Return</sup></a></p>

* In this example, rectangular matracies represents a table w/ measurements
  * Subjects (rows) & features (columns)
  

* Tutles data
  * dataset used to understand principles. 3D biometric measurements on painted turtles (sex, length, width, height = 4D - compare 3 = 3D)
  
```{r}
turtles <- read.table(here("BookStuff", "data", "PaintedTurtles.txt"), header=T)
turtles[1:4,]
```
Last 3 columns = length measurements (mm), 1st col = factor variable sex

* Athletes data
  * Performances of 33 atheletes in 10 disciplines of decathlon: (m100, m400, m1500) times in seconds for 100m, 400m, and 1500m
  * m110 is time to finish 110 meters hurdles
  * pole is pole-vault height
  * highj and long are results for high & long jumps (in meters)
  * weight, disc, and javel are lengths in meters for throw weight, discus and javelin

```{r}
#Variables for first 3 atheletes
load(here("BookStuff", "data", "athletes.RData"))
unscaledAthletes <- athletes #for later use
athletes[1:3,]

```

* Cell types data
  * gene expression profiles of sorted T-cell populations from different subjects
    * Columns are subset of gene expression measurements 
    * Rows = 156 genes showing differential expression between cell types

```{r}
load(here("BookStuff", "data", "Msig3transp.RData"))
round(Msig3transp,2)[1:5, 1:6]
```

* Bacterial species abundances data
  * Matracies of counts used in microbial ecology studies
    * Columns = different species (OTUs) - identified by numerical tags
    * Rows = labeled via samples measured
      * Integer numbers = #times each OTU observed in samples
  * Note matrix values have many 0's == "sparse data"
      
```{r}
data("GlobalPatterns", package="phyloseq")
GPOTUs <- as.matrix(t(phyloseq::otu_table(GlobalPatterns)))
GPOTUs[1:4, 6:13]
```

* mRNA reads data
  * RNA-seq transcriptome data report the #seqs reads matching each gene (or sub-gene structures - exons) in several saples (More in CH8)
  
```{r}
data("airway", package="airway")
assay(airway)[1:3, 1:4]
```
In RNA-seq (airway data), should report the <u>genes in rows & samples in columns</u> - this is transposed vs the other matracies loaded (rows <-> cols)
  * Different conventions can lead to errors so pay atttention to data 
  
* Proteomic Profiles
  * Columns =  aligned mass spectroscopy peask/molecules identified through m/z-ratios
    * Entries in matrix = measured intensities
```{r}
metab <- t(as.matrix(read.csv(here("BookStuff", "data", "metabolites.csv"), row.names=1)))
metab[1:4, 1:4]
```

* When a peak is not detected for a m/z score in mass spec run = 0 recorded in metab data
  * Similarly, 0's in GPOTUs & airway data = no matching sequence reads
* Find the frequency of zeros in these data matricies
```{r}
colSums(assay(airway)==0)
rowSums(GPOTUs)
rowSums(metab)
```

* Q) What are the columns of these matracies usually called? 
  * The columns of these matricies are labelled w/ numbers
    * For airways data (RNA-seq) the columns indicate sample names b/c transposed
  * Coulumns should indicate features/variables of the dataset
* Q) In each of these examples, what are the rows of the matrix
  * The rows in these datasets should indicate sample ID (except RNA-seq airways which indicates genes b/c transposed)
* Q) What does a cell in a matrix represent
  * A cell represents the count/frequency something appeared for that sample (row) feature (column)
* Q) In atheletes data subset the 3rd variable (col) for the 5th athlete (row)
```{r}
athletes[5,3]
```


#### Low-dimensional data summaries & preparation
* If studying one variable (ex) 3rd column of turtles matrix - weights) = looking at 1D data (A vector of data)
  * Can visualize with "univariate plots" such as a histogram
  * If computing a 1 number summary (mean or median) = zero-dimension summary of the 1D data
  * This is an example of dimension reduction
* In CH3 we used 2D scatterplots
  * Saw if there are too many observations -> then group data into hexagonal bins
    * 2D histograms
  * When considering 2 variables (x & y) measured together on set of observation
    * The **correlation coefficient** measures how variables co-vary
      * This is a single #summary of 2D data - formula involves summaries x(bar) & y(bar)  
<button data-toggle="collapse" data-target="#c1">Correlation Coefficient</button>
<div id="c1" class="collapse">
\begin{equation}
\hat{\rho}=
\frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}
{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}
\sqrt{\sum_{j=1}^n (y_j-\bar{y})^2}}
\tag{7.1}
\end{equation}
</div>

* In R - use `cor()` function to calculate correlation coefficient
  * When applied to a matrix, this calculates all 2way correlations between <u>continous variables</u> (more in CH9)
  
* Q) Compute the matrix of all correlations between the measurements from turtles data - what do you notice?
```{r}
cor(turtles[,-1]) #remove categorical variable to calculate other columns
```
* A) The square matrix is symmetric & all values close to 1 (diagonal values always 1)

* Beneficial to start multidimensial analysis by checking simple 1D & 2D summary statistics & make visual displays (such as below 2 questions)

* Q) Produce pairwise scatter plots & 1D histograms on the diagonal for turtles data (Use GGally package). & Guess underlying/"true dimension" of the data
* A) 
```{r}
ggpairs(turtles[,-1], axisLabels="none")
```
>All pairs of bivariate scatterplots for the three biometric measurements on painted turtles.

All 3 variables mostly reflect same "underlying" variable (maybe size of the turtle)


* Q) Make pairs plot on athletes data - what do you notice?
```{r}
pheatmap(cor(athletes), cell.width=10, cell.height=10)
```
>Heatmap of correlations between variables in the athletes data. Higher values are color coded red-orange. The hierarchical clustering shows the grouping of the related disciplines.

Shows hierarchical clustering of variables (dendogram trees) -> clusters into 3 groups (running, throwing, jumping)

#### Pre-processing data
* Different variables (columns) may be measured using different units = different baselines & scales -> not directly comparable in this form
  * Common measures of scales are range & standard dev
  * ex) athletes data has measuremens in seconds & metres
    * choice is arbritary (time = secs or milisec, lengths = cm or feet)
* PCA & other methods require transformation to numeric values w/ common scale for comparisons
   * **Centering** = subtract mean, making the mean of centered data as origin
   * **Scaling/Standardizing** = Divide by standard deviation to make new std as 1
     * Centering & Scaling encountered when computing correlation coefficient
       * **Correlation Coefficient** = vector product of centered & scaled variables
     * Use the `scale()` function for centering and scaling
       * default behaviour when given matrix/dataframe = make each column have a mean & stdev of 1
      
* Q) Compute means & stdev of turtle data & use scale function to center+standardize continous variables (name this scaledTurtles) - verify new values for mean & std of scaledTurtles
* A)

```{r}
apply(turtles[,-1],2,mean)
apply(turtles[,-1],2,sd)
scaledTurtles <- scale(turtles[,-1])
apply(scaledTurtles,2,mean)
apply(scaledTurtles,2,sd)
```

```{r}
# How this works is explained in the next 3 code blocks
# Original data
turtles[,-1]
# Scaled data (center = subtract col mean from data points, scale = divide datapoints by col sd)
as.data.frame(scaledTurtles)
# Check by turning turtles into scaled dataset - subtract mean from data then divide by sd of original data (this is called centering + scaling)
Uselessturtlefunction <- function(x){
  length <- (x[,2] - mean(x[,2]))/sd(x[,2])
  width <- (x[,3] - mean(x[,3]))/sd(x[,3])
  height <- (x[,4] - mean(x[,4]))/sd(x[,4])
  combined <- data.frame(length=length, width=width, height=height)
  return(combined)
}
Uselessturtlefunction(turtles)
```


* Q) Make a scatterplot of scaled & centered width + height variables of turtle data -> color points by sex
```{r}
data.frame(scaledTurtles, sex=turtles[,1]) %>% 
  ggplot(aes(x=width, y=height, group=sex)) +
  geom_point(aes(color=sex)) + coord_fixed()
```
> Turtles data projected onto the plane defined by the width and height variables: each point colored according to sex.

* Have encountered data transformation choices in CH4 & 5
  * Used log & asinh functions -> used for variance stabiliztion (make variance of replicate measurements of one & same variable in different parts of its dynamic range more similar)
  * Standardizing transformation used here -> make scale of different variables the same (measured by mean & stv)
  
* May be preferable to leave variables at different scales depending on data (might be important)
  * If original scale for data is important, then leave data as is
* Some data variables have different percisions = priori (More in CH9 - weighting these variables)

* After pre-processing data -> can use data simplification via data reduction

* **
<p id="4"><b><font size="5">Dimension reduction</b></font><a href="#0"><sup>Return</sup></a></p>

* Many different perspectives
  * Invented to reduce 2-variable scatterplot -> single coordinate
  * Used to summarize batched psychological tests ran on same subjects

* Provides overall scores to summarize many test-variables at once
  * PCA is a **unsupervised learning technique** (so is clustering)
    * Treats all variables as having the same status
    * Not trying to predict/explain one vatiable's value from others
      * Uses mathematical modeling to find stucture for all variables (columns)
    * Exploratory technique to map & show relations between variables + observations
    
* Usually suppose to use linear algebra methods but textbok tried to lower the use & focus on visualizations+data
  * Textbook uses geometrical projections -> take pts in higher-dimension spaces and projects onto lower dimensions
  
<center><img src="http://web.stanford.edu/class/bios221/book/figure/chap8-projectv-1.png"  width="250" height="250"></center>

>Point A is projected onto the red line generated by the vector v. The dashed projection line is perpendicular (or orthogonal) to the red line. The intersection point of the projection line and the red line is called the orthogonal projection of A onto the red line generated by the vector v.
  
* <mark style='background-color:yellow'>PCA is a linear technique, which means it looks for linear-relationships between variables</mark> (f(ax+by)=af(x)+b(y))
  * Linear constraints decreases computational requirements (non-linear in CH9)

#### Lower-dimensional projections

```{r}
# One way of projecting 2D data onto line via athletes data
athletes <- data.frame(scale(athletes))
ath_gg <- ggplot(athletes, aes(x = weight, y = disc)) + geom_point(size=2, shape=21)
ath_gg + geom_point(aes(y=0), colour="red") + geom_segment(aes(xend=weight, yend=0), linetype="dashed")
```
> Scatterplot of two variables showing the projection on the horizontal x axis (defined by y = 0 ) in red and the lines of projection appear as dashed.

* Task
  * a) Calculate the variance of the red points in the above figure
  * b) Make a plot showing projection lines onto y-axis & projected points
  * c) compute the variance of the points projected onto y-axis

* a) 
```{r}
var(athletes$weight)
```

* b)
```{r}
ath_gg1 <- ggplot(athletes, aes(x = weight, y = disc)) + geom_point(size=2, shape=21)
ath_gg + geom_point(aes(x=0), colour="red") + geom_segment(aes(xend=0, yend=disc), linetype="dashed")
```

* c)
```{r}
var(athletes$disc)
```

#### How to summarize 2D data by a line? 
* Lose info about points when projected from 2D (plane) -> 1D (line)
  * If we do this by using original coordinates (as above scatter plot on athelete$weight) -> lose info about disc variable (ex) project onto x = lose info of y, and opposite is true - so find an angle of projection that minimizes loss of both variables)
  * <mark style='background-color:yellow'>Goal is to keep info about both variables</mark>
  
* Many ways to project point cloud onto a line (ex) regression lines)

* **Regressing one variable on the other**
  * Linear regression - lines that summarize scatter plot
    * **supervised method** which prefers minimizing the residual sum of squares in one direction (the response variable)
    
* **Regression of disc variable on weight of athletes data**
  * `lm()` linear model function to find regression line
    * Slope and intercept given by values in coefficients of resulting object reg1

```{r}
reg1 <- lm(disc~weight, data=athletes)
a1 <- reg1$coefficients[1] #this is intercept
b1 <- reg1$coefficients[2] #this is slope
pline1 <- ath_gg + geom_abline(intercept=a1, slope=b1, col="blue", lwd=1.5)
pline1 + geom_segment(aes(xend=weight, yend=reg1$fitted), colour="red", arrow=arrow(length=unit(0.15,"cm"))) 
```
> The blue line minimizes the sum of squares of the vertical residuals (in red).

* **Regression of weight on discus**
```{r}
# Produces line by reversing roles of 2 vars (weight -> response variable)
reg2 <- lm(weight ~ disc, data=athletes) #note weight becomes y = response
a2 <- reg2$coefficients[1] #intercept
b2 <- reg2$coefficients[2] #slope
pline2 <- ath_gg + geom_abline(intercept=-a2/b2, slope=1/b2, col="darkgreen", lwd=1.5)
pline2 + geom_segment(aes(xend=reg2$fitted, yend=disc), colour="orange", arrow=arrow(length(unit(0.15, "cm"))))
```
> The green line minimizes the sum of squares of the horizontal residuals (in orange)

* Each of the shown regression lines is an approximate linear relationship between disc & weight of athletes data
  * Relationship differs depending on which variables chosen to be predictor & which is response
  
* Q) How large is variance of projected points that lie on blue regression line above? Compare to variance of the data when projected on original axes disc and weight
* A) Pyrhagoras' theorem shows squared length of hypotenuse of right-angled triangle is = to sum of squared lengths of the 2other sides
```{r}
var(athletes$weight) + var(reg1$fitted)
```
Variences of points on the original axes weight & disc are 1 (scaled variables)

* **Line minimizing distances in both directions**
  * Line chosen to minimize sum of squares of orthagonal (perpendicular) projections of data pts = **principle component line**
```{r}
xy <- cbind(athletes$disc, athletes$weight) #make matrix
svda <- svd(xy) #calculate singular value decomposition PC's 
pc = xy %*% svda$v[, 1] %*% t(svda$v[, 1]) #transform matrix into subspace using calculated svd (in this case pc=A*V*t(V) - note %*% is used for matrix multiplication & V = scalar values (direction/eigenvalues)
bp = svda$v[2, 1] / svda$v[1, 1]
ap = mean(pc[, 2]) - bp * mean(pc[, 1])
ath_gg + geom_segment(xend = pc[, 1], yend = pc[, 2]) +
  geom_abline(intercept = ap, slope = bp, col = "purple", lwd = 1.5)
```
> The purple principal component line minimizes the sums of squares of the orthogonal projections.

* Q) a) What is particular about the slope of the purple line? <br> b) Redo the plots on the orignal(unscaled) variables - what happens?</br>
* A) Lines computed depends on choice of units. Made stdev equal for both variables -> PCA line is diagonal cutting through middle of both regression lines
  * Data was centered via subtracting means = line passes through origin (0,0)
```{r}
xy1 <- cbind(unscaledAthletes$disc, unscaledAthletes$weight)
svda1 <- svd(xy1)
pc1 = xy1 %*% svda1$v[, 1] %*% t(svda1$v[, 1])
bp1 = svda1$v[2, 1] / svda1$v[1, 1]
ap1 = mean(pc1[, 2]) - bp1 * mean(pc1[, 1])
ath_gg + geom_segment(xend = pc1[, 1], yend = pc1[, 2]) +
  geom_abline(intercept = ap1, slope = bp1, col = "purple", lwd = 1.5)
```

* Q) Compute variance of points on the purple line
* A) Coordinates of points computer when plot was made (in pc vector)
```{r}
apply(pc, 2, var)
sum(apply(pc,2,var))
```
Variences along this axis is longer vs other variences calculated in the previous Pythagoras question

<center><img src="http://web.stanford.edu/class/bios221/book/figure/chap8-chap8-r-PCAR1R2-1-1.png"  width="250" height="250"></center>
> 3 ways of fitting a line: The blue line minimizes the sum of squares of the vertical residuals, the green line minimizes the horizontal residuals (Blue/Green = linear regression), the purple line, called the principal component (SVD), minimizes the orthogonal projections. Notice the ordering of the slopes of the three lines.

* Pythagoras' theorem tells 2 things
  * If minimize both horizontal/vertical directions -> minimizing the orthagonal projections onto line from each pt
  * Total variability of pts measured by sum of squares of the projection of the pts onto center = origin (0,0) if data is centered
    * Can decompose into sum of squares of projections onto line + variances along the line
    * For fixed variance, minimizing projection distances -> maximizes variance along the line
    * Define first principal component -> line w/ max variance

* **
<p id="5"><b><font size="5">The new linear combinations</b></font><a href="#0"><sup>Return</sup></a></p>

* PC line previously found can be written as 

\begin{equation}
PC= \frac{1}{2} \mbox{disc} + \frac{1}{2} \mbox{weight}.
\tag{7.2}
\end{equation}

* PCs are linear combinations of variables which was originally measured -> provide new coordinates
  * Understanding linear combination (analogy)
    * Make healthy juice, need a recipe like the following
    
\begin{equation*}
V = 2 \times \text{Beet} + 1 \times \text{Carrot} +
\frac{1}{2} \text{Gala} +
\frac{1}{2} \text{GrannySmith} +
0.02 \times \text{Ginger} +
0.25 \times \text{Lemon}.
\end{equation*}

* This recipe is a linear combination of individual juice types (original variables) which result in new variable (V) w/ coefficients (2, 1, 0.5, 0.5, 0.02, 025) AKA loadings

* Q) How would you calculate the calories of the juice?
* A) IDK

#### Optimal lines
* Linear combination of variables define a line in higher dimensions, same as constructing lines in a scatterplot plane (2D)
  * Many ways to choose lines to project data but there is a "best line"
    * Total variance of all points in all variables can be de-decomposed
    * PCA -> total sums of squares distances between points & any line can be decomposed into distances to line & variance along line
* PC (variable calculated above for line question) minimizes distance to line & maximizes variance of projections along line
* Why maximize variance along line? - see following example of 3D -> 2D

* Q) Which of the 2 projections do you find more informative? 
<center><img src="http://web.stanford.edu/class/bios221/book/images/CAM3.png"  width="250" height="250"><img src="http://web.stanford.edu/class/bios221/book/images/CAM4.png"  width="250" height="250"></center>
* A) Author states the projection minimizing the area of the shadow is more informative (the 2nd image of a clear camel)
  * Choosing the perspective w/ maximal spread (variance) of points = more information
  * Want to see highest variation as possible (PCA)

* **
<p id="6"><b><font size="5">The PCA workflow</b></font><a href="#0"><sup>Return</sup></a></p>

<center><img src="http://web.stanford.edu/class/bios221/book/images/orgacp1.png"  width="250" height="500"></center>

* PCA is based on finding axis w/ largest variability, removing the variability in that direction, and iterating to find next best orthogonal axis
  * Don't have to run iterations -> all axes can be found via **Singular Value Decomposition (SVD)**
  
* In figure above
  1) Calculate means and variances
  2) Pick to work with rescaled covariences (correlation matrix) or not
  3) Pick _k_, number of components relevant to data (think she means eigenvalues & associated eigenvectors)
    * k = rank of the approximation chosen - give careful explaination of how choice is made
* Impossible to choose #of components before doing analysis (unlike clustering)
  * Choice of k needs visualization of the plot of <u>variances</u> & successive principal components before projecting the data
* End result of PCA workflow = useful maps for variables & samples
  * Understanding how maps are made will decrease information needed

* **
<p id="7"><b><font size="5">The inner workings of PCA: rank reduction</b></font><a href="#0"><sup>Return</sup></a></p>
* This section may require a background in linear algebra (even if faint)
  * Provides insight into SVD method used for PCA
* SVD (singular value decomposition) of a matrix is used to find horizontal & vertical vectors (singular vectors) & normalizing values (singular values) (S = UAV^t)

#### Rank-one matricies
* Simple generative model shows the meaning of rank of a matrix & how to find it
  * Suppose 2 vectors (_u_ = a one column matrix)
  * Suppose v<sup>t</sup>=t(v) (a one row matrix - transposed from one column matrix v)
  * Multiply a copy of _u_ by each elements of v<sup>t</sup> looks like the following 
    * (row x col)
    * U = c(1,2,3,4) a col vector
    * Vt = c(2,4,8) a row vector
  
* Step1

|X|2|4|8|
|---|---|---|---|
1| | | |
2| | | |
3| | | |
4| | | |

* Step2

|X|<font color="green">2</font>|4|8|
|---|---|---|---|
<font color="blue">1</font>|<font color="darkyellow">2</font>| | |
2|4| | |
3|6| | |
4|8| | |

* Step3

|X|2|4|8|
|---|---|---|---|
1|2|4| |
2|4|8| |
3|6|12| |
4|8|16| |

* Step4

|X|2|4|8|
|---|---|---|---|
1|2|4|8|
2|4|8|16|
3|6|12|24|
4|8|16|32|

* Matrix X here is said to be rank-1 b/c _u_ and _v_ both have one column  (note not V<sup>t</sup> which is mentioned here... this is completely misleading. U is matrix w/ 1 col & V (the untransposed V<sup>t</sup>) is also a matrix w/ 1 col)

<button data-toggle="collapse" data-target="#c2">column vectors u & v</button>
<div id="c2" class="collapse">

<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>u</mi>
  <mo>=</mo>
  <mrow>
    <mo>(</mo>
    <mstyle scriptlevel="1">
      <mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false">
        <mtr>
          <mtd>
            <mn>1</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>2</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>3</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>4</mn>
          </mtd>
        </mtr>
      </mtable>
    </mstyle>
    <mo>)</mo>
  </mrow>
</math> 

&

<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mi>v</mi>
  <mo>=</mo>
  <mrow>
    <mo>(</mo>
    <mstyle scriptlevel="1">
      <mtable rowspacing=".2em" columnspacing="0.333em" displaystyle="false">
        <mtr>
          <mtd>
            <mn>2</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>4</mn>
          </mtd>
        </mtr>
        <mtr>
          <mtd>
            <mn>8</mn>
          </mtd>
        </mtr>
      </mtable>
    </mstyle>
    <mo>)</mo>
  </mrow>
</math>

The transpose of v is written v<sup>t</sup>= t(v)=(2 4 6 8).  

The (2, 3) entry of the matrix X, written x<sub>2, 3</sub>, is obtained by multiplying u<sub>2</sub> by v<sub>3</sub>. We can write this

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
  <mi>X</mi>
  <mo>=</mo>
  <mrow>
    <mo>(</mo>
    <mtable rowspacing="4pt" columnspacing="1em">
      <mtr>
        <mtd>
          <mn>2</mn>
        </mtd>
        <mtd>
          <mn>4</mn>
        </mtd>
        <mtd>
          <mn>8</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>4</mn>
        </mtd>
        <mtd>
          <mn>8</mn>
        </mtd>
        <mtd>
          <mn>16</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>6</mn>
        </mtd>
        <mtd>
          <mn>12</mn>
        </mtd>
        <mtd>
          <mn>24</mn>
        </mtd>
      </mtr>
      <mtr>
        <mtd>
          <mn>8</mn>
        </mtd>
        <mtd>
          <mn>16</mn>
        </mtd>
        <mtd>
          <mn>32</mn>
        </mtd>
      </mtr>
    </mtable>
    <mo>)</mo>
  </mrow>
  <mo>=</mo>
  <mi>u</mi>
  <mo>&#x2217;<!-- ∗ --></mo>
  <mi>t</mi>
  <mo stretchy="false">(</mo>
  <mi>v</mi>
  <mo stretchy="false">)</mo>
  <mo>=</mo>
  <mi>u</mi>
  <mo>&#x2217;<!-- ∗ --></mo>
  <msup>
    <mi>v</mi>
    <mi>t</mi>
  </msup>
</math>

>Note - row X col (v<sup>t</sup> * u)

</div>

* Q) Why can we say X=u*v<sup>t</sup> is better than spelling out the whole matrix?
* A) If we wanted to reverse the process & simplify another matrix X. Can we express it in a similar way as a product of vectors w/o loss of info? 

* Q) decompose this matrix

\begin{equation}
\begin{array}{rrrrr}
\hline
{\large X} & x_{.1} &  x_{.2} &  x_{.3} &  x_{.4} \\
\hline
x_{1.} & 780 & 936 & 1300 & 728\\
x_{2.} &  75 &  90 &  125 &  70\\
x_{3.} & 540 & 648 &  900 & 504\\
\hline
\end{array}

\tag{7.3}
\end{equation}

* X has been redrawn as a series of rectangles as follows (coloured boxes = areas proportional to numbers in cells of the matrix)

<center><img src="http://web.stanford.edu/class/bios221/book/images/SVD-mosaicXplot0.png"  width="250" height="250"></center>

> Some special matrices have numbers in them that make them easy to decompose. Each colored rectangle in this diagram has an area that corresponds to the number in it.

* What numbers can we put in the white _u_ & _v_ boxes so the values of the sides of the rectangle give the numbers as their products?
  * A matrix w/ special property of being perfect "rectangular" (ex) matrix X) said to be rank-1
  * Can represent the numbers in X by areas of rectangles - sides of rectangles are given by values in side vectors (_u_ & _v_)
  
<center><img src="http://web.stanford.edu/class/bios221/book/images/SVD-mosaicXplot1.png"  width="250" height="250">
<img src="http://web.stanford.edu/class/bios221/book/images/SVD-mosaicXplot2.png"  width="250" height="250">
<img src="http://web.stanford.edu/class/bios221/book/images/SVD-mosaicXplot3.png"  width="250" height="250"></center>

> The numbers in the cells are equal to the product of the corresponding margins in (A), (B) and (C). We could make the cells from products in several ways. In (C) we force the margins to have norm 1 .

* Decomposition of X is not unique, there are several candidates for vectors _u_ & _v_ (3 different ways to get area of rectangles as above)
  * Choose marginal vectors so each vector has the coordinats **sum of squares add to 1** (vectors _u_ and _v_ have norm1)
  * Keep track of one extra number (#used to multiply each of the products & represents "overall scale" of X)
    * This is the #in the white box at the upper left corner of matrix 3 above (2348.3) = **singular value s1** (note singular value is the square root of the eigenvalue used to form col/row of matrix U/V)
    
* In this R-code, start by assuming values in _u_, _v_, and s1 is known
  * Will show a function to find these values later
  
```{r}
#Check multiplication & norm properties

X = matrix(c(780,  75, 540,
             936,  90, 648,
            1300, 125, 900,
             728,  70, 504), nrow = 3)
u = c(0.8196, 0.0788, 0.5674)
v = c(0.4053, 0.4863, 0.6754, 0.3782)
s1 = 2348.2
sum(u^2)
sum(v^2)
s1*u %*% t(v) 
X-s1*u %*% t(v)
```

* Q) Try svd(X) and look for components of the output of `svd()` function
  * Check norm of the columns of the matracies that result from this
  * Where did the value s1=2348.2 come from?
* A)
```{r}
svd(X)$u[, 1]
svd(X)$v[, 1]
sum(svd(X)$u[, 1]^2)
sum(svd(X)$v[, 1]^2)
svd(X)$d #these are singular values (square roots of eigenvalues)
```

* In this case, can see the 2nd & 3rd singular values are 0 (not looking at decimals - 2.141733e-13 6.912584e-15)
  * This is why we say matrix X is rank-1 
* Usually in other matricies X, it is rare to write X as this two-vector product
  * Next shown, how to decompose X when it is not rank-1
  
#### How do we find such a decomposition in a unique way?
* Above decomposition = 3elements (horizontal, vertical, single vectors, and diagonal corner(singular value))
  * Can find these via singular value decomposition function (svd)
```{r}
#example

Xtwo = matrix(c(12.5, 35.0, 25.0, 25, 9, 14, 26, 18, 16, 21, 49, 32,
       18, 28, 52, 36, 18, 10.5, 64.5, 36), ncol = 4, byrow = TRUE)
USV = svd(Xtwo)
```

* Q) Look at USV object (result from svd() function). What are the components?
* A)
```{r}
names(USV)
USV$d #singular values
```
135.1 is the first singular value in USV$d[1] (1.351*e02)

* Q) Check how each succesive pair of singular vectors improves approximation of Xtwo. What do you notice about the 3rd & 4th singular values?

```{r}
Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1])

Xtwo - USV$d[1] * USV$u[, 1] %*% t(USV$v[, 1]) -
       USV$d[2] * USV$u[, 2] %*% t(USV$v[, 2])
```

* 3rd & 4th values are small, don't improve approximation -> conclude Xtwo is rank-2 (b/c 2 singular values/4 are above 0)

* Many ways to write rank-2 matrix (ex) Xtwo) as sum of rank-1 matrix
  * in order to ensure uniqueness, we add another condition to the singular vectors (above used norm of vectors=1)
* Output vectors of singular decomposition don't have only norms=1
  * each vector in U matrix is ortogonal to all previous
  * Same for V matrix (\begin{equation}u_{\bullet 1} \perp u_{\bullet 2}\end{equation} -> sum of products of values in the same positons is 0) \begin{equation}\sum_i u_{i1} u_{i2} = 0\end{equation}

* Task: Check ortonormality by computing cross product of U & V matracies
```{r}
t(USV$u) %*% USV$u
t(USV$v) %*% USV$v
```

* Submit rescaled turtles matrix to SVD
```{r}
turtles.svd = svd(scaledTurtles)
turtles.svd$d #singular values (squared eigenvalues)
turtles.svd$v #v matrix (A)^t(A)
dim(turtles.svd$u) #u matrix (A)(A)^t
```

* Q) What can be concluded from tutles matrix resulting svd output?
* A) 1st column of turtles.svd$v shows coefficients for 3 variables which are ~equal
  * Other noticeable coincidences
```{r}
sum(turtles.svd$v[,1]^2)
sum(turtles.svd$d^2) / 47
```
Coefficients are srt(1/3) & sum of squares of the singular values is = (n-1)xp

#### **Singular value decomposition** 
* X is decomposed <u>additively</u> into rank-1 pieces
  * Each _u_ vectors is combined into U matrix
  * Each _v_ vectors is combined into V matrix
* SVD is 

\begin{equation}
{\mathbf X} = U S V^t, V^t V={\mathbb I}, U^t U={\mathbb I},
\tag{7.4}
\end{equation}

* Where
  * S = diagonal matrix of singular values
  * V<sup>t</sup> is the transposed V
  * I = Identity matrix
* Can rewite element-wise as 

\begin{equation*}
X_{ij} = u_{i1}s_1v_{1j} +  u_{i2}s_2v_{2j} + u_{i3}s_3v_{3j} +... +
u_{ir}s_rv_{rj},
\end{equation*}

* **U & V are said to be orthonormal (norm=1 - not related to normal distribution) b/c self-crossproducts are identity matrix**
```{r}
#check this statement
round(crossprod(USV$v, USV$v),1)
round(crossprod(USV$u, USV$u),1)
```
Confirmed

#### Princial components
* Singular vectors from the SVD (`svd()`) contain coefficents to put in front of original variables to make more informative = Principal component
  * Written as 

\begin{equation*}
Z_1=c_1 X_{\bullet 1} +c_2 X_{\bullet 2}+ c_3 X_{\bullet 3}+\cdots c_p X_{\bullet p}.
\end{equation*}

* If usv=svd(X), then (c1, c2, c3, ...) given by 1st column of usv$v
  * New variables Z1,Z2,Z3,... will have variences decreasing in size \begin{equation}s_1^2 \geq s_2^2 \geq s_3^2 \geq ...\end{equation}
  
* <mark style='background-color:lightgreen'>Q) Compute 1st PC for turtles data by multiplying by 1st singular value of usv$d[1] by usv$u[,1]. What is another way of computing it?</mark>
* A) 
```{r}
turtles.svd$d[1] %*% turtles.svd$u[,1]
scaledTurtles %*% turtles.svd$v[,1]
```

**Matrix multiplications can show that XV & US are the same**  
  * Remember V is orthogonal (so V<sup>t</sup>V = I & XV=USV<sup>t</sup>V=USI) -> where I = identity matrix
  
* If lost about which output matracies to deal with, think of dimension of PCs
  * each C=(c1,c2,...,cp) is of length n (#of observations)
  * n is also the number of rows of each of the vectors X1, X2,..,Xp
* 2 useful facts
  * ?? always chosen to be less than # of original variables or #of observations
    * Lowering the dimension problem: 
  
  \begin{equation*}
k\leq \min(n,p)
\end{equation*}

* **PC Transformation**
  * 1st PC has the largest varience (accounts for most vatiability in data as possible)
  * Each successive PC has highest varience possible
    * Constrained due to having to be ortogonal to preceeding components
    
\begin{equation*}
\max_{aX \perp bX}\mbox{var}(\mbox{Proj}_{aX} (X)), \qquad \mbox{where } bX=\mbox{previous components}
\end{equation*}

  
* **
<p id="8"><b><font size="5">Plotting the observations in the principal plane</b></font><a href="#0"><sup>Return</sup></a></p>

> In fact, PCA is such a fundamental technique that there are many different implementations of it in various R packages. Unfortunately, the input arguments and the formating and naming of their output is not standardized, and some even use different conventions for the scaling of their output. We will experiment with several different ones to familiarize ourselves with these choices.

* Using atheletes data ($discus & $weight)
  * Previously found prinipal component (PC) - the purple line in dimension reduction (above)
  * Showed that Z1 is the linear combination of the diagonal
  * Coefficients sum of squares must add to 1, so Z1=(-0.707*athletes$disc)-(0.707*athletes$weight)
```{r}
svda$v[,1]
```


> This is the same as if the two coordinates were c1 = 0.7071 and c2 = 0.7071.

* Q) What part of the output of svd functions leads us to the **first PC coefficients, also known as PC loadings?**
* A) 
```{r}
svda$v[,1]
```

* If we roate the plane (discus, weight) into making the purple line the horizontal x-axis -> this is the **first principal plane**

```{r}
ppdf = tibble(PC1n = -svda$u[, 1] * svda$d[1], #PC1 values is U[col1] %*% singularvalue[1] put into tibble
              PC2n = svda$u[, 2] * svda$d[2]) #PC2 values is U[col2] %*% singularvalue[2] put into tibble
ggplot(ppdf, aes(x = PC1n, y = PC2n)) + geom_point() + xlab("PC1 ")+
    ylab("PC2") + geom_point(aes(x=PC1n,y=0),color="red") +
    geom_segment(aes(xend = PC1n, yend = 0), color = "red") +
    geom_hline(yintercept = 0, color = "purple", lwd=1.5, alpha=0.5) +
    xlim(-3.5, 2.7) + ylim(-2,2) + coord_fixed()

#no clue what this is trying to show
segm = tibble(xmin = pmin(ppdf$PC1n, 0), xmax = pmax(ppdf$PC1n, 0), yp = seq(-1, -2, length = nrow(ppdf)), yo = ppdf$PC2n)
ggplot(ppdf, aes(x = PC1n, y = PC2n)) + geom_point() + ylab("PC2") + xlab("PC1") +
    geom_hline(yintercept=0,color="purple",lwd=1.5,alpha=0.5) +
    geom_point(aes(x=PC1n,y=0),color="red")+
    xlim(-3.5, 2.7)+ylim(-2,2)+coord_fixed() +
    geom_segment(aes(xend=PC1n,yend=0), color="red")+
    geom_segment(data=segm,aes(x=xmin,xend=xmax,y=yo,yend=yo), color="green",alpha=0.5)
```

> In the case where we only have two original variables, the PCA transformation is a simple rotation; the new coordinates are always chosen to be the horizontal and vertical axes.

* Q) 
  * a) What is the mean of the sums of squares of the red segments?
  * b) How does this compare to the varience of the red points?
  * c) Compute ratio of the stdev of red segments to green segements. Compare this to ratio of singular values 1 & 2
  
* A) 
  * a) Mean sums of squares of red segments = square of 2nd singular value
```{r}
svda$d[2]^2
```
  * b) Varience of red points is var(ppdf$PC1n)
    * This is larger vs mean sums of squares calculated in a (????)
```{r}
var(ppdf$PC1n)
```

  * c) Take ratios of stdev of pts on vertical & horizontal axes via
```{r}
sd(ppdf$PC1n)/sd(ppdf$PC2n)  #horizontal PC1 vs vertical PC2
svda$d[1]/svda$d[2] #or singular value 1 vs singular value 2
```

* Use `prcomp()` to find PCA of the 1st 2 columns of athletes dataset - look at output then compare to `svd()`
```{r}
prcomp(athletes[,1:2])
svd(athletes[,1:2])$v
```

#### PCA of turtles data

* "true" dimensionality of rescaled data (scaledTurtles)
  * Use `prcomp()` to list all values needed to plot PCA
```{r}
cor(scaledTurtles) #generate a correlation matrix
```
```{r}
# Find PCs
pcaturtles <-  princomp(scaledTurtles)
pcaturtles
```

```{r}
fviz_eig(pcaturtles, geom = "bar", bar_width = 0.4) + ggtitle("")
```

> The screeplot shows the eigenvalues for the standardized turtles data (scaledTurtles): there is one large value and two small ones.The data are (almost) one-dimensional. We will see why this dimension is called an axis of size, a frequent phenomenon in biometric data.

* Q) Many PCA functions created by different people -> problematic naming conventions
  * Compare 4 of them & look at the results
  * What happens when you disable the scaling in the prcomp and princomp functions?
```{r}
svd(scaledTurtles)$v[, 1]
prcomp(turtles[, -1])$rotation[, 1]
princomp(scaledTurtles)$loadings[, 1]
ade4::dudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]
```

* Comparable when scalings on (default = off)

```{r}
svd(scaledTurtles)$v[, 1]
prcomp(turtles[, -1], scale=T)$rotation[, 1]
princomp(scaledTurtles, cor=T)$loadings[, 1] #cor is the scaling for princomp
ade4::dudi.pca(turtles[, -1], nf = 2, scannf = FALSE)$c1[, 1]
```

* For the following, assume matrix X is centered (mean subtracted) & scaled (div stdev)

* Q) Coordinates ofthe observations in the new variables from `prcomp()` = res -> prcomp()$scores
  * Look at PC1 for scaledTurtles & compare to res$scores
  * Compare stdev sd1 to res & stdev of scores
```{r}
res = princomp(scaledTurtles)
PC1 = scaledTurtles %*% res$loadings[,1] #compute PC1 using A x v 
sd1 = sqrt(mean(res$scores[, 1]^2))
```

* Combine PC scores (U*S matrix) & loading-coefficients(V)
  * **Plots w/ both samples & variables represented = biplot (PCA score + loarding plot)**
  * use factoextra package
  
```{r}
fviz_pca_biplot(pcaturtles, label="var", habillage = turtles[,1]) + ggtitle("")
```
> A biplot of the first two dimensions showing both variables and observations. The arrows show the variables. The turtles are labeled by sex. The extended horizontal direction is due to the size of the first eigenvalue, which is much larger than the second.

> Beware the aspect ratio when plotting a PCA. It is rare to have the two components be of similar norm, so square shaped plots will be the exception. More common are elongated plots, which show that the horizontal (first) principal component is more important than the second. This matters, e.g., for interpreting distances between points in the plots.

* Q) Is it possible to have a PCA plot with the PC1 as the horizontal axis whose height is longer than its width?
* A) 

> The variance of points in the PC1 direction is \begin{equation}\lambda_1=s_1^2\end{equation} which is always larger than \begin{equation}\lambda_2=s_2^2\end{equation}, so the PCA plot will always be wider than high.

* Q) Looking at the above biplot, does male or female turtles tend to be larger & what do the arrows tell us about the correlations?
* A) Larger in terms of what (width or length?). Arrows tell if variable is +ve/-ve correlated 

* Q) Compare variance of each new coordinate to the eigenvalues produced by PCA `dudi.pca()` - no clue what's happening here
* A) 
```{r}
pcadudit = dudi.pca(scaledTurtles, nf = 2, scannf = FALSE)
apply(pcadudit$li, 2, function(x) sum(x^2)/48)
pcadudit$eig
```

* Look at relationships between variables (old vs new)
  * Do this by drawing a **correlation circle**
    * Aspect ratio=1 & variables represented by arrows
    * Lengths of arrows indicate quality of projections onto PC1
```{r}
fviz_pca_var(pcaturtles, col.circle = "black") + ggtitle("") +
  xlim(c(-1.2, 1.2)) + ylim(c(-1.2, 1.2))
```
> Part of the “circle of correlations” showing the original variables. Their correlations with each other and with the new principal components are given by the angles between the vectors and between the axes and the vectors

* Q) Explain the relationships between the number of rows of our turtles data matrix and the following numbers:
```{r}
svd(scaledTurtles)$d/pcaturtles$sdev
sqrt(47)
```

* A) 

> When computing the variance covariance matrix, many implementations use 1 / (n−1) as the denominator. Here, n = 48 so the sum of the variances are off by a factor of 48/47.

> If we only want the first one then it is just c1 = s1u1 note  
<math xmlns="http://www.w3.org/1998/Math/MathML">
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msub>
    <mi>c</mi>
    <mn>1</mn>
  </msub>
  <mrow class="MJX-TeXAtom-ORD">
    <mo stretchy="false">|</mo>
  </mrow>
  <msup>
    <mrow class="MJX-TeXAtom-ORD">
      <mo stretchy="false">|</mo>
    </mrow>
    <mn>2</mn>
  </msup>
  <mo>=</mo>
  <msubsup>
    <mi>s</mi>
    <mn>1</mn>
    <mi>t</mi>
  </msubsup>
  <msub>
    <mi>u</mi>
    <mn>1</mn>
  </msub>
  <msubsup>
    <mi>u</mi>
    <mn>1</mn>
    <mi>t</mi>
  </msubsup>
  <msub>
    <mi>s</mi>
    <mn>1</mn>
  </msub>
  <mo>=</mo>
  <msubsup>
    <mi>s</mi>
    <mn>1</mn>
    <mn>2</mn>
  </msubsup>
  <msubsup>
    <mi>u</mi>
    <mn>1</mn>
    <mi>t</mi>
  </msubsup>
  <msub>
    <mi>u</mi>
    <mn>1</mn>
  </msub>
  <mo>=</mo>
  <msubsup>
    <mi>s</mi>
    <mn>1</mn>
    <mn>2</mn>
  </msubsup>
  <mo>=</mo>
  <msub>
    <mi>&#x03BB;<!-- λ --></mi>
    <mn>1</mn>
  </msub>
</math>

* This is a good example that all variation in data can be captured in low-dimensional space
  * 3D data replaced by a line here
* X<sup>t</sup>C=VSU</sup>t</sup>US=VS</sup>2</sup>
  * **The principal components (PC) = columns of the matrix C=US**
  * p columns of U (matrix of USV$u) = output from `svd()` -> rescaled to have norms (s<sub>1</sub><sup>2</sup>,s<sub>2</sub><sup>2</sup>,...,s<sub>p</sub><sup>2</sup>)
    * Each column has a different variance to explain (note in decreasing #'s)
    
* If matrix X is from study of n different samples, then PC generates new coordinates for the n points (**called scores in results of PCA functions**)

> Eigen decompositions = crossproduct of X with itself satifies  
X<sup>t</sup>X = VSU<sup>t</sup>USV<sup>t</sup>=VS<sup>2</sup>V<sup>t</sup>=VΛV<sup>t</sup>  
V = eigenvector matrix of symmetric matrix X<sup>t</sup>X  
Λ is the diagonal matrix of eigenvalues of X<sup>t</sup>X

* Summary of SVD and PCA
  * Each PC has varience measured by eigenvalue (square of associated singular value)
  * New (transformed) variables are orthogonal
    * Since they are also centered -> they are uncorrelated (in normal distriubted data = independent data)
  * When variables rescaled, sum of variences of all variables = #of variables(=p)
    * sum of variences computed by adding diagonal crossproduct matrix
      * Sum of diagonal elements = **trace** of matrix
  * PCs are ordered by size of the eigenvalues
    * Use screeplot to decide how mnay components (eigenvalues) to keep
      * Use biplot & annotate each PC axis w/ proportion of varience explained 

#### Complete analysis: decathlon athletes (step by step)
* First, look at rounded correlation matrix to see essential bivariate associations
```{r}
cor(athletes) %>% round(1)
```

* Second, look at screeplot to choose rank k for data
  * This shows eigenvalues in screeplot (where the clear drop appears - should be the afther the 2nd value in this data)
    * Good approximation for how many eigen values to keep (in this case rank2)
```{r}
pca.ath <- dudi.pca(athletes, scannf=F)
pca.ath$eig
```
```{r}
fviz_eig(pca.ath, geom="bar", bar_width=0.3) + ggtitle("")
```
> The screeplot is the first thing to consult, it tells us that it is satisfactory to use a two-dimensional plot.

* Interpretation of 1st 2 axes by projecting loadings of original variables projected onto new variables
  * Correlation circle shows projection of original variables onto 2new principal axes
    * Angles between vectors = correlations
```{r}
fviz_pca_var(pca.ath, col.circle="black") + ggtitle("")
```
> Correlation circle of the original variables. Right side of plane = track & field events (m100-m1500), left side of plane = throwing + jumping events

* Is there an opposition of skills characterized by the correlation matrix? (-ve correlation between variables of the 2 groups)
  * How to interpret?
    * Those who threw best have lower scores in track
    * Athletes running short times are stronger (same w/ best throwers & jumpers)
      * should change scores of track variables & redo analysis
      
* Q) What transformations of variables make best athletic performances to vary in same direction (most positively correlated)
* A) Change signs on the running performances -> most variables will become +ve correlated

```{r}
athletes[,c(1,5,6,10)] = -athletes[,c(1,5,6,10)]
cor(athletes) %>% round(1)
```
```{r}
pcan.ath <- dudi.pca(athletes, nf=2, scannf=F)
pcan.ath$eig
```
```{r}
fviz_pca_var(pcan.ath, col.circle="black") + ggtitle("")
```
> Correlation circle after changing the signs of the running variables.

* This correlation circle i the transformed variable
  * can an axis of size (all arrows pointing same direction)
* All -ve coorelations are small
  * screeplot shows no change b/c eigenvalues are unchanged
  * Signs in coefficents of PC loadings for m variables are the only outputs that change
  
* Plot atheletes projected in 1st PC
```{r}
fviz_pca_ind(pcan.ath) + ggtitle("") + ylim(c(-2.5,5.7))
```
> First principal plane showing the projections of the athletes. Do you notice something about the organization of the numbers?

* Q) Look at athletes themselves shown by above graph - notice slight ordering effect. What is the relation between quality of atheletes & number on graph?
* A) Join the dots following the order of numbers - will notice more connecting numbers on one side of plot vs randomly assigned numbers
  * complementary info found on olympic data ($score variable is vector of final scores at competion in 1988)
```{r}
data("olympic", package="ade4")
olympic$score
```

* Make a scatterplot scomparing 1st PC score of athletes to score from data
```{r}
p = ggplot(tibble(pc1 = pcan.ath$li[, 1], score = olympic$score, id = rownames(athletes)),
   aes(x = score, y = pc1, label = id)) + geom_text()
p + stat_smooth(method = "lm", se = FALSE)
```
> Scatterplot of the scores given as a supplementary variable and the first principal component. The points are labeled by their order in the data set. We can see a very strong correlation between this supplementary score variable and the first principal coordinate, why is it not a perfectly linear fit?

* Can see strong correlation between both variables
  * Note athelete#1 has highest score but not highest value for PC1 linear combo (why?)
  
#### How to choose k (#of dimentsions)?
* Step 1 of PCA = make screeplot of variances of the new variables (=eigen values)
  * Can't decide how many dimensions needed before seeing this plot
  * Situations when PC are poorly defined (>2 or 3 PCs have similar variances) - shown in below screeplot
    * subspace corresponding to group of similar eigenvalues exist
    * This would generate a 3D space (by u2, u3, u4 matracies)
      * Vectors not meaningful individually & cant interpret loadings
      * B/c slight change in observations can give completely different set of 3 vectors (would generate same 3D sapce but have different loadings) = Unstable PC
      
<center><img src="http://web.stanford.edu/class/bios221/book/figure/chap8-screeploteq-1.png"  width="250" height="250"></center>
> A screeplot showing ‘dangerously’ similar variances. Choosing to cutoff at a hard threshold of 80% of the variance would give unstable PC plots. With so such cutoff, the axes corresponding to the 3D subspace of 3 similar eigenvalues are unstable and cannot be individually interpreted.
    
* **
<p id="9"><b><font size="5">PCA as an exploratory tool: using extra information </b></font><a href="#0"><sup>Return</sup></a></p>
* Unlike linear regression, PCA treats all variables the same (pre-processing to have equivalent sdevs via scaling)
  * Can map other continous variables/categorical factors onto plots to help interpret results
    * Usually have complementary info on samples (ex) diagonostic labels or cell types)
* Can use complementary variables to update interpretation (ex) Bioconductor SummarizedExperiment class -> 2nd best addtional columns of dataframe w/ numeric data)
  * This information is often stored within row names of the matrix

* Example for storing complementary variables in row names
  * use `substr()` to extract cell types & show screeplot + PCA

```{r}
#Screeplot
pcaMsig3 <- dudi.pca(Msig3transp, center = TRUE, scale = TRUE,
                    scannf = FALSE, nf = 4)
fviz_screeplot(pcaMsig3) + ggtitle("")
```
> T-cell expression PCA screeplot.


```{r}
ids = rownames(Msig3transp)
celltypes = factor(substr(ids, 7, 9))
status = factor(substr(ids, 1, 3))
table(celltypes)
```

```{r}
#PCA plot
cbind(pcaMsig3$li, tibble(Cluster = celltypes, sample = ids)) %>%
ggplot(aes(x = Axis1, y = Axis2)) +
  geom_point(aes(color = Cluster), size = 5) +
  geom_hline(yintercept = 0, linetype = 2) +
  geom_vline(xintercept = 0, linetype = 2) +
  scale_color_discrete(name = "Cluster") + coord_fixed()
```
> PCA of gene expression for a subset of 156 genes involved in specificities of each of the three separate T-cell types: effector, naïve and memory. Again, we see that the plot is elongated along the the first axis, as that explains much of the variance. Notice that one of the T-cells seems to be mislabeled.

#### Mass Spectroscopy Data Analysis
* This data requires careful preprocessing to obtain desired matrix w/ relevant features(in columns)
  * Raw mass spectroscopy readings -> extract peaks of relevant figures then align across multiple samples & estimate peak heights
* Suggests Bioconductor xcms package
* ex) load matrix of data from  mat1xcms.RData file

```{r}
load(here("BookStuff", "data", "mat1xcms.RData"))
dim(mat1)
```
```{r}
pcamat1 = dudi.pca(t(mat1), scannf = FALSE, nf = 3)
fviz_eig(pcamat1, geom = "bar", bar_width = 0.7) + ggtitle("")
dfmat1 = cbind(pcamat1$li, tibble(
    label = rownames(pcamat1$li),
    number = substr(label, 3, 4),
    type = factor(substr(label, 1, 2))))
```
>  Screeplot showing the eigenvalues for the mice data.


```{r}
pcsplot = ggplot(dfmat1,
  aes(x=Axis1, y=Axis2, label=label, group=number, colour=type)) +
 geom_text(size = 4, vjust = -0.5)+ geom_point(size = 3)+ylim(c(-18,19))
pcsplot + geom_hline(yintercept = 0, linetype = 2) +
  geom_vline(xintercept = 0, linetype = 2)
```
> The first principal plane for the mat1 data. It explains 59% of the variance.

* Q) Looking at the above PCA plot, do the samples seem to be randomly placed on the plane? Notice any structure explained by the labels?
* A) 

> The answer becomes (even more) evident if you make this plot. Knockouts are always below their paired wildtype sample. We will revisit this example when we look at supervised multivariate methods in our next chapter.

```{r}
pcsplot + geom_line(colour = "red")
```

#### Biplots and scaling
* Previous example, #variables measured was to large to allow useful concurrent plotting of both variables+samples
* This example, plot PCA biplot of data where chemical measurements made on different wines (also has a categorical wine.class variable)
  * First look at 2d correlations + heatmap of variables
  
```{r}
load(here("BookStuff", "data", "wine.RData"))
load(here("BookStuff", "data", "wineClass.RData"))
wine[1:2, 1:7]
```

```{r}
pheatmap(1 - cor(wine), treeheight_row = 0.2)
winePCAd = dudi.pca(wine, scannf=FALSE)
```
> The difference between 1 and the correlation can be used as a distance between variables and is used to make a heatmap of the associations between the variables.

```{r}
table(wine.class)
```

```{r}
fviz_pca_biplot(winePCAd, geom = "point", habillage = wine.class,
   col.var = "violet", addEllipses = TRUE, ellipse.level = 0.69) +
   ggtitle("") + coord_fixed()
```

> PCA biplot including ellipses for the three types of wine: barolo, grignolino and barbera. For each ellipsis, the axis lengths are given by one standard deviation. Small angles between the vectors Phenols, Flav and Proa indicate that they are strongly correlated, whereas Hue and Alcohol are uncorrelated.

* **PCA Biplot** = shows both space of observations & space of variables
  * In this example
    * Arrows = directions of old variables projected onto plane defined by PC1 & PC2
    * Observations = coloured dots (colors according to type of wine)
    * Can interpret variables directions based on sample points
      * ex) blue points are from barbera group -> shows higher malic acid content vs other wines

* Interpretation of multivariate plots requires use of all information possible
  * This example used samples + groups + variables to explain differences betwene wines
  
#### Example of weighted PCA
* May want to see variablity between different groups/observations but use a weight for each group/observation
  * ex) if groups have different sizes
* Using Hiiragi data -> extract WT samples & top 100 features w/ highest overall variance


```{r}
data("x", package = "Hiiragi2013")
xwt = x[, x$genotype == "WT"] # pull out WT samples
sel = order(rowVars(Biobase::exprs(xwt)), decreasing = TRUE)[1:100]
xwt = xwt[sel, ]
tab = table(xwt$sampleGroup)
tab
```

```{r}
xwt$weight = 1 / as.numeric(tab[xwt$sampleGroup])
pcaMouse = dudi.pca(as.data.frame(t(Biobase::exprs(xwt))),
  row.w = xwt$weight,
  center = TRUE, scale = TRUE, nf = 2, scannf = FALSE)
fviz_eig(pcaMouse) + ggtitle("")
```

> Screeplot from the weighted PCA of the Hiiragi data. The drop after the second eigenvalue suggests that a two-dimensional PCA is appropriate.

```{r}
fviz_pca_ind(pcaMouse, geom = "point", col.ind = xwt$sampleGroup) +
  ggtitle("") + coord_fixed()
```
> Output from weighted PCA on the Hiiragi data. The samples are colored according to their groups.

* Note that from tab data -> group are represented unequally
  * function `dudi.pca()` from ade4 package has row.w= argument to enter weights
    * output of this result is seen in the above screeplot + PCA plot

* **
<p id="10"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>

#### 7.1


* **

<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*

> [Unsupervised learning](https://www.mathworks.com/discovery/unsupervised-learning.html) is a type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses.

>[Supervised learning](https://www.mathworks.com/discovery/supervised-learning.html?s_tid=srchtitle) is a type of machine learning algorithm that uses a known dataset (called the training dataset) to make predictions. The training dataset includes input data and response values. From it, the supervised learning algorithm seeks to build a model that can make predictions of the response values for a new dataset. Using larger training datasets and optimizing model hyperparamamters can often increase the model’s predictive power and ensure that it can generalize well for new datasets. A test dataset is often used to validate the model.

* Discretization
  * Take continous functions/variables & transform into discrete functions/variables
  * 1st step in many types of analysis b/c discrete data easier to analyze
  * Theory
    * Process of discretization
      * Analyze continous values for variables
      * Divide into segments
      * Group into bins
        * Need to know how to select for #bins & how wide bins should be
  * Certain amount of error introducted per discretization
    * goal is to minimize error when choosing #bins & width
    * soluton: increase #intervals for division of function/variable
      * problem: increase intervals = less discretization
      * "what is min# intervals that can be used to divide to get ~accurate results"
  * Types
    * Statistic variable discretization = Discretize one variable at a time (most common)
    * Dynamic variable discretization = discrete all variables simultaneously. Need to keep track of independent interactiosn between variables
    * Unsupervised discretization algorithms = simplest algorithms, specify one parameter (#intervals to use or how many values per interval)
    * Supervised discretization algorithms = don't specify #bins, based on entropy and purity calculations

</body>
  