---
title: "Chapter 4: Mixture Models"
output: html_notebook
---

<body>
      <font face = "Times New Roman">
  
<p id="0"><u><font size="5"><b>Table of contents</b></font></u></p>  
<a href="#1">[1]</a> Mixture Models
<br> &emsp; <a href="#2">[1.1]</a> Summary
<br> &emsp; <a href="#3">[1.2]</a> Finite Mixtures
<br> &emsp; <a href="#4">[1.3]</a> Empirical distributions and the nonparametric bootstrap
<br> &emsp; <a href="#5">[1.4]</a> Infinite Mixtures
<br> &emsp; <a href="#6">[1.5]</a> Excercises</br> 
<a href="#x">[X]</a> Misc.

* **

<p id="1"><b><font size="5">Mixture Models</b></font><a href="#0"><sup>Return</sup></a></p>
* Heterogeneity(diversity) is a problem w/ biological data analysis
  * Quantities in data do not show unimodal distribution
    * ex) CpG island/nonIsland in ch2 have 2 modes
      * Look at data as a simple mixture of <u>few (X)</u> components -> X in this case is 2 == **Finite mixture**
    * Other mixtures can a complex mixture of <u>many (X)</u> components -> X in this case can equal #Observations == **Infinite mixture**
    
* In ch1 used a simple generative model (Poisson distribution) to infer a detected epitope
  * In real life, a proper model fitting this data is not easily done
  * But simple models such as Poisson and Binomial distriburions can be used to build realistic models (Mixture Models)
* Mixtures are natural for flow cytometry data, biometric measurements, RNA-Seq, ChIP-Seq, microbiome analysis & more data types

* **

<p id="2"><b><font size="5">Summary</b></font><a href="#0"><sup>Return</sup></a></p>
* EM algorithm can be used to solve difficult optimization problem
  * iterations of pretending part1 of the solution is known - used to figure out part2 of the solution
  * Then alternate & pretend part2 of the solution is known - used to figure out part1 of the solution
  * Interation because this keeps occuring until solutions converge
* **Finite Mixture Models**
  * Model mixtures of >2 normal distributions w/ different variences & means
    * Decompose a sample from data by using mixture
      * Don't need to know the latent/unknown variable --> This is because of EM algorithm
  * EM approach requirements
    * Know parametric form of distributions
    * Know number of components
    * CH5 - find groupings in data w/o relying on the requirements of the EM approach == Clustering
      * Strong relationship between clustering & mixture model statistics
* **Common Infinite Mixture Models**
  * Infinite mixture models used to contruct new distributions (ex) Gamma-Poisson or Laplace) from simple ones (Binomial, normal, Poisson)
    * examples
      * Mixtures of normals (Hierarchical model on means + Variences)
      * Beta-binomial mixtures (Probability p in binomial distriution is generated by beta(a,b) distribution)
      * Gamma-Poisson for read counts
      * Gamma-exponential for PCR
* **Uses of Mixture Models**
  * Dataset has layers of experimental variability (ex) technical replicates, technician, temporal, etc)
    * Lowest layer measurement of precision may be limited to physical detection limits (ex) baseline Absorbance)
      * May model w/ Poisson distribution if counting-based assay (discrete), or normal distribution if continous measurement
    * Top layer (may involve many layers) (ex) instrument-to-instrument variation, reagent kits, operator)
  * Mixture models show that there is diverse amount of variability(variences) in data
    * Shows that, need to propoerly transform data (ex) Varience stabilizing transformations) before further analysis
      * ex) RNA-seq sample in CH8 - normalize NGS reads in microbial ecology
  * Two-component model in multiple testing
    * CH6
* ECDF & Bootstrapping
  * Use observed sample for mixture -> generate many simulated samples = information about sampling distribution of estimate
    * Bootstrapping method
      * Method to evaluate estimates, including when no formulas are available (Non-parametric)
      

* **
<p id="3"><b><font size="5">Finite Mixtures</b></font><a href="#0"><sup>Return</sup></a></p>
* Simple examples & computer experiments
  * ex) mixture model w/ 2 equal-sized components
    * Decompose generating process into steps
* Step 1
  * Flip a fair coin
  * Make a random normal distribution w/ mean=1, variance=0.25
  * Make a random normal distribution w/ mean =3, variance=0.25 (note mean is the parameter) 
  * repeat these steps 10,000 times
  
![](https://i.ytimg.com/vi/uKRP1ARFxfI/hqdefault.jpg)
```{r}
cflips <- (runif(1e4) > 0.5) # generate 10,000 observations of uniform distribution (min=0, max=1) - are similated values > sd 0.5 (logical) 
table(cflips)
```

```{r}
# This function takes in #flips fl, if fl>=1 then simulate 1 observation using distribution1 parameters, if no flips specified
  # ex) oneFlip(0), then simulate 1 observation using distribution2 parameters
  # In R, TRUE=1 & FALSE=0

oneFlip <- function(fl, mean1=1, mean2=3, sd1=0.5, sd2=0.5){ #Sqrt var(0.25) = 0.5 sd
  if(fl){
    rnorm(1, mean1, sd1)
  } else {
    rnorm(1, mean2, sd2)
  }}
```


* **
<p id="4"><b><font size="5">Empirical distributions and the nonparametric bootstrap</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="5"><b><font size="5">Infinite mixtures</b></font><a href="#0"><sup>Return</sup></a></p>

* **
<p id="6"><b><font size="5">Excercises</b></font><a href="#0"><sup>Return</sup></a></p>


* **

<p id="x"><b><font size="5">Miscellaneous</b></font><a href="#0"><sup>Return</sup></a></p>

*Run* = *Ctrl+Shift+Enter*  
*Insert Chunk* =*Ctrl+Alt+I*  
*Preview*=*Ctrl+Shift+K*

* **Dimensionality**
  * In statistics, how many attributes a dataset has
    * ex) Healthcare data has many variables/attributes -> blood pressure, weight, cholesterol level
    * Often can't capture all variables
      * Some variables are inter-related (ex) Weight + blood pressure)
  * High Dimensional Data
    * a lot of variables = hard to calculate
      * **#Features can > #Observations**
      * ex) Microarray for gene expression - can be made up of many samples (environmental microarray GeneChip), each w/ subset of genes
        * One sample can have millions of gene combos
  * **Dimensionality Curse**
    * Add additional variables to multivariate model
      * Increase dimensions added to dataset = more difficult to predict quantities
      * **each added variable = exponential decrease in prediction power**
    * ex) Predict location of bacteria in 25cm<sup>2</sup> petri dish
      * variable 1 = (2D dish)@25cm<sup>2</sup>
      * add variable 2 = (3D beaker)@125cm<sup>3</sup>
      * note exponent increase
        * Increase computational burden -> harder to predict location
    * Statistical dimensionality curse
      * sample size (n) increases exponentially with data (w/ d dimensions) -> increase dimensions = increase sample size
  * Reduce Dimensionallity
    * Simplify understanding of data numerically or visually
    * Data integrity is maintained
    * ex) Combine related data into groups (MDS -> multidimensional scaling)
      * This also identifies similarities in data
    * ex) Clustering to group
  * Unidimensionality vs Multidimensionality 
    * Uni
      * Measurement scale w/ only one dimension - can be use to measure single attribute
      * Represented by a single number line
        * ex) 
          * Height of people
          * Weight of cars
          * IQ
          * Liquid VOlume
      * Types of unidimensional scales
        * Thurstone(Equal-Appearing Interval) scale - agree/disagree statements w/ numerical values
        * Likert(Summative) scale - rate items based on level of agreement
        * Guttman(Cumulative) scale - binary yes/no

* **Mixture Distribution**
  * A mix of >2 probability distributions
    * random variables taken from >1 parent population distribution to form a combined distribtion
    * parent distributions can be univariate or multivariate
      * Combined distribution = Mixture distribution
        * should have same dimensionality as parent distributions
        * parent distributions should be all discrete probability or all continous probability distributions
  * Can be a mix of different distributions (Normal+T) or mix of same distribution w/ different parameters
    * 3 normal distributions w/ different parameters(mean) -> Gaussian Mixture Model
    * Mixture models can be used to find exptected values, maximum likelihood parameters estimate, and others
  * When to use?
    * Show how variables are differently distributed
      * ex) Stress effect on exam scores
        * Model for exam scores = normal distribution(=0.7) & bimodal distribution(p=0.3) -> note probabilities in different distributions add to 1
        
  > A random variable has a p1 chance of following a D1 distribution, a p2 chance of following a D2 distribution and a pn chance of following a Dn distribution, where “n” is the number of possible distributions. In the example above, we have two possible distributions, so:  
  > ex) Pbimodal=0.3. Pnormal=0.7  
  > <u>Mixture model formula</u>  
  > ![](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/05/mixture-distribution-formula.png)  
  > f1, f2, fn are component distributions  
  > λk is mixing weights - probabilities how much each individual distribution contributes to mixture

* **Finite & Infinite statistics**
  * Finite stats -> calculated from finite sets
    * ex) Sample used for calculations - have countable #data points in sample -> output calculation is a finite statistic
  
  
  * Infinite stats -> Calculated from infinite sets
    * ex) Probability density function - infinite data points under curve
    * ex) Normal distribution z-table (many decimal places to z-values)
    
    > Finite Set Definition -> If you can count the number of objects in your set, that’s a finite set.  
    > A finite set has a certain, countable number of objects. For example, you might have a fruit bowl with ten pieces of fruit. More technically, a finite set has a first element, second element, and so on, until the set reaches its last element.  
    > Finite Set Notation  
      In notation, a finite set is: {1, 2, 3, 4, 5}  
      Where you can replace 1 through 5 with any amount of any number. For example: {101, 222, 433, 97894, 5213457} or {.21, .22, .43, .7654, .975}
      
    > Infinite Set Definition  -> If you can’t count the number of objects, it’s an infinite set.  
    > try counting the number of stars in the universe. You won’t be able to, because there are an infinite number of items in the set of all stars.  
    > More technically, infinite sets don’t have a last element (e.g. a last number, letter, or object); The last of a last element makes counting go toward infinity. “The number of stars in the universe” is an example of an infinite set.  
    > Infinite Set Notation  
    Usually, but not always, the items in the infinite set will give you a clue to the missing contents. For example:  
    > {1, 2, 3, 4, 5, …} indicates it goes on and on to 6, 7, 8, 9, 10 … and beyond (basically, keep counting and never stop).  
    > {100, 200, 300, …} indicates you keep counting by one hundred until infinity
    
        
* **EM (Expectation-maximization) Algorithm**
  * Method to find maximum-likelihood estimates for model parameters
    * when data is incomplete, has missing points, or has hidden/latent variables
  * Iterative maximum-likelihood
    * MLE can estimate best fit model for set of complete data
    * EM algorithm can find model parameters if missing data
      * Choose random values for missing data points(guesses) -> estimate second set of data
      * New values from second set of data used to make better guesses for first set
      * **Back-and-Forth**
        * Values will converge at some point if this process is repeated many times
  * MLE vs EM
    * Both find best fit parameters
    * Both use different methods to find them
      * MLE -> gathers all data & uses data to construct most likely model
      * EM -> guesses parameters (based on missing data) - fixes model to fit guessed & observed data
        * Steps
          1. Intial guess of model parameters -> makes probability distribution (Expected step of expected distribution)
          2. Observed data fed into model
          3. Probability distribution from expected step is edited to include observed data (M-step)
          4. Steps 2-4 repeated until distribution which doesn't change from E-step to M-step is reached
    * EM always improves parameter estimate via multi-step process
      * May need random starts to find best model
        * Algorithm looks for local maxima which may not be close to optimal global maxima
        * -> EM works better if you restart & re-take inital guess (step1)
          * From all possible paramters, can choose the greatest maximum likelihood
      * Uses calculus integration & conditional probabilities
  * Limitations of EM
    * Slow, prefers data w/ low missing data & low dimensionality (E-step is rate limiting)

</body>
  